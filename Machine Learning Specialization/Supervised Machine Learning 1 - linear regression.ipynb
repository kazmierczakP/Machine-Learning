{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67763ca4",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dc2fee",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5cf7ad",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad77d81",
   "metadata": {},
   "source": [
    "$$ y_j = w_0+w_1x_1+w_2x_2+...+w_jx_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f6de52",
   "metadata": {},
   "source": [
    "# Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121b1652",
   "metadata": {},
   "source": [
    "$$ J = \\frac{1}{2m} \\sum_{i=0}^{m-1} (f(x)^{(i)}_j-y^{(i)}_j)^2  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9f9dbe",
   "metadata": {},
   "source": [
    "$$ J = \\frac{1}{2m} \\sum_{i=0}^{m-1} (w \\cdot x^{(i)}_j+b-y^{(i)}_j)^2  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e14e805",
   "metadata": {},
   "source": [
    "## MSE \n",
    "$$ 1/n*SUM(yhat-y)^2 $$\n",
    "\n",
    "$$ yhat=f(x) $$\n",
    "\n",
    "We minimize cost function with regards to f(x) parameters like w and b (f(x)=w1*x+w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ef588",
   "metadata": {},
   "source": [
    "Loss - one observation\n",
    "\n",
    "Cost - all obserwations (average loss for all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a6b94d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e935262",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([1,2,3,4,5])\n",
    "y=np.array([3,3.5,4,4.5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "223497ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2b02f6e77f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf82a9",
   "metadata": {},
   "source": [
    "f(x)=w1*x+w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96aa160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(w1,w0,x,y):\n",
    "    y_hat=w1*x+w0\n",
    "    errors=(y_hat-y)**2\n",
    "    cost=np.sum(errors)/len(x) #mse cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03e6f471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0=2.5\n",
    "w1=0.5\n",
    "c=cost(w1,w0,x,y)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "274ece24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_gradient(w1,w0,x,y):\n",
    "    errors_dw0=2*w1*x+2*w0-2*y # derrivative with respect to w0 is 2*(x*w1+w0-y)\n",
    "    dw0=np.sum(errors_dw0)/len(x)\n",
    "    \n",
    "    errors_dw1=2*w1*x**2+2*x*w0-2*x*y # derrivative with respect to w1 is 2*x*(x*w1+w0-y)\n",
    "    dw1=np.sum(errors_dw1)/len(x)\n",
    "    \n",
    "    return dw0,dw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f5be07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw0: 0.0\n",
      "dw1: 0.0\n"
     ]
    }
   ],
   "source": [
    "dw0, dw1 =cost_gradient(w1,w0,x,y)\n",
    "print(\"dw0:\", dw0)\n",
    "print(\"dw1:\", dw1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a46c0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0 - loss: 9.974799999999998, w0: 0.08, w1: 0.26, dw0: -8.00000, dw1: -26.00000\n",
      "\n",
      "STEP 100 - loss: 0.44175399938405324, w0: 0.94, w1: 0.93, dw0: -0.52850, dw1: 0.14639\n",
      "\n",
      "STEP 200 - loss: 0.22439661088325416, w0: 1.39, w1: 0.81, dw0: -0.37667, dw1: 0.10433\n",
      "\n",
      "STEP 300 - loss: 0.11398615303109871, w0: 1.71, w1: 0.72, dw0: -0.26846, dw1: 0.07436\n",
      "\n",
      "STEP 400 - loss: 0.05790124472775029, w0: 1.94, w1: 0.66, dw0: -0.19134, dw1: 0.05300\n",
      "\n",
      "STEP 500 - loss: 0.0294119421690471, w0: 2.10, w1: 0.61, dw0: -0.13637, dw1: 0.03777\n",
      "\n",
      "STEP 600 - loss: 0.014940306486032732, w0: 2.21, w1: 0.58, dw0: -0.09719, dw1: 0.02692\n",
      "\n",
      "STEP 700 - loss: 0.007589187977239291, w0: 2.30, w1: 0.56, dw0: -0.06927, dw1: 0.01919\n",
      "\n",
      "STEP 800 - loss: 0.0038550597477848086, w0: 2.35, w1: 0.54, dw0: -0.04937, dw1: 0.01367\n",
      "\n",
      "STEP 900 - loss: 0.001958244505678652, w0: 2.40, w1: 0.53, dw0: -0.03519, dw1: 0.00975\n",
      "\n",
      "STEP 1000 - loss: 0.0009947242831253183, w0: 2.43, w1: 0.52, dw0: -0.02508, dw1: 0.00695\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b02ff3b550>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "STEPS = 1000\n",
    "LEARNING_RATE = 0.01\n",
    "loss_array=np.array([])\n",
    "w0 = 0\n",
    "w1 = 0\n",
    "\n",
    "for step in range(0, STEPS + 1):\n",
    "    dw0, dw1 = cost_gradient(w1,w0,x,y)\n",
    "    w0-=(dw0 * LEARNING_RATE)\n",
    "    w1-=(dw1 * LEARNING_RATE)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        loss = cost(w1,w0,x,y)\n",
    "        print(f\"STEP {step} - loss: {loss}, w0: {w0:0.2f}, w1: {w1:0.2f}, dw0: {dw0:0.5f}, dw1: {dw1:0.5f}\\n\")\n",
    "        loss_array=np.append(loss_array, loss)\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(loss_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4173c3",
   "metadata": {},
   "source": [
    "## NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6311a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf5e11",
   "metadata": {},
   "source": [
    "### vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8307622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n",
      "[0 1 2 3]\n",
      "[0.20458615 0.69759513 0.11172562 0.5213304 ]\n",
      "[5 2 7 4]\n"
     ]
    }
   ],
   "source": [
    "a=np.zeros(4)\n",
    "b=np.arange(4)\n",
    "c=np.random.rand(4)\n",
    "d=np.array([5,2,7,4])\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "a=np.array([1,3,5,2,4,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e1bfda3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91d644a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41501b2",
   "metadata": {},
   "source": [
    "#### indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e915efa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81743a10",
   "metadata": {},
   "source": [
    "#### slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "903567ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed00a8",
   "metadata": {},
   "source": [
    "#### single vector operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af3b573b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -3, -5, -2, -4, -6])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=-a\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0722276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=np.sum(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "655076ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=np.mean(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e125b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  9, 25,  4, 16, 36])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=a**2\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddbfa8a",
   "metadata": {},
   "source": [
    "#### element-wise operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b037e016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  8, 10, 12])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([1,2,3,4])\n",
    "b=np.array([5,6,7,8])\n",
    "c=a+b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bdb0d923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 12, 21, 32])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=a*b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b697b874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 6, 8])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=a*2\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf6dcb",
   "metadata": {},
   "source": [
    "#### vector dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3421817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([1,2,3,4])\n",
    "b=np.array([5,6,7,8])\n",
    "c=np.dot(a,b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a90907c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78aa284b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4c925d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 6, 8])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=np.dot(a,2)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0f0e7",
   "metadata": {},
   "source": [
    "### matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f072891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.zeros((1,5))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e97ed3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0bc7ceb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=np.zeros((2,4))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a5aad5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "080d5b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.93004632, 0.85685483, 0.69243577, 0.58968056],\n",
       "       [0.40136806, 0.44647313, 0.17091541, 0.97910394]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=np.random.random_sample((2,4))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ceeb8441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [3, 4, 6],\n",
       "       [1, 2, 3]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=np.array([[1,2,3],[3,4,6],[1,2,3]])\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cee116",
   "metadata": {},
   "source": [
    "#### indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e540826d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.arange(6)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "340dfd31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=a.reshape(3,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c087ff",
   "metadata": {},
   "source": [
    "If we went numpy to adjust number of rows give -1 as firsy argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "add572f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=a.reshape(-1,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315c12b",
   "metadata": {},
   "source": [
    "#### slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e8789dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.arange(6).reshape(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f8f5b819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a063e522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d526c5a",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8f196-7a61-4649-a3f2-48f9cc137da8",
   "metadata": {},
   "source": [
    "Since by calculation we know that for `linear function` with one variable derrivative will always be like:\n",
    "\n",
    "$$ 2x*(w_1*x+w_0-y) $$ \n",
    "we can describe cost as average of multiple such calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ececa66b",
   "metadata": {},
   "source": [
    "$$ dw_j=\\frac{1}{m} \\sum_{i=0}^{m-1} (f(x^{(i)})-y^{(i)})*x^{(i)}_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f9a69",
   "metadata": {},
   "source": [
    "$$ f(x^{(i)})=w_0+w_1x_1+w_2x_2+...+w_jx_j $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6059749",
   "metadata": {},
   "outputs": [],
   "source": [
    "dw+=(x[row]*w+b-y[row])*x[row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "982b525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([1,2,3])\n",
    "w=np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d93523",
   "metadata": {},
   "source": [
    "#### no vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0e77cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=0\n",
    "for i in range(len(x)):\n",
    "    y+=x[i]*w[i]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0679d968",
   "metadata": {},
   "source": [
    "#### vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c448e7",
   "metadata": {},
   "source": [
    "Way to write coputations so that it runs faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de061f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "556d0441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.dot(w,x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd34a4d0",
   "metadata": {},
   "source": [
    "#### mutiple variable regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8739625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a20a7feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.array([[1,2104,5,1,45],[1,1416,3,2,40],[1,852,2,1,35]]) # 1 for w0\n",
    "y_train=np.array([460,232,178])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02c9cbc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.85181137e+02,  3.91335350e-01,  1.87537674e+01, -5.33603245e+01,\n",
       "       -2.64213162e+01])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=np.array([785.1811367994083,0.39133535,18.75376741,-53.36032453,-26.42131618])\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abcde578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x,w):\n",
    "    y=np.dot(x,w)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86677dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459.9999976194082"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=predict(x_train[0,:],w)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1138f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(x,y,w):\n",
    "    n=x.shape[0]\n",
    "    y_hat=np.dot(x,w)\n",
    "    cost=(y_hat-y)**2\n",
    "    sum_cost=np.sum(cost)\n",
    "    mse=sum_cost/(2*n)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8136033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5578904734917598e-12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=cost(x_train,y_train,w)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfae863",
   "metadata": {},
   "source": [
    "derrivative of linear function can be described as sum over all n observations:\n",
    "\n",
    "Sum over n observations so we have n y_hat and n y and mulitipy by n xi and take average of it\n",
    "\n",
    "dw_i=1/n * SUM((y_hat-y)*x_i)\n",
    "\n",
    "y_hat=x*w per observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cbf357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x,y,w):\n",
    "    rows,features=x.shape\n",
    "    dw=np.zeros(features)\n",
    "    errors=np.dot(x,w)-y\n",
    "    for feature in range(features):\n",
    "        for row in range(rows):\n",
    "            error=errors[row]\n",
    "            dw[feature]+=error*x[row,feature]\n",
    "    return dw/rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf1a39cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.67392514e-06, -2.72623580e-03, -6.27197270e-06, -2.21745575e-06,\n",
       "       -6.92403390e-05])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw=gradient(x_train,y_train,w)\n",
    "dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f734a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x,y,w,cost_function,gradient_function,steps=1000,alpha=5e-7):\n",
    "    history=np.array([])\n",
    "    for step in range(steps):\n",
    "        dw=gradient_function(x,y,w)\n",
    "        w-=alpha*dw\n",
    "        loss=cost(x,y,w)\n",
    "        if step%100==0:\n",
    "            history=np.append(history,loss)\n",
    "            print(f'STEP: {step} - loss: {loss}')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "94462c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w=np.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "579cef2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 0 - loss: 2529.462952231633\n",
      "STEP: 100 - loss: 695.9903158352025\n",
      "STEP: 200 - loss: 694.9206979323068\n",
      "STEP: 300 - loss: 693.8604297851185\n",
      "STEP: 400 - loss: 692.8094286135915\n",
      "STEP: 500 - loss: 691.7676123706057\n",
      "STEP: 600 - loss: 690.7348997355003\n",
      "STEP: 700 - loss: 689.7112101076154\n",
      "STEP: 800 - loss: 688.6964635999465\n",
      "STEP: 900 - loss: 687.690581032794\n"
     ]
    }
   ],
   "source": [
    "history=gradient_descent(x_train,y_train,w,cost,gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c78c89e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00223541,  0.20396569,  0.00374919, -0.0112487 , -0.0658614 ])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0887bfc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1beca1903a0>]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6f648ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([426.18530497, 286.16747201, 171.46763087])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(x_train,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8ee2f116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([460, 232, 178])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb58e0",
   "metadata": {},
   "source": [
    "# Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c1f220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "41663d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.array([[1,2104,5,1,45],[1,1416,3,2,40],[1,852,2,1,35]]) # 1 for w0\n",
    "y_train=np.array([460,232,178])\n",
    "w=np.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0008e528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 0 - loss: 48359.372799640616\n",
      "STEP: 100 - loss: 5012.976840185865\n",
      "STEP: 200 - loss: 1087.8717290569036\n",
      "STEP: 300 - loss: 732.4364435156646\n",
      "STEP: 400 - loss: 700.2412757007337\n",
      "STEP: 500 - loss: 697.3160912797399\n",
      "STEP: 600 - loss: 697.0413581339509\n",
      "STEP: 700 - loss: 697.0066290529734\n",
      "STEP: 800 - loss: 696.9936336430951\n",
      "STEP: 900 - loss: 696.9826071217781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1beca200fa0>]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history=gradient_descent(x_train,y_train,w,cost,gradient,alpha=5e-9)\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c08245",
   "metadata": {},
   "source": [
    "### normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1546b1a4",
   "metadata": {},
   "source": [
    "It is good to have values of your data normalized to range between -1 : 1 or 0-1 or somewhere around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1b19f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.array([[1,2104,5,1,45],[1,1416,3,2,40],[1,852,2,1,35]]) # 1 for w0\n",
    "y_train=np.array([460,232,178])\n",
    "w=np.zeros(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ff951b",
   "metadata": {},
   "source": [
    "#### mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ed6602",
   "metadata": {},
   "source": [
    "$$x_i := \\dfrac{x_i - \\mu_i}{max - min} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c369f9",
   "metadata": {},
   "source": [
    "$ \\mu_j = \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "00331a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_normalize(x):\n",
    "    x_train_normalized=np.ones((3,5))\n",
    "    rows,features=x.shape\n",
    "    for feature in range(1,features):\n",
    "        for row in range(rows):\n",
    "            x_train_normalized[row,feature]=(x[row,feature]-np.mean(x[:,feature]))/(np.max(x[:,feature])-np.min(x[:,feature]))\n",
    "    return x_train_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c7cab807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.51650692,  0.55555556, -0.33333333,  0.5       ],\n",
       "       [ 1.        , -0.03301384, -0.11111111,  0.66666667,  0.        ],\n",
       "       [ 1.        , -0.48349308, -0.44444444, -0.33333333, -0.5       ]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_normalized=mean_normalize(x_train)\n",
    "x_train_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "da6dcd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 0 - loss: 48607.34107775067\n",
      "STEP: 100 - loss: 8277.466757535356\n",
      "STEP: 200 - loss: 1787.2368330708316\n",
      "STEP: 300 - loss: 509.94000140298687\n",
      "STEP: 400 - loss: 182.1618443341837\n",
      "STEP: 500 - loss: 75.78893205062674\n",
      "STEP: 600 - loss: 35.25973606364686\n",
      "STEP: 700 - loss: 17.974124423697912\n",
      "STEP: 800 - loss: 9.868220997071111\n",
      "STEP: 900 - loss: 5.727628736823065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1becb864dc0>]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history=gradient_descent(x_train_normalized,y_train,w,cost,gradient,alpha=0.01)\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b6d3edb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([289.98748034,  92.34095414,  99.40774914, -60.32473182,\n",
       "        89.35362717])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0b59a10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([457.69380718, 235.67715709, 176.59147674])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(x_train_normalized,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "fa2620d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([460, 232, 178])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe50ef4",
   "metadata": {},
   "source": [
    "#### z-score normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394aa9d0",
   "metadata": {},
   "source": [
    "$$x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254c750a",
   "metadata": {},
   "source": [
    "$ \\mu_j = \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j  $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b806f5",
   "metadata": {},
   "source": [
    "$ \\sigma^2_j = \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3d522cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_normalize(x):\n",
    "    mu=np.mean(x,axis=0)\n",
    "    sigma=np.std(x,axis=0)\n",
    "    sigma[0]=1\n",
    "    x_norm=(x-mu)/sigma\n",
    "    x_norm[:,0]=1\n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "db1e4dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.26311506,  1.33630621, -0.70710678,  1.22474487],\n",
       "       [ 1.        , -0.08073519, -0.26726124,  1.41421356,  0.        ],\n",
       "       [ 1.        , -1.18237987, -1.06904497, -0.70710678, -1.22474487]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train=np.array([[1,2104,5,1,45],[1,1416,3,2,40],[1,852,2,1,35]]) # 1 for w0\n",
    "y_train=np.array([460,232,178])\n",
    "w=np.zeros(5)\n",
    "x_train_normalized=z_score_normalize(x_train)\n",
    "x_train_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "86e32802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 0 - loss: 48254.7717616363\n",
      "STEP: 100 - loss: 5582.449635877936\n",
      "STEP: 200 - loss: 745.7966398649063\n",
      "STEP: 300 - loss: 99.90278945590886\n",
      "STEP: 400 - loss: 13.383073446502612\n",
      "STEP: 500 - loss: 1.7928151365063023\n",
      "STEP: 600 - loss: 0.2401685882598563\n",
      "STEP: 700 - loss: 0.03217347141940435\n",
      "STEP: 800 - loss: 0.004310033482956535\n",
      "STEP: 900 - loss: 0.0005773834781969068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1becb92b4c0>]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history=gradient_descent(x_train_normalized,y_train,w,cost,gradient,alpha=0.01)\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "47e7df6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([459.98690403, 231.98894904, 177.98658794])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(x_train_normalized,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "29268617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([460, 232, 178])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75365cea",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21ed8b0",
   "metadata": {},
   "source": [
    "Designing new features from existing ones and feeding them to model instead of original ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f03bda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3161f88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x,y,w,cost_function,gradient_function,steps=1000,alpha=5e-7):\n",
    "    for step in range(steps):\n",
    "        dw=gradient_function(x,y,w)\n",
    "        w-=alpha*dw\n",
    "        loss=cost(x,y,w)\n",
    "        if step%100==0:\n",
    "            print(f'STEP: {step} - loss: {loss}')\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ea933e",
   "metadata": {},
   "source": [
    "#### without feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "8a27b5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(0,20,1)\n",
    "y=1+x**2\n",
    "matr = np.linspace((1,0),(1,19),20)\n",
    "w=np.zeros(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "0e96560b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 0 - loss: 11088.199682937498\n",
      "STEP: 100 - loss: 855.0723659974004\n",
      "STEP: 200 - loss: 833.3780991260588\n",
      "STEP: 300 - loss: 812.814712786459\n",
      "STEP: 400 - loss: 793.3232563174618\n",
      "STEP: 500 - loss: 774.8478520718494\n",
      "STEP: 600 - loss: 757.3355351969284\n",
      "STEP: 700 - loss: 740.7361017961424\n",
      "STEP: 800 - loss: 725.0019650057262\n",
      "STEP: 900 - loss: 710.0880185738071\n",
      "w1: 15.630425573376368, w0: -12.290463523693642\n"
     ]
    }
   ],
   "source": [
    "w=gradient_descent(matr,y,w,cost,gradient,alpha=0.001)\n",
    "predicted=predict(matr,w)\n",
    "print(f'w1: {w[1]}, w0: {w[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "3096ccfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1becc04bd60>"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,y,label='true values')\n",
    "plt.plot(x,predicted,color='red',label='prediction')\n",
    "plt.title('no feature engineering')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c9d592",
   "metadata": {},
   "source": [
    "#### with feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c9ba43-4478-47c9-8057-8de860bc6729",
   "metadata": {},
   "source": [
    "So we take x^2 instead of x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "3c39e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(0,20,1)\n",
    "y=1+x**2\n",
    "matr = np.linspace((1,0),(1,19),20)\n",
    "matr=matr**2\n",
    "w=np.zeros(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "2bbaebd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 0 - loss: 7329.218075123962\n",
      "STEP: 100 - loss: 0.22670484044740236\n",
      "STEP: 200 - loss: 0.2264973418450031\n",
      "STEP: 300 - loss: 0.2262900331621121\n",
      "STEP: 400 - loss: 0.2260829142248882\n",
      "STEP: 500 - loss: 0.22587598485966637\n",
      "STEP: 600 - loss: 0.225669244892931\n",
      "STEP: 700 - loss: 0.22546269415132883\n",
      "STEP: 800 - loss: 0.2252563324616735\n",
      "STEP: 900 - loss: 0.2250501596509201\n",
      "w1: 1.0043505669962975, w0: 0.008956967008735975\n"
     ]
    }
   ],
   "source": [
    "w=gradient_descent(matr,y,w,cost,gradient,alpha=0.00001)\n",
    "predicted=predict(matr,w)\n",
    "print(f'w1: {w[1]}, w0: {w[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "1e63d1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1becda04f70>"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,y,label='true values')\n",
    "plt.plot(x,predicted,color='red',label='prediction')\n",
    "plt.title('feature engineering')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b11461",
   "metadata": {},
   "source": [
    "#### selecting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "5cc8b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(0,20,1)\n",
    "y=1+x**2\n",
    "matr = np.c_[np.ones(20,dtype=int),x,x**2,x**3]\n",
    "w=np.zeros(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "15ba10cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 0 - loss: 1161.8616024639264\n",
      "STEP: 100 - loss: 394.1797402371666\n",
      "STEP: 200 - loss: 387.99041315175845\n",
      "STEP: 300 - loss: 381.8982874537316\n",
      "STEP: 400 - loss: 375.90183662635553\n",
      "STEP: 500 - loss: 369.9995581263567\n",
      "STEP: 600 - loss: 364.18997300742467\n",
      "STEP: 700 - loss: 358.47162554962676\n",
      "STEP: 800 - loss: 352.8430828946481\n",
      "STEP: 900 - loss: 347.30293468675546\n",
      "w0: 0.0015409905642508176, w1: 0.011935472356649244, w2: 0.07908593552952486, w3: 0.05536811466343547\n"
     ]
    }
   ],
   "source": [
    "w=gradient_descent(matr,y,w,cost,gradient,alpha=0.0000001)\n",
    "predicted=predict(matr,w)\n",
    "print(f'w0: {w[0]}, w1: {w[1]}, w2: {w[2]}, w3: {w[3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380350b1",
   "metadata": {},
   "source": [
    "One can clearly see that w2 (for x^2) has bigger value than other w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "22cc065b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1becdc2a6a0>"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,y,label='true values')\n",
    "plt.plot(x,predicted,color='red',label='prediction')\n",
    "plt.title('feature engineering')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a340be1",
   "metadata": {},
   "source": [
    "# Feature scaling + engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcaaaf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd534964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_normalize(x):\n",
    "    mu=np.mean(x,axis=0)\n",
    "    sigma=np.std(x,axis=0)\n",
    "    sigma[0]=1\n",
    "    x_norm=(x-mu)/sigma\n",
    "    x_norm[:,0]=1\n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27a5ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(0,20,1)\n",
    "y=1+x**2\n",
    "matr = np.c_[np.ones(20,dtype=int),x,x**2,x**3]\n",
    "matr=z_score_normalize(matr)\n",
    "w=np.zeros(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bd8cf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 0 - loss: 13667.8401144954\n",
      "STEP: 100 - loss: 1045.2089965415844\n",
      "STEP: 200 - loss: 146.46158109397035\n",
      "STEP: 300 - loss: 27.563910156884845\n",
      "STEP: 400 - loss: 11.096964426159253\n",
      "STEP: 500 - loss: 8.434917805871276\n",
      "STEP: 600 - loss: 7.6952747275385835\n",
      "STEP: 700 - loss: 7.2740823348131\n",
      "STEP: 800 - loss: 6.94645082857563\n",
      "STEP: 900 - loss: 6.673897628202946\n",
      "w0: 124.49462517969737, w1: 32.13369658647402, w2: 40.67114045554739, w3: 42.26105504514576\n"
     ]
    }
   ],
   "source": [
    "w=gradient_descent(matr,y,w,cost,gradient,alpha=0.01)\n",
    "predicted=predict(matr,w)\n",
    "print(f'w0: {w[0]}, w1: {w[1]}, w2: {w[2]}, w3: {w[3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8efb031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22fb5b55070>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,y,label='true values')\n",
    "plt.plot(x,predicted,color='red',label='prediction')\n",
    "plt.title('feature engineering')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29c11e",
   "metadata": {},
   "source": [
    "#### complex function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "72a94f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(0,20,1)\n",
    "y=np.cos(x/2)\n",
    "matr = np.c_[np.ones(20,dtype=int),x,x**2,x**3,x**4,x**5,x**6,x**7,x**8,x**9,x**10,x*11,x**12,x**13]\n",
    "matr=z_score_normalize(matr)\n",
    "w=np.zeros(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51e5a5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 0 - loss: 0.22593411991111728\n",
      "STEP: 100 - loss: 0.14075090108957658\n",
      "STEP: 200 - loss: 0.1178794847342229\n",
      "STEP: 300 - loss: 0.10321623282175411\n",
      "STEP: 400 - loss: 0.09296656219649577\n",
      "STEP: 500 - loss: 0.08565354799797203\n",
      "STEP: 600 - loss: 0.08038411544363014\n",
      "STEP: 700 - loss: 0.07654865979118854\n",
      "STEP: 800 - loss: 0.07372127486453262\n",
      "STEP: 900 - loss: 0.07160317142609571\n",
      "STEP: 1000 - loss: 0.06998458362705834\n",
      "STEP: 1100 - loss: 0.06871822149461283\n",
      "STEP: 1200 - loss: 0.06770065600722185\n",
      "STEP: 1300 - loss: 0.06685925253836239\n",
      "STEP: 1400 - loss: 0.0661429972328481\n",
      "STEP: 1500 - loss: 0.06551605628507715\n",
      "STEP: 1600 - loss: 0.06495325393857491\n",
      "STEP: 1700 - loss: 0.0644368976070453\n",
      "STEP: 1800 - loss: 0.06395454880315621\n",
      "STEP: 1900 - loss: 0.06349745811485591\n",
      "STEP: 2000 - loss: 0.0630594664066435\n",
      "STEP: 2100 - loss: 0.06263623335541411\n",
      "STEP: 2200 - loss: 0.06222469580653724\n",
      "STEP: 2300 - loss: 0.06182268748576519\n",
      "STEP: 2400 - loss: 0.06142867199840685\n",
      "STEP: 2500 - loss: 0.06104155536703189\n",
      "STEP: 2600 - loss: 0.060660554412862666\n",
      "STEP: 2700 - loss: 0.06028510434480616\n",
      "STEP: 2800 - loss: 0.05991479387602686\n",
      "STEP: 2900 - loss: 0.059549319667512965\n",
      "STEP: 3000 - loss: 0.05918845434106658\n",
      "STEP: 3100 - loss: 0.058832024019352\n",
      "STEP: 3200 - loss: 0.05847989255487429\n",
      "STEP: 3300 - loss: 0.0581319504552514\n",
      "STEP: 3400 - loss: 0.05778810710575548\n",
      "STEP: 3500 - loss: 0.057448285306876956\n",
      "STEP: 3600 - loss: 0.0571124174372782\n",
      "STEP: 3700 - loss: 0.05678044275794787\n",
      "STEP: 3800 - loss: 0.05645230551761056\n",
      "STEP: 3900 - loss: 0.05612795362071519\n",
      "STEP: 4000 - loss: 0.05580733769042902\n",
      "STEP: 4100 - loss: 0.05549041040898528\n",
      "STEP: 4200 - loss: 0.05517712605277965\n",
      "STEP: 4300 - loss: 0.054867440164220724\n",
      "STEP: 4400 - loss: 0.054561309319614316\n",
      "STEP: 4500 - loss: 0.054258690964493914\n",
      "STEP: 4600 - loss: 0.053959543296323895\n",
      "STEP: 4700 - loss: 0.05366382518048376\n",
      "STEP: 4800 - loss: 0.05337149608963696\n",
      "STEP: 4900 - loss: 0.05308251605953847\n",
      "STEP: 5000 - loss: 0.05279684565640249\n",
      "STEP: 5100 - loss: 0.05251444595240531\n",
      "STEP: 5200 - loss: 0.05223527850691828\n",
      "STEP: 5300 - loss: 0.051959305351782795\n",
      "STEP: 5400 - loss: 0.051686488979440906\n",
      "STEP: 5500 - loss: 0.05141679233308866\n",
      "STEP: 5600 - loss: 0.051150178798268185\n",
      "STEP: 5700 - loss: 0.05088661219548646\n",
      "STEP: 5800 - loss: 0.050626056773572826\n",
      "STEP: 5900 - loss: 0.05036847720357235\n",
      "STEP: 6000 - loss: 0.05011383857303181\n",
      "STEP: 6100 - loss: 0.049862106380579085\n",
      "STEP: 6200 - loss: 0.04961324653072368\n",
      "STEP: 6300 - loss: 0.049367225328830616\n",
      "STEP: 6400 - loss: 0.04912400947622993\n",
      "STEP: 6500 - loss: 0.048883566065440046\n",
      "STEP: 6600 - loss: 0.04864586257548449\n",
      "STEP: 6700 - loss: 0.048410866867291194\n",
      "STEP: 6800 - loss: 0.04817854717916454\n",
      "STEP: 6900 - loss: 0.04794887212232412\n",
      "STEP: 7000 - loss: 0.047721810676504786\n",
      "STEP: 7100 - loss: 0.04749733218561553\n",
      "STEP: 7200 - loss: 0.04727540635345338\n",
      "STEP: 7300 - loss: 0.04705600323947108\n",
      "STEP: 7400 - loss: 0.04683909325459677\n",
      "STEP: 7500 - loss: 0.04662464715710429\n",
      "STEP: 7600 - loss: 0.04641263604853322\n",
      "STEP: 7700 - loss: 0.04620303136965696\n",
      "STEP: 7800 - loss: 0.0459958048964997\n",
      "STEP: 7900 - loss: 0.04579092873639985\n",
      "STEP: 8000 - loss: 0.0455883753241199\n",
      "STEP: 8100 - loss: 0.04538811741800287\n",
      "STEP: 8200 - loss: 0.04519012809617292\n",
      "STEP: 8300 - loss: 0.04499438075278163\n",
      "STEP: 8400 - loss: 0.044800849094297754\n",
      "STEP: 8500 - loss: 0.04460950713584141\n",
      "STEP: 8600 - loss: 0.044420329197559616\n",
      "STEP: 8700 - loss: 0.044233289901046975\n",
      "STEP: 8800 - loss: 0.044048364165805765\n",
      "STEP: 8900 - loss: 0.04386552720574956\n",
      "STEP: 9000 - loss: 0.043684754525746695\n",
      "STEP: 9100 - loss: 0.04350602191820499\n",
      "STEP: 9200 - loss: 0.043329305459696335\n",
      "STEP: 9300 - loss: 0.04315458150762071\n",
      "STEP: 9400 - loss: 0.042981826696909614\n",
      "STEP: 9500 - loss: 0.04281101793676834\n",
      "STEP: 9600 - loss: 0.042642132407456165\n",
      "STEP: 9700 - loss: 0.04247514755710431\n",
      "STEP: 9800 - loss: 0.04231004109857161\n",
      "STEP: 9900 - loss: 0.042146791006336934\n",
      "STEP: 10000 - loss: 0.04198537551342756\n",
      "STEP: 10100 - loss: 0.04182577310838501\n",
      "STEP: 10200 - loss: 0.041667962532264645\n",
      "STEP: 10300 - loss: 0.04151192277567194\n",
      "STEP: 10400 - loss: 0.04135763307583309\n",
      "STEP: 10500 - loss: 0.04120507291369939\n",
      "STEP: 10600 - loss: 0.04105422201108683\n",
      "STEP: 10700 - loss: 0.040905060327848256\n",
      "STEP: 10800 - loss: 0.04075756805907897\n",
      "STEP: 10900 - loss: 0.04061172563235569\n",
      "STEP: 11000 - loss: 0.04046751370500691\n",
      "STEP: 11100 - loss: 0.040324913161416176\n",
      "STEP: 11200 - loss: 0.04018390511035673\n",
      "STEP: 11300 - loss: 0.04004447088235723\n",
      "STEP: 11400 - loss: 0.03990659202709858\n",
      "STEP: 11500 - loss: 0.039770250310841755\n",
      "STEP: 11600 - loss: 0.0396354277138847\n",
      "STEP: 11700 - loss: 0.039502106428050264\n",
      "STEP: 11800 - loss: 0.03937026885420283\n",
      "STEP: 11900 - loss: 0.039239897599794765\n",
      "STEP: 12000 - loss: 0.03911097547644055\n",
      "STEP: 12100 - loss: 0.038983485497521536\n",
      "STEP: 12200 - loss: 0.03885741087581561\n",
      "STEP: 12300 - loss: 0.03873273502115786\n",
      "STEP: 12400 - loss: 0.038609441538126704\n",
      "STEP: 12500 - loss: 0.0384875142237583\n",
      "STEP: 12600 - loss: 0.03836693706528652\n",
      "STEP: 12700 - loss: 0.03824769423791101\n",
      "STEP: 12800 - loss: 0.03812977010259005\n",
      "STEP: 12900 - loss: 0.038013149203861014\n",
      "STEP: 13000 - loss: 0.03789781626768364\n",
      "STEP: 13100 - loss: 0.03778375619931177\n",
      "STEP: 13200 - loss: 0.03767095408118816\n",
      "STEP: 13300 - loss: 0.03755939517086402\n",
      "STEP: 13400 - loss: 0.037449064898944555\n",
      "STEP: 13500 - loss: 0.037339948867056265\n",
      "STEP: 13600 - loss: 0.037232032845839855\n",
      "STEP: 13700 - loss: 0.03712530277296654\n",
      "STEP: 13800 - loss: 0.03701974475117669\n",
      "STEP: 13900 - loss: 0.03691534504634252\n",
      "STEP: 14000 - loss: 0.036812090085552654\n",
      "STEP: 14100 - loss: 0.036709966455220275\n",
      "STEP: 14200 - loss: 0.03660896089921198\n",
      "STEP: 14300 - loss: 0.036509060317000304\n",
      "STEP: 14400 - loss: 0.03641025176183664\n",
      "STEP: 14500 - loss: 0.03631252243894574\n",
      "STEP: 14600 - loss: 0.03621585970374254\n",
      "STEP: 14700 - loss: 0.03612025106006815\n",
      "STEP: 14800 - loss: 0.03602568415844811\n",
      "STEP: 14900 - loss: 0.03593214679437047\n",
      "STEP: 15000 - loss: 0.03583962690658359\n",
      "STEP: 15100 - loss: 0.03574811257541495\n",
      "STEP: 15200 - loss: 0.03565759202110895\n",
      "STEP: 15300 - loss: 0.035568053602184346\n",
      "STEP: 15400 - loss: 0.035479485813811225\n",
      "STEP: 15500 - loss: 0.035391877286206824\n",
      "STEP: 15600 - loss: 0.03530521678304998\n",
      "STEP: 15700 - loss: 0.03521949319991491\n",
      "STEP: 15800 - loss: 0.035134695562722466\n",
      "STEP: 15900 - loss: 0.03505081302621036\n",
      "STEP: 16000 - loss: 0.0349678348724204\n",
      "STEP: 16100 - loss: 0.034885750509205046\n",
      "STEP: 16200 - loss: 0.03480454946874953\n",
      "STEP: 16300 - loss: 0.034724221406112976\n",
      "STEP: 16400 - loss: 0.0346447560977856\n",
      "STEP: 16500 - loss: 0.03456614344026353\n",
      "STEP: 16600 - loss: 0.03448837344863997\n",
      "STEP: 16700 - loss: 0.034411436255212965\n",
      "STEP: 16800 - loss: 0.034335322108109564\n",
      "STEP: 16900 - loss: 0.034260021369925744\n",
      "STEP: 17000 - loss: 0.034185524516383785\n",
      "STEP: 17100 - loss: 0.03411182213500291\n",
      "STEP: 17200 - loss: 0.03403890492378778\n",
      "STEP: 17300 - loss: 0.03396676368993115\n",
      "STEP: 17400 - loss: 0.0338953893485323\n",
      "STEP: 17500 - loss: 0.03382477292133013\n",
      "STEP: 17600 - loss: 0.03375490553545177\n",
      "STEP: 17700 - loss: 0.033685778422174996\n",
      "STEP: 17800 - loss: 0.033617382915705374\n",
      "STEP: 17900 - loss: 0.0335497104519689\n",
      "STEP: 18000 - loss: 0.033482752567417035\n",
      "STEP: 18100 - loss: 0.0334165008978472\n",
      "STEP: 18200 - loss: 0.03335094717723621\n",
      "STEP: 18300 - loss: 0.033286083236587806\n",
      "STEP: 18400 - loss: 0.033221901002793985\n",
      "STEP: 18500 - loss: 0.033158392497509064\n",
      "STEP: 18600 - loss: 0.03309554983603731\n",
      "STEP: 18700 - loss: 0.03303336522623359\n",
      "STEP: 18800 - loss: 0.03297183096741703\n",
      "STEP: 18900 - loss: 0.03291093944929723\n",
      "STEP: 19000 - loss: 0.03285068315091337\n",
      "STEP: 19100 - loss: 0.032791054639585065\n",
      "STEP: 19200 - loss: 0.03273204656987659\n",
      "STEP: 19300 - loss: 0.03267365168257281\n",
      "STEP: 19400 - loss: 0.032615862803666365\n",
      "STEP: 19500 - loss: 0.03255867284335798\n",
      "STEP: 19600 - loss: 0.03250207479506763\n",
      "STEP: 19700 - loss: 0.03244606173445815\n",
      "STEP: 19800 - loss: 0.032390626818469305\n",
      "STEP: 19900 - loss: 0.03233576328436346\n",
      "STEP: 20000 - loss: 0.03228146444878367\n",
      "STEP: 20100 - loss: 0.03222772370682091\n",
      "STEP: 20200 - loss: 0.032174534531093216\n",
      "STEP: 20300 - loss: 0.03212189047083622\n",
      "STEP: 20400 - loss: 0.032069785151003546\n",
      "STEP: 20500 - loss: 0.03201821227137741\n",
      "STEP: 20600 - loss: 0.031967165605690746\n",
      "STEP: 20700 - loss: 0.03191663900075906\n",
      "STEP: 20800 - loss: 0.03186662637562246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 20900 - loss: 0.03181712172069766\n",
      "STEP: 21000 - loss: 0.03176811909694022\n",
      "STEP: 21100 - loss: 0.031719612635016756\n",
      "STEP: 21200 - loss: 0.031671596534485924\n",
      "STEP: 21300 - loss: 0.03162406506299059\n",
      "STEP: 21400 - loss: 0.031577012555457924\n",
      "STEP: 21500 - loss: 0.031530433413309564\n",
      "STEP: 21600 - loss: 0.0314843221036818\n",
      "STEP: 21700 - loss: 0.031438673158653105\n",
      "STEP: 21800 - loss: 0.03139348117448233\n",
      "STEP: 21900 - loss: 0.03134874081085578\n",
      "STEP: 22000 - loss: 0.03130444679014134\n",
      "STEP: 22100 - loss: 0.03126059389665406\n",
      "STEP: 22200 - loss: 0.031217176975928018\n",
      "STEP: 22300 - loss: 0.031174190933998137\n",
      "STEP: 22400 - loss: 0.03113163073668977\n",
      "STEP: 22500 - loss: 0.031089491408916963\n",
      "STEP: 22600 - loss: 0.031047768033988694\n",
      "STEP: 22700 - loss: 0.03100645575292339\n",
      "STEP: 22800 - loss: 0.030965549763771492\n",
      "STEP: 22900 - loss: 0.030925045320946372\n",
      "STEP: 23000 - loss: 0.03088493773456167\n",
      "STEP: 23100 - loss: 0.030845222369778703\n",
      "STEP: 23200 - loss: 0.03080589464615922\n",
      "STEP: 23300 - loss: 0.030766950037027473\n",
      "STEP: 23400 - loss: 0.030728384068838606\n",
      "STEP: 23500 - loss: 0.03069019232055555\n",
      "STEP: 23600 - loss: 0.030652370423031955\n",
      "STEP: 23700 - loss: 0.03061491405840357\n",
      "STEP: 23800 - loss: 0.030577818959486126\n",
      "STEP: 23900 - loss: 0.030541080909180136\n",
      "STEP: 24000 - loss: 0.030504695739883313\n",
      "STEP: 24100 - loss: 0.03046865933290952\n",
      "STEP: 24200 - loss: 0.03043296761791401\n",
      "STEP: 24300 - loss: 0.03039761657232659\n",
      "STEP: 24400 - loss: 0.030362602220790798\n",
      "STEP: 24500 - loss: 0.030327920634609117\n",
      "STEP: 24600 - loss: 0.03029356793119557\n",
      "STEP: 24700 - loss: 0.030259540273534537\n",
      "STEP: 24800 - loss: 0.030225833869645648\n",
      "STEP: 24900 - loss: 0.0301924449720552\n",
      "STEP: 25000 - loss: 0.03015936987727357\n",
      "STEP: 25100 - loss: 0.03012660492527902\n",
      "STEP: 25200 - loss: 0.030094146499007372\n",
      "STEP: 25300 - loss: 0.030061991023847802\n",
      "STEP: 25400 - loss: 0.030030134967144455\n",
      "STEP: 25500 - loss: 0.02999857483770388\n",
      "STEP: 25600 - loss: 0.02996730718530833\n",
      "STEP: 25700 - loss: 0.02993632860023453\n",
      "STEP: 25800 - loss: 0.029905635712779304\n",
      "STEP: 25900 - loss: 0.029875225192788214\n",
      "STEP: 26000 - loss: 0.02984509374919241\n",
      "STEP: 26100 - loss: 0.02981523812954982\n",
      "STEP: 26200 - loss: 0.029785655119590655\n",
      "STEP: 26300 - loss: 0.029756341542771392\n",
      "STEP: 26400 - loss: 0.029727294259829055\n",
      "STEP: 26500 - loss: 0.029698510168346137\n",
      "STEP: 26600 - loss: 0.02966998620231679\n",
      "STEP: 26700 - loss: 0.029641719331719614\n",
      "STEP: 26800 - loss: 0.029613706562095093\n",
      "STEP: 26900 - loss: 0.029585944934128784\n",
      "STEP: 27000 - loss: 0.02955843152323795\n",
      "STEP: 27100 - loss: 0.029531163439164315\n",
      "STEP: 27200 - loss: 0.02950413782557097\n",
      "STEP: 27300 - loss: 0.029477351859644323\n",
      "STEP: 27400 - loss: 0.02945080275170035\n",
      "STEP: 27500 - loss: 0.02942448774479589\n",
      "STEP: 27600 - loss: 0.029398404114344124\n",
      "STEP: 27700 - loss: 0.02937254916773475\n",
      "STEP: 27800 - loss: 0.029346920243959386\n",
      "STEP: 27900 - loss: 0.029321514713238968\n",
      "STEP: 28000 - loss: 0.029296329976658746\n",
      "STEP: 28100 - loss: 0.029271363465804973\n",
      "STEP: 28200 - loss: 0.029246612642407733\n",
      "STEP: 28300 - loss: 0.029222074997986014\n",
      "STEP: 28400 - loss: 0.029197748053498822\n",
      "STEP: 28500 - loss: 0.029173629358999482\n",
      "STEP: 28600 - loss: 0.029149716493293683\n",
      "STEP: 28700 - loss: 0.02912600706360243\n",
      "STEP: 28800 - loss: 0.029102498705228214\n",
      "STEP: 28900 - loss: 0.029079189081225636\n",
      "STEP: 29000 - loss: 0.02905607588207516\n",
      "STEP: 29100 - loss: 0.02903315682536175\n",
      "STEP: 29200 - loss: 0.029010429655456037\n",
      "STEP: 29300 - loss: 0.028987892143200467\n",
      "STEP: 29400 - loss: 0.028965542085598272\n",
      "STEP: 29500 - loss: 0.028943377305506425\n",
      "STEP: 29600 - loss: 0.028921395651332282\n",
      "STEP: 29700 - loss: 0.028899594996733134\n",
      "STEP: 29800 - loss: 0.028877973240320536\n",
      "STEP: 29900 - loss: 0.028856528305367102\n",
      "STEP: 30000 - loss: 0.028835258139516262\n",
      "STEP: 30100 - loss: 0.028814160714497817\n",
      "STEP: 30200 - loss: 0.028793234025843474\n",
      "STEP: 30300 - loss: 0.028772476092608235\n",
      "STEP: 30400 - loss: 0.028751884957094843\n",
      "STEP: 30500 - loss: 0.028731458684579332\n",
      "STEP: 30600 - loss: 0.028711195363043267\n",
      "STEP: 30700 - loss: 0.028691093102904946\n",
      "STEP: 30800 - loss: 0.028671150036758485\n",
      "STEP: 30900 - loss: 0.028651364319111677\n",
      "STEP: 31000 - loss: 0.028631734126129394\n",
      "STEP: 31100 - loss: 0.028612257655379465\n",
      "STEP: 31200 - loss: 0.028592933125581126\n",
      "STEP: 31300 - loss: 0.02857375877635717\n",
      "STEP: 31400 - loss: 0.02855473286798796\n",
      "STEP: 31500 - loss: 0.028535853681169426\n",
      "STEP: 31600 - loss: 0.028517119516773208\n",
      "STEP: 31700 - loss: 0.028498528695609898\n",
      "STEP: 31800 - loss: 0.028480079558194905\n",
      "STEP: 31900 - loss: 0.028461770464517026\n",
      "STEP: 32000 - loss: 0.02844359979381032\n",
      "STEP: 32100 - loss: 0.02842556594432749\n",
      "STEP: 32200 - loss: 0.028407667333117347\n",
      "STEP: 32300 - loss: 0.02838990239580357\n",
      "STEP: 32400 - loss: 0.028372269586367023\n",
      "STEP: 32500 - loss: 0.02835476737693022\n",
      "STEP: 32600 - loss: 0.028337394257544096\n",
      "STEP: 32700 - loss: 0.028320148735977952\n",
      "STEP: 32800 - loss: 0.028303029337510922\n",
      "STEP: 32900 - loss: 0.028286034604727357\n",
      "STEP: 33000 - loss: 0.02826916309731246\n",
      "STEP: 33100 - loss: 0.02825241339185251\n",
      "STEP: 33200 - loss: 0.028235784081636232\n",
      "STEP: 33300 - loss: 0.028219273776458203\n",
      "STEP: 33400 - loss: 0.028202881102425904\n",
      "STEP: 33500 - loss: 0.028186604701767613\n",
      "STEP: 33600 - loss: 0.028170443232643727\n",
      "STEP: 33700 - loss: 0.02815439536895873\n",
      "STEP: 33800 - loss: 0.02813845980017763\n",
      "STEP: 33900 - loss: 0.028122635231142678\n",
      "STEP: 34000 - loss: 0.028106920381892324\n",
      "STEP: 34100 - loss: 0.028091313987484455\n",
      "STEP: 34200 - loss: 0.028075814797817844\n",
      "STEP: 34300 - loss: 0.028060421577459853\n",
      "STEP: 34400 - loss: 0.02804513310547339\n",
      "STEP: 34500 - loss: 0.0280299481752466\n",
      "STEP: 34600 - loss: 0.028014865594325485\n",
      "STEP: 34700 - loss: 0.027999884184246697\n",
      "STEP: 34800 - loss: 0.02798500278037392\n",
      "STEP: 34900 - loss: 0.027970220231735544\n",
      "STEP: 35000 - loss: 0.027955535400864047\n",
      "STEP: 35100 - loss: 0.02794094716363781\n",
      "STEP: 35200 - loss: 0.02792645440912423\n",
      "STEP: 35300 - loss: 0.027912056039424156\n",
      "STEP: 35400 - loss: 0.02789775096952063\n",
      "STEP: 35500 - loss: 0.027883538127126118\n",
      "STEP: 35600 - loss: 0.02786941645253372\n",
      "STEP: 35700 - loss: 0.027855384898469355\n",
      "STEP: 35800 - loss: 0.027841442429945833\n",
      "STEP: 35900 - loss: 0.027827588024118905\n",
      "STEP: 36000 - loss: 0.027813820670144207\n",
      "STEP: 36100 - loss: 0.027800139369036504\n",
      "STEP: 36200 - loss: 0.0277865431335309\n",
      "STEP: 36300 - loss: 0.027773030987945074\n",
      "STEP: 36400 - loss: 0.027759601968042863\n",
      "STEP: 36500 - loss: 0.027746255120900516\n",
      "STEP: 36600 - loss: 0.027732989504773535\n",
      "STEP: 36700 - loss: 0.027719804188965406\n",
      "STEP: 36800 - loss: 0.027706698253698342\n",
      "STEP: 36900 - loss: 0.02769367078998472\n",
      "STEP: 37000 - loss: 0.027680720899500456\n",
      "STEP: 37100 - loss: 0.027667847694459957\n",
      "STEP: 37200 - loss: 0.027655050297492045\n",
      "STEP: 37300 - loss: 0.02764232784151861\n",
      "STEP: 37400 - loss: 0.027629679469632636\n",
      "STEP: 37500 - loss: 0.02761710433497961\n",
      "STEP: 37600 - loss: 0.027604601600639052\n",
      "STEP: 37700 - loss: 0.027592170439508495\n",
      "STEP: 37800 - loss: 0.027579810034187396\n",
      "STEP: 37900 - loss: 0.027567519576863857\n",
      "STEP: 38000 - loss: 0.02755529826920214\n",
      "STEP: 38100 - loss: 0.02754314532223067\n",
      "STEP: 38200 - loss: 0.02753105995623333\n",
      "STEP: 38300 - loss: 0.027519041400639637\n",
      "STEP: 38400 - loss: 0.027507088893917665\n",
      "STEP: 38500 - loss: 0.02749520168346885\n",
      "STEP: 38600 - loss: 0.027483379025521882\n",
      "STEP: 38700 - loss: 0.02747162018502979\n",
      "STEP: 38800 - loss: 0.027459924435567308\n",
      "STEP: 38900 - loss: 0.027448291059229864\n",
      "STEP: 39000 - loss: 0.02743671934653318\n",
      "STEP: 39100 - loss: 0.027425208596314643\n",
      "STEP: 39200 - loss: 0.02741375811563616\n",
      "STEP: 39300 - loss: 0.02740236721968643\n",
      "STEP: 39400 - loss: 0.027391035231686706\n",
      "STEP: 39500 - loss: 0.02737976148279559\n",
      "STEP: 39600 - loss: 0.027368545312016705\n",
      "STEP: 39700 - loss: 0.027357386066105806\n",
      "STEP: 39800 - loss: 0.027346283099480423\n",
      "STEP: 39900 - loss: 0.02733523577412935\n",
      "STEP: 40000 - loss: 0.02732424345952449\n",
      "STEP: 40100 - loss: 0.027313305532532327\n",
      "STEP: 40200 - loss: 0.027302421377327156\n",
      "STEP: 40300 - loss: 0.02729159038530604\n",
      "STEP: 40400 - loss: 0.027280811955003176\n",
      "STEP: 40500 - loss: 0.027270085492006498\n",
      "STEP: 40600 - loss: 0.027259410408874928\n",
      "STEP: 40700 - loss: 0.027248786125056497\n",
      "STEP: 40800 - loss: 0.027238212066807176\n",
      "STEP: 40900 - loss: 0.027227687667111422\n",
      "STEP: 41000 - loss: 0.02721721236560274\n",
      "STEP: 41100 - loss: 0.027206785608486404\n",
      "STEP: 41200 - loss: 0.027196406848461297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 41300 - loss: 0.027186075544644306\n",
      "STEP: 41400 - loss: 0.027175791162494793\n",
      "STEP: 41500 - loss: 0.027165553173740576\n",
      "STEP: 41600 - loss: 0.027155361056303402\n",
      "STEP: 41700 - loss: 0.027145214294227137\n",
      "STEP: 41800 - loss: 0.027135112377605786\n",
      "STEP: 41900 - loss: 0.02712505480251167\n",
      "STEP: 42000 - loss: 0.027115041070926178\n",
      "STEP: 42100 - loss: 0.027105070690670353\n",
      "STEP: 42200 - loss: 0.02709514317533555\n",
      "STEP: 42300 - loss: 0.02708525804421652\n",
      "STEP: 42400 - loss: 0.02707541482224405\n",
      "STEP: 42500 - loss: 0.027065613039918675\n",
      "STEP: 42600 - loss: 0.027055852233246004\n",
      "STEP: 42700 - loss: 0.027046131943671274\n",
      "STEP: 42800 - loss: 0.027036451718015896\n",
      "STEP: 42900 - loss: 0.02702681110841427\n",
      "STEP: 43000 - loss: 0.027017209672251446\n",
      "STEP: 43100 - loss: 0.02700764697210161\n",
      "STEP: 43200 - loss: 0.026998122575666868\n",
      "STEP: 43300 - loss: 0.026988636055717337\n",
      "STEP: 43400 - loss: 0.02697918699003162\n",
      "STEP: 43500 - loss: 0.026969774961337643\n",
      "STEP: 43600 - loss: 0.02696039955725512\n",
      "STEP: 43700 - loss: 0.02695106037023761\n",
      "STEP: 43800 - loss: 0.02694175699751635\n",
      "STEP: 43900 - loss: 0.026932489041043655\n",
      "STEP: 44000 - loss: 0.026923256107437687\n",
      "STEP: 44100 - loss: 0.02691405780792776\n",
      "STEP: 44200 - loss: 0.02690489375829998\n",
      "STEP: 44300 - loss: 0.026895763578844257\n",
      "STEP: 44400 - loss: 0.026886666894300647\n",
      "STEP: 44500 - loss: 0.026877603333808064\n",
      "STEP: 44600 - loss: 0.02686857253085147\n",
      "STEP: 44700 - loss: 0.026859574123212417\n",
      "STEP: 44800 - loss: 0.026850607752916957\n",
      "STEP: 44900 - loss: 0.026841673066186955\n",
      "STEP: 45000 - loss: 0.02683276971339032\n",
      "STEP: 45100 - loss: 0.026823897348992758\n",
      "STEP: 45200 - loss: 0.026815055631509155\n",
      "STEP: 45300 - loss: 0.026806244223456156\n",
      "STEP: 45400 - loss: 0.026797462791305775\n",
      "STEP: 45500 - loss: 0.02678871100543816\n",
      "STEP: 45600 - loss: 0.026779988540096167\n",
      "STEP: 45700 - loss: 0.026771295073339953\n",
      "STEP: 45800 - loss: 0.026762630287002022\n",
      "STEP: 45900 - loss: 0.02675399386664331\n",
      "STEP: 46000 - loss: 0.02674538550150878\n",
      "STEP: 46100 - loss: 0.02673680488448508\n",
      "STEP: 46200 - loss: 0.026728251712056884\n",
      "STEP: 46300 - loss: 0.02671972568426525\n",
      "STEP: 46400 - loss: 0.02671122650466548\n",
      "STEP: 46500 - loss: 0.026702753880286305\n",
      "STEP: 46600 - loss: 0.026694307521588696\n",
      "STEP: 46700 - loss: 0.02668588714242596\n",
      "STEP: 46800 - loss: 0.026677492460003338\n",
      "STEP: 46900 - loss: 0.026669123194839394\n",
      "STEP: 47000 - loss: 0.026660779070726708\n",
      "STEP: 47100 - loss: 0.02665245981469342\n",
      "STEP: 47200 - loss: 0.0266441651569653\n",
      "STEP: 47300 - loss: 0.026635894830928368\n",
      "STEP: 47400 - loss: 0.026627648573091516\n",
      "STEP: 47500 - loss: 0.026619426123050155\n",
      "STEP: 47600 - loss: 0.026611227223449656\n",
      "STEP: 47700 - loss: 0.026603051619950073\n",
      "STEP: 47800 - loss: 0.026594899061190048\n",
      "STEP: 47900 - loss: 0.02658676929875222\n",
      "STEP: 48000 - loss: 0.02657866208712904\n",
      "STEP: 48100 - loss: 0.026570577183687955\n",
      "STEP: 48200 - loss: 0.026562514348637822\n",
      "STEP: 48300 - loss: 0.02655447334499599\n",
      "STEP: 48400 - loss: 0.02654645393855474\n",
      "STEP: 48500 - loss: 0.026538455897849294\n",
      "STEP: 48600 - loss: 0.02653047899412469\n",
      "STEP: 48700 - loss: 0.026522523001305497\n",
      "STEP: 48800 - loss: 0.026514587695962884\n",
      "STEP: 48900 - loss: 0.026506672857284346\n",
      "STEP: 49000 - loss: 0.026498778267042267\n",
      "STEP: 49100 - loss: 0.02649090370956487\n",
      "STEP: 49200 - loss: 0.02648304897170504\n",
      "STEP: 49300 - loss: 0.026475213842810978\n",
      "STEP: 49400 - loss: 0.02646739811469729\n",
      "STEP: 49500 - loss: 0.02645960158161556\n",
      "STEP: 49600 - loss: 0.026451824040226075\n",
      "STEP: 49700 - loss: 0.02644406528956928\n",
      "STEP: 49800 - loss: 0.026436325131038163\n",
      "STEP: 49900 - loss: 0.026428603368350418\n",
      "STEP: 50000 - loss: 0.026420899807521138\n",
      "STEP: 50100 - loss: 0.02641321425683572\n",
      "STEP: 50200 - loss: 0.02640554652682392\n",
      "STEP: 50300 - loss: 0.026397896430232624\n",
      "STEP: 50400 - loss: 0.02639026378200001\n",
      "STEP: 50500 - loss: 0.0263826483992306\n",
      "STEP: 50600 - loss: 0.0263750501011689\n",
      "STEP: 50700 - loss: 0.02636746870917448\n",
      "STEP: 50800 - loss: 0.02635990404669757\n",
      "STEP: 50900 - loss: 0.02635235593925402\n",
      "STEP: 51000 - loss: 0.026344824214401614\n",
      "STEP: 51100 - loss: 0.026337308701715294\n",
      "STEP: 51200 - loss: 0.026329809232764213\n",
      "STEP: 51300 - loss: 0.026322325641088128\n",
      "STEP: 51400 - loss: 0.026314857762174116\n",
      "STEP: 51500 - loss: 0.026307405433433788\n",
      "STEP: 51600 - loss: 0.026299968494180998\n",
      "STEP: 51700 - loss: 0.026292546785608568\n",
      "STEP: 51800 - loss: 0.026285140150767605\n",
      "STEP: 51900 - loss: 0.02627774843454465\n",
      "STEP: 52000 - loss: 0.026270371483640524\n",
      "STEP: 52100 - loss: 0.026263009146548805\n",
      "STEP: 52200 - loss: 0.026255661273535465\n",
      "STEP: 52300 - loss: 0.026248327716616882\n",
      "STEP: 52400 - loss: 0.026241008329540672\n",
      "STEP: 52500 - loss: 0.026233702967763995\n",
      "STEP: 52600 - loss: 0.026226411488434565\n",
      "STEP: 52700 - loss: 0.026219133750370216\n",
      "STEP: 52800 - loss: 0.026211869614039734\n",
      "STEP: 52900 - loss: 0.026204618941543086\n",
      "STEP: 53000 - loss: 0.026197381596592302\n",
      "STEP: 53100 - loss: 0.026190157444492677\n",
      "STEP: 53200 - loss: 0.026182946352124342\n",
      "STEP: 53300 - loss: 0.026175748187923335\n",
      "STEP: 53400 - loss: 0.02616856282186325\n",
      "STEP: 53500 - loss: 0.02616139012543774\n",
      "STEP: 53600 - loss: 0.0261542299716425\n",
      "STEP: 53700 - loss: 0.026147082234956837\n",
      "STEP: 53800 - loss: 0.02613994679132799\n",
      "STEP: 53900 - loss: 0.026132823518151903\n",
      "STEP: 54000 - loss: 0.026125712294257458\n",
      "STEP: 54100 - loss: 0.026118612999889733\n",
      "STEP: 54200 - loss: 0.026111525516692387\n",
      "STEP: 54300 - loss: 0.02610444972769229\n",
      "STEP: 54400 - loss: 0.02609738551728264\n",
      "STEP: 54500 - loss: 0.02609033277120735\n",
      "STEP: 54600 - loss: 0.02608329137654481\n",
      "STEP: 54700 - loss: 0.026076261221692265\n",
      "STEP: 54800 - loss: 0.026069242196351077\n",
      "STEP: 54900 - loss: 0.02606223419151015\n",
      "STEP: 55000 - loss: 0.026055237099431507\n",
      "STEP: 55100 - loss: 0.02604825081363611\n",
      "STEP: 55200 - loss: 0.02604127522888719\n",
      "STEP: 55300 - loss: 0.02603431024117763\n",
      "STEP: 55400 - loss: 0.026027355747714115\n",
      "STEP: 55500 - loss: 0.026020411646903923\n",
      "STEP: 55600 - loss: 0.026013477838339943\n",
      "STEP: 55700 - loss: 0.02600655422278735\n",
      "STEP: 55800 - loss: 0.02599964070216963\n",
      "STEP: 55900 - loss: 0.025992737179555175\n",
      "STEP: 56000 - loss: 0.02598584355914329\n",
      "STEP: 56100 - loss: 0.0259789597462518\n",
      "STEP: 56200 - loss: 0.025972085647303196\n",
      "STEP: 56300 - loss: 0.025965221169811776\n",
      "STEP: 56400 - loss: 0.025958366222371288\n",
      "STEP: 56500 - loss: 0.02595152071464185\n",
      "STEP: 56600 - loss: 0.0259446845573371\n",
      "STEP: 56700 - loss: 0.025937857662212897\n",
      "STEP: 56800 - loss: 0.02593103994205411\n",
      "STEP: 56900 - loss: 0.02592423131066283\n",
      "STEP: 57000 - loss: 0.025917431682846763\n",
      "STEP: 57100 - loss: 0.025910640974406783\n",
      "STEP: 57200 - loss: 0.0259038591021258\n",
      "STEP: 57300 - loss: 0.02589708598375707\n",
      "STEP: 57400 - loss: 0.025890321538012878\n",
      "STEP: 57500 - loss: 0.02588356568455304\n",
      "STEP: 57600 - loss: 0.025876818343974223\n",
      "STEP: 57700 - loss: 0.025870079437798055\n",
      "STEP: 57800 - loss: 0.025863348888462\n",
      "STEP: 57900 - loss: 0.025856626619306093\n",
      "STEP: 58000 - loss: 0.025849912554565223\n",
      "STEP: 58100 - loss: 0.02584320661935603\n",
      "STEP: 58200 - loss: 0.025836508739667797\n",
      "STEP: 58300 - loss: 0.025829818842352976\n",
      "STEP: 58400 - loss: 0.025823136855115097\n",
      "STEP: 58500 - loss: 0.025816462706500794\n",
      "STEP: 58600 - loss: 0.02580979632588821\n",
      "STEP: 58700 - loss: 0.025803137643478257\n",
      "STEP: 58800 - loss: 0.025796486590285068\n",
      "STEP: 58900 - loss: 0.025789843098125785\n",
      "STEP: 59000 - loss: 0.02578320709961137\n",
      "STEP: 59100 - loss: 0.025776578528137268\n",
      "STEP: 59200 - loss: 0.02576995731787467\n",
      "STEP: 59300 - loss: 0.02576334340376078\n",
      "STEP: 59400 - loss: 0.025756736721490165\n",
      "STEP: 59500 - loss: 0.025750137207505835\n",
      "STEP: 59600 - loss: 0.025743544798990065\n",
      "STEP: 59700 - loss: 0.025736959433856465\n",
      "STEP: 59800 - loss: 0.025730381050740846\n",
      "STEP: 59900 - loss: 0.025723809588992796\n",
      "STEP: 60000 - loss: 0.02571724498866726\n",
      "STEP: 60100 - loss: 0.025710687190516945\n",
      "STEP: 60200 - loss: 0.025704136135983057\n",
      "STEP: 60300 - loss: 0.025697591767187715\n",
      "STEP: 60400 - loss: 0.02569105402692649\n",
      "STEP: 60500 - loss: 0.02568452285865952\n",
      "STEP: 60600 - loss: 0.02567799820650441\n",
      "STEP: 60700 - loss: 0.02567148001522813\n",
      "STEP: 60800 - loss: 0.025664968230239997\n",
      "STEP: 60900 - loss: 0.02565846279758286\n",
      "STEP: 61000 - loss: 0.025651963663927406\n",
      "STEP: 61100 - loss: 0.02564547077656339\n",
      "STEP: 61200 - loss: 0.02563898408339299\n",
      "STEP: 61300 - loss: 0.025632503532923494\n",
      "STEP: 61400 - loss: 0.025626029074260127\n",
      "STEP: 61500 - loss: 0.02561956065709921\n",
      "STEP: 61600 - loss: 0.025613098231721337\n",
      "STEP: 61700 - loss: 0.02560664174898373\n",
      "STEP: 61800 - loss: 0.025600191160314534\n",
      "STEP: 61900 - loss: 0.0255937464177056\n",
      "STEP: 62000 - loss: 0.025587307473705505\n",
      "STEP: 62100 - loss: 0.02558087428141385\n",
      "STEP: 62200 - loss: 0.025574446794474293\n",
      "STEP: 62300 - loss: 0.025568024967067993\n",
      "STEP: 62400 - loss: 0.025561608753907762\n",
      "STEP: 62500 - loss: 0.025555198110231304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 62600 - loss: 0.025548792991795972\n",
      "STEP: 62700 - loss: 0.025542393354871296\n",
      "STEP: 62800 - loss: 0.025535999156234175\n",
      "STEP: 62900 - loss: 0.025529610353162306\n",
      "STEP: 63000 - loss: 0.025523226903428786\n",
      "STEP: 63100 - loss: 0.025516848765295495\n",
      "STEP: 63200 - loss: 0.025510475897508194\n",
      "STEP: 63300 - loss: 0.025504108259290496\n",
      "STEP: 63400 - loss: 0.02549774581033811\n",
      "STEP: 63500 - loss: 0.025491388510813527\n",
      "STEP: 63600 - loss: 0.025485036321340748\n",
      "STEP: 63700 - loss: 0.025478689202998894\n",
      "STEP: 63800 - loss: 0.025472347117318328\n",
      "STEP: 63900 - loss: 0.02546601002627417\n",
      "STEP: 64000 - loss: 0.025459677892281603\n",
      "STEP: 64100 - loss: 0.025453350678190833\n",
      "STEP: 64200 - loss: 0.02544702834728129\n",
      "STEP: 64300 - loss: 0.02544071086325737\n",
      "STEP: 64400 - loss: 0.02543439819024329\n",
      "STEP: 64500 - loss: 0.02542809029277789\n",
      "STEP: 64600 - loss: 0.02542178713580946\n",
      "STEP: 64700 - loss: 0.02541548868469205\n",
      "STEP: 64800 - loss: 0.025409194905179823\n",
      "STEP: 64900 - loss: 0.025402905763422218\n",
      "STEP: 65000 - loss: 0.025396621225960044\n",
      "STEP: 65100 - loss: 0.025390341259720266\n",
      "STEP: 65200 - loss: 0.02538406583201181\n",
      "STEP: 65300 - loss: 0.025377794910521268\n",
      "STEP: 65400 - loss: 0.02537152846330748\n",
      "STEP: 65500 - loss: 0.025365266458798597\n",
      "STEP: 65600 - loss: 0.02535900886578652\n",
      "STEP: 65700 - loss: 0.02535275565342333\n",
      "STEP: 65800 - loss: 0.02534650679121695\n",
      "STEP: 65900 - loss: 0.025340262249026656\n",
      "STEP: 66000 - loss: 0.025334021997059347\n",
      "STEP: 66100 - loss: 0.02532778600586527\n",
      "STEP: 66200 - loss: 0.02532155424633386\n",
      "STEP: 66300 - loss: 0.0253153266896902\n",
      "STEP: 66400 - loss: 0.025309103307490317\n",
      "STEP: 66500 - loss: 0.02530288407161862\n",
      "STEP: 66600 - loss: 0.025296668954282498\n",
      "STEP: 66700 - loss: 0.02529045792800928\n",
      "STEP: 66800 - loss: 0.025284250965642956\n",
      "STEP: 66900 - loss: 0.025278048040339153\n",
      "STEP: 67000 - loss: 0.025271849125563135\n",
      "STEP: 67100 - loss: 0.025265654195084808\n",
      "STEP: 67200 - loss: 0.02525946322297549\n",
      "STEP: 67300 - loss: 0.025253276183605344\n",
      "STEP: 67400 - loss: 0.02524709305163797\n",
      "STEP: 67500 - loss: 0.02524091380202903\n",
      "STEP: 67600 - loss: 0.02523473841002095\n",
      "STEP: 67700 - loss: 0.025228566851141266\n",
      "STEP: 67800 - loss: 0.025222399101198036\n",
      "STEP: 67900 - loss: 0.02521623513627692\n",
      "STEP: 68000 - loss: 0.025210074932738163\n",
      "STEP: 68100 - loss: 0.02520391846721327\n",
      "STEP: 68200 - loss: 0.025197765716601532\n",
      "STEP: 68300 - loss: 0.025191616658067212\n",
      "STEP: 68400 - loss: 0.02518547126903593\n",
      "STEP: 68500 - loss: 0.025179329527193\n",
      "STEP: 68600 - loss: 0.025173191410478257\n",
      "STEP: 68700 - loss: 0.025167056897084584\n",
      "STEP: 68800 - loss: 0.02516092596545482\n",
      "STEP: 68900 - loss: 0.02515479859427831\n",
      "STEP: 69000 - loss: 0.025148674762488048\n",
      "STEP: 69100 - loss: 0.025142554449258725\n",
      "STEP: 69200 - loss: 0.02513643763400238\n",
      "STEP: 69300 - loss: 0.025130324296366757\n",
      "STEP: 69400 - loss: 0.02512421441623226\n",
      "STEP: 69500 - loss: 0.025118107973709364\n",
      "STEP: 69600 - loss: 0.025112004949135264\n",
      "STEP: 69700 - loss: 0.02510590532307192\n",
      "STEP: 69800 - loss: 0.025099809076303343\n",
      "STEP: 69900 - loss: 0.02509371618983271\n",
      "STEP: 70000 - loss: 0.0250876266448797\n",
      "STEP: 70100 - loss: 0.025081540422878645\n",
      "STEP: 70200 - loss: 0.025075457505474963\n",
      "STEP: 70300 - loss: 0.025069377874523506\n",
      "STEP: 70400 - loss: 0.02506330151208603\n",
      "STEP: 70500 - loss: 0.025057228400428233\n",
      "STEP: 70600 - loss: 0.025051158522018096\n",
      "STEP: 70700 - loss: 0.025045091859522882\n",
      "STEP: 70800 - loss: 0.025039028395807194\n",
      "STEP: 70900 - loss: 0.02503296811393028\n",
      "STEP: 71000 - loss: 0.025026910997144332\n",
      "STEP: 71100 - loss: 0.025020857028891258\n",
      "STEP: 71200 - loss: 0.025014806192802053\n",
      "STEP: 71300 - loss: 0.02500875847269272\n",
      "STEP: 71400 - loss: 0.0250027138525633\n",
      "STEP: 71500 - loss: 0.02499667231659555\n",
      "STEP: 71600 - loss: 0.024990633849150286\n",
      "STEP: 71700 - loss: 0.024984598434765984\n",
      "STEP: 71800 - loss: 0.024978566058156668\n",
      "STEP: 71900 - loss: 0.024972536704208643\n",
      "STEP: 72000 - loss: 0.024966510357979987\n",
      "STEP: 72100 - loss: 0.024960487004697975\n",
      "STEP: 72200 - loss: 0.024954466629756734\n",
      "STEP: 72300 - loss: 0.024948449218715744\n",
      "STEP: 72400 - loss: 0.024942434757298217\n",
      "STEP: 72500 - loss: 0.024936423231387766\n",
      "STEP: 72600 - loss: 0.02493041462702838\n",
      "STEP: 72700 - loss: 0.02492440893042086\n",
      "STEP: 72800 - loss: 0.024918406127922642\n",
      "STEP: 72900 - loss: 0.02491240620604419\n",
      "STEP: 73000 - loss: 0.024906409151448594\n",
      "STEP: 73100 - loss: 0.024900414950949405\n",
      "STEP: 73200 - loss: 0.02489442359150814\n",
      "STEP: 73300 - loss: 0.024888435060233964\n",
      "STEP: 73400 - loss: 0.02488244934438045\n",
      "STEP: 73500 - loss: 0.024876466431344762\n",
      "STEP: 73600 - loss: 0.024870486308665986\n",
      "STEP: 73700 - loss: 0.02486450896402314\n",
      "STEP: 73800 - loss: 0.024858534385233565\n",
      "STEP: 73900 - loss: 0.024852562560251326\n",
      "STEP: 74000 - loss: 0.02484659347716611\n",
      "STEP: 74100 - loss: 0.024840627124200845\n",
      "STEP: 74200 - loss: 0.024834663489710675\n",
      "STEP: 74300 - loss: 0.024828702562181282\n",
      "STEP: 74400 - loss: 0.02482274433022676\n",
      "STEP: 74500 - loss: 0.024816788782589413\n",
      "STEP: 74600 - loss: 0.024810835908137034\n",
      "STEP: 74700 - loss: 0.024804885695862103\n",
      "STEP: 74800 - loss: 0.024798938134879912\n",
      "STEP: 74900 - loss: 0.024792993214427662\n",
      "STEP: 75000 - loss: 0.024787050923862276\n",
      "STEP: 75100 - loss: 0.024781111252659915\n",
      "STEP: 75200 - loss: 0.024775174190413647\n",
      "STEP: 75300 - loss: 0.02476923972683279\n",
      "STEP: 75400 - loss: 0.02476330785174108\n",
      "STEP: 75500 - loss: 0.024757378555075673\n",
      "STEP: 75600 - loss: 0.024751451826885502\n",
      "STEP: 75700 - loss: 0.02474552765733049\n",
      "STEP: 75800 - loss: 0.02473960603667979\n",
      "STEP: 75900 - loss: 0.02473368695530998\n",
      "STEP: 76000 - loss: 0.0247277704037053\n",
      "STEP: 76100 - loss: 0.024721856372455123\n",
      "STEP: 76200 - loss: 0.02471594485225307\n",
      "STEP: 76300 - loss: 0.024710035833895933\n",
      "STEP: 76400 - loss: 0.024704129308282245\n",
      "STEP: 76500 - loss: 0.024698225266411633\n",
      "STEP: 76600 - loss: 0.024692323699382505\n",
      "STEP: 76700 - loss: 0.024686424598392452\n",
      "STEP: 76800 - loss: 0.02468052795473569\n",
      "STEP: 76900 - loss: 0.024674633759802164\n",
      "STEP: 77000 - loss: 0.024668742005077657\n",
      "STEP: 77100 - loss: 0.0246628526821409\n",
      "STEP: 77200 - loss: 0.02465696578266389\n",
      "STEP: 77300 - loss: 0.02465108129840993\n",
      "STEP: 77400 - loss: 0.02464519922123295\n",
      "STEP: 77500 - loss: 0.0246393195430763\n",
      "STEP: 77600 - loss: 0.02463344225597186\n",
      "STEP: 77700 - loss: 0.02462756735203884\n",
      "STEP: 77800 - loss: 0.024621694823482743\n",
      "STEP: 77900 - loss: 0.024615824662594814\n",
      "STEP: 78000 - loss: 0.024609956861750115\n",
      "STEP: 78100 - loss: 0.02460409141340733\n",
      "STEP: 78200 - loss: 0.02459822831010765\n",
      "STEP: 78300 - loss: 0.024592367544473547\n",
      "STEP: 78400 - loss: 0.024586509109208034\n",
      "STEP: 78500 - loss: 0.0245806529970935\n",
      "STEP: 78600 - loss: 0.024574799200991246\n",
      "STEP: 78700 - loss: 0.02456894771383989\n",
      "STEP: 78800 - loss: 0.02456309852865481\n",
      "STEP: 78900 - loss: 0.024557251638528014\n",
      "STEP: 79000 - loss: 0.02455140703662544\n",
      "STEP: 79100 - loss: 0.02454556471618777\n",
      "STEP: 79200 - loss: 0.024539724670528614\n",
      "STEP: 79300 - loss: 0.024533886893034297\n",
      "STEP: 79400 - loss: 0.024528051377162196\n",
      "STEP: 79500 - loss: 0.024522218116440737\n",
      "STEP: 79600 - loss: 0.02451638710446823\n",
      "STEP: 79700 - loss: 0.02451055833491207\n",
      "STEP: 79800 - loss: 0.024504731801507203\n",
      "STEP: 79900 - loss: 0.024498907498057138\n",
      "STEP: 80000 - loss: 0.024493085418430947\n",
      "STEP: 80100 - loss: 0.024487265556564528\n",
      "STEP: 80200 - loss: 0.024481447906457808\n",
      "STEP: 80300 - loss: 0.02447563246217621\n",
      "STEP: 80400 - loss: 0.02446981921784799\n",
      "STEP: 80500 - loss: 0.024464008167664236\n",
      "STEP: 80600 - loss: 0.02445819930587847\n",
      "STEP: 80700 - loss: 0.02445239262680551\n",
      "STEP: 80800 - loss: 0.0244465881248207\n",
      "STEP: 80900 - loss: 0.02444078579435908\n",
      "STEP: 81000 - loss: 0.024434985629915754\n",
      "STEP: 81100 - loss: 0.024429187626043358\n",
      "STEP: 81200 - loss: 0.024423391777353302\n",
      "STEP: 81300 - loss: 0.024417598078513424\n",
      "STEP: 81400 - loss: 0.024411806524248437\n",
      "STEP: 81500 - loss: 0.02440601710933924\n",
      "STEP: 81600 - loss: 0.02440022982862109\n",
      "STEP: 81700 - loss: 0.024394444676984713\n",
      "STEP: 81800 - loss: 0.024388661649373963\n",
      "STEP: 81900 - loss: 0.024382880740786587\n",
      "STEP: 82000 - loss: 0.02437710194627275\n",
      "STEP: 82100 - loss: 0.02437132526093433\n",
      "STEP: 82200 - loss: 0.024365550679925192\n",
      "STEP: 82300 - loss: 0.024359778198450074\n",
      "STEP: 82400 - loss: 0.024354007811763268\n",
      "STEP: 82500 - loss: 0.02434823951516917\n",
      "STEP: 82600 - loss: 0.02434247330402137\n",
      "STEP: 82700 - loss: 0.02433670917372171\n",
      "STEP: 82800 - loss: 0.024330947119720038\n",
      "STEP: 82900 - loss: 0.02432518713751364\n",
      "STEP: 83000 - loss: 0.02431942922264648\n",
      "STEP: 83100 - loss: 0.024313673370708937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 83200 - loss: 0.024307919577336835\n",
      "STEP: 83300 - loss: 0.02430216783821163\n",
      "STEP: 83400 - loss: 0.024296418149058963\n",
      "STEP: 83500 - loss: 0.024290670505649068\n",
      "STEP: 83600 - loss: 0.024284924903795437\n",
      "STEP: 83700 - loss: 0.0242791813393549\n",
      "STEP: 83800 - loss: 0.024273439808226466\n",
      "STEP: 83900 - loss: 0.02426770030635201\n",
      "STEP: 84000 - loss: 0.02426196282971447\n",
      "STEP: 84100 - loss: 0.024256227374338045\n",
      "STEP: 84200 - loss: 0.024250493936287476\n",
      "STEP: 84300 - loss: 0.024244762511667885\n",
      "STEP: 84400 - loss: 0.024239033096623962\n",
      "STEP: 84500 - loss: 0.024233305687339897\n",
      "STEP: 84600 - loss: 0.024227580280038047\n",
      "STEP: 84700 - loss: 0.024221856870979698\n",
      "STEP: 84800 - loss: 0.024216135456463964\n",
      "STEP: 84900 - loss: 0.024210416032826747\n",
      "STEP: 85000 - loss: 0.024204698596441792\n",
      "STEP: 85100 - loss: 0.024198983143719204\n",
      "STEP: 85200 - loss: 0.0241932696711046\n",
      "STEP: 85300 - loss: 0.02418755817508023\n",
      "STEP: 85400 - loss: 0.024181848652163034\n",
      "STEP: 85500 - loss: 0.024176141098905035\n",
      "STEP: 85600 - loss: 0.024170435511892522\n",
      "STEP: 85700 - loss: 0.02416473188774625\n",
      "STEP: 85800 - loss: 0.024159030223120232\n",
      "STEP: 85900 - loss: 0.02415333051470233\n",
      "STEP: 86000 - loss: 0.024147632759212678\n",
      "STEP: 86100 - loss: 0.024141936953404202\n",
      "STEP: 86200 - loss: 0.024136243094061934\n",
      "STEP: 86300 - loss: 0.024130551178002915\n",
      "STEP: 86400 - loss: 0.024124861202075118\n",
      "STEP: 86500 - loss: 0.02411917316315764\n",
      "STEP: 86600 - loss: 0.02411348705816046\n",
      "STEP: 86700 - loss: 0.024107802884023977\n",
      "STEP: 86800 - loss: 0.02410212063771784\n",
      "STEP: 86900 - loss: 0.024096440316242036\n",
      "STEP: 87000 - loss: 0.024090761916625524\n",
      "STEP: 87100 - loss: 0.024085085435926072\n",
      "STEP: 87200 - loss: 0.02407941087122998\n",
      "STEP: 87300 - loss: 0.024073738219652058\n",
      "STEP: 87400 - loss: 0.02406806747833492\n",
      "STEP: 87500 - loss: 0.02406239864444843\n",
      "STEP: 87600 - loss: 0.024056731715190315\n",
      "STEP: 87700 - loss: 0.024051066687784783\n",
      "STEP: 87800 - loss: 0.024045403559482727\n",
      "STEP: 87900 - loss: 0.024039742327561404\n",
      "STEP: 88000 - loss: 0.02403408298932418\n",
      "STEP: 88100 - loss: 0.024028425542100253\n",
      "STEP: 88200 - loss: 0.024022769983243666\n",
      "STEP: 88300 - loss: 0.024017116310134205\n",
      "STEP: 88400 - loss: 0.02401146452017589\n",
      "STEP: 88500 - loss: 0.024005814610797805\n",
      "STEP: 88600 - loss: 0.024000166579453103\n",
      "STEP: 88700 - loss: 0.02399452042361864\n",
      "STEP: 88800 - loss: 0.023988876140795118\n",
      "STEP: 88900 - loss: 0.02398323372850681\n",
      "STEP: 89000 - loss: 0.023977593184300938\n",
      "STEP: 89100 - loss: 0.023971954505747503\n",
      "STEP: 89200 - loss: 0.023966317690439352\n",
      "STEP: 89300 - loss: 0.023960682735991244\n",
      "STEP: 89400 - loss: 0.023955049640040645\n",
      "STEP: 89500 - loss: 0.023949418400246537\n",
      "STEP: 89600 - loss: 0.023943789014288695\n",
      "STEP: 89700 - loss: 0.023938161479869595\n",
      "STEP: 89800 - loss: 0.023932535794711773\n",
      "STEP: 89900 - loss: 0.023926911956558876\n",
      "STEP: 90000 - loss: 0.02392128996317519\n",
      "STEP: 90100 - loss: 0.023915669812345282\n",
      "STEP: 90200 - loss: 0.023910051501873638\n",
      "STEP: 90300 - loss: 0.023904435029584916\n",
      "STEP: 90400 - loss: 0.02389882039332302\n",
      "STEP: 90500 - loss: 0.023893207590951714\n",
      "STEP: 90600 - loss: 0.023887596620353814\n",
      "STEP: 90700 - loss: 0.02388198747943057\n",
      "STEP: 90800 - loss: 0.02387638016610234\n",
      "STEP: 90900 - loss: 0.023870774678308563\n",
      "STEP: 91000 - loss: 0.023865171014005856\n",
      "STEP: 91100 - loss: 0.023859569171169666\n",
      "STEP: 91200 - loss: 0.023853969147793266\n",
      "STEP: 91300 - loss: 0.023848370941887197\n",
      "STEP: 91400 - loss: 0.023842774551479953\n",
      "STEP: 91500 - loss: 0.023837179974616974\n",
      "STEP: 91600 - loss: 0.023831587209360724\n",
      "STEP: 91700 - loss: 0.02382599625379063\n",
      "STEP: 91800 - loss: 0.023820407106003076\n",
      "STEP: 91900 - loss: 0.02381481976411006\n",
      "STEP: 92000 - loss: 0.023809234226241146\n",
      "STEP: 92100 - loss: 0.02380365049054057\n",
      "STEP: 92200 - loss: 0.023798068555169624\n",
      "STEP: 92300 - loss: 0.023792488418304312\n",
      "STEP: 92400 - loss: 0.023786910078137144\n",
      "STEP: 92500 - loss: 0.023781333532875332\n",
      "STEP: 92600 - loss: 0.023775758780741656\n",
      "STEP: 92700 - loss: 0.02377018581997347\n",
      "STEP: 92800 - loss: 0.023764614648823137\n",
      "STEP: 92900 - loss: 0.023759045265557714\n",
      "STEP: 93000 - loss: 0.023753477668458957\n",
      "STEP: 93100 - loss: 0.02374791185582237\n",
      "STEP: 93200 - loss: 0.023742347825958222\n",
      "STEP: 93300 - loss: 0.023736785577190493\n",
      "STEP: 93400 - loss: 0.023731225107856694\n",
      "STEP: 93500 - loss: 0.023725666416308397\n",
      "STEP: 93600 - loss: 0.023720109500910493\n",
      "STEP: 93700 - loss: 0.023714554360041232\n",
      "STEP: 93800 - loss: 0.02370900099209234\n",
      "STEP: 93900 - loss: 0.023703449395467997\n",
      "STEP: 94000 - loss: 0.023697899568585725\n",
      "STEP: 94100 - loss: 0.02369235150987556\n",
      "STEP: 94200 - loss: 0.023686805217780188\n",
      "STEP: 94300 - loss: 0.02368126069075508\n",
      "STEP: 94400 - loss: 0.023675717927267158\n",
      "STEP: 94500 - loss: 0.023670176925796576\n",
      "STEP: 94600 - loss: 0.023664637684834438\n",
      "STEP: 94700 - loss: 0.02365910020288433\n",
      "STEP: 94800 - loss: 0.023653564478461835\n",
      "STEP: 94900 - loss: 0.023648030510093235\n",
      "STEP: 95000 - loss: 0.023642498296317437\n",
      "STEP: 95100 - loss: 0.023636967835683306\n",
      "STEP: 95200 - loss: 0.023631439126752266\n",
      "STEP: 95300 - loss: 0.023625912168095838\n",
      "STEP: 95400 - loss: 0.02362038695829709\n",
      "STEP: 95500 - loss: 0.02361486349594949\n",
      "STEP: 95600 - loss: 0.023609341779657338\n",
      "STEP: 95700 - loss: 0.023603821808035875\n",
      "STEP: 95800 - loss: 0.023598303579710083\n",
      "STEP: 95900 - loss: 0.023592787093315817\n",
      "STEP: 96000 - loss: 0.023587272347498734\n",
      "STEP: 96100 - loss: 0.023581759340915186\n",
      "STEP: 96200 - loss: 0.023576248072230844\n",
      "STEP: 96300 - loss: 0.023570738540121473\n",
      "STEP: 96400 - loss: 0.02356523074327261\n",
      "STEP: 96500 - loss: 0.023559724680379632\n",
      "STEP: 96600 - loss: 0.02355422035014701\n",
      "STEP: 96700 - loss: 0.02354871775128901\n",
      "STEP: 96800 - loss: 0.02354321688252895\n",
      "STEP: 96900 - loss: 0.02353771774259916\n",
      "STEP: 97000 - loss: 0.023532220330241528\n",
      "STEP: 97100 - loss: 0.023526724644206704\n",
      "STEP: 97200 - loss: 0.02352123068325418\n",
      "STEP: 97300 - loss: 0.023515738446152245\n",
      "STEP: 97400 - loss: 0.023510247931677965\n",
      "STEP: 97500 - loss: 0.023504759138616566\n",
      "STEP: 97600 - loss: 0.023499272065762467\n",
      "STEP: 97700 - loss: 0.0234937867119178\n",
      "STEP: 97800 - loss: 0.023488303075893148\n",
      "STEP: 97900 - loss: 0.023482821156507572\n",
      "STEP: 98000 - loss: 0.02347734095258807\n",
      "STEP: 98100 - loss: 0.0234718624629693\n",
      "STEP: 98200 - loss: 0.023466385686494416\n",
      "STEP: 98300 - loss: 0.023460910622013848\n",
      "STEP: 98400 - loss: 0.0234554372683859\n",
      "STEP: 98500 - loss: 0.023449965624476956\n",
      "STEP: 98600 - loss: 0.023444495689160384\n",
      "STEP: 98700 - loss: 0.02343902746131716\n",
      "STEP: 98800 - loss: 0.023433560939835663\n",
      "STEP: 98900 - loss: 0.023428096123611637\n",
      "STEP: 99000 - loss: 0.023422633011548077\n",
      "STEP: 99100 - loss: 0.023417171602554764\n",
      "STEP: 99200 - loss: 0.02341171189554917\n",
      "STEP: 99300 - loss: 0.023406253889454727\n",
      "STEP: 99400 - loss: 0.0234007975832028\n",
      "STEP: 99500 - loss: 0.023395342975730926\n",
      "STEP: 99600 - loss: 0.02338989006598332\n",
      "STEP: 99700 - loss: 0.023384438852911422\n",
      "STEP: 99800 - loss: 0.023378989335472663\n",
      "STEP: 99900 - loss: 0.02337354151263129\n",
      "STEP: 100000 - loss: 0.023368095383357858\n",
      "STEP: 100100 - loss: 0.023362650946628944\n",
      "STEP: 100200 - loss: 0.023357208201428066\n",
      "STEP: 100300 - loss: 0.02335176714674451\n",
      "STEP: 100400 - loss: 0.0233463277815738\n",
      "STEP: 100500 - loss: 0.023340890104917345\n",
      "STEP: 100600 - loss: 0.02333545411578271\n",
      "STEP: 100700 - loss: 0.023330019813183674\n",
      "STEP: 100800 - loss: 0.023324587196139224\n",
      "STEP: 100900 - loss: 0.023319156263674708\n",
      "STEP: 101000 - loss: 0.023313727014820804\n",
      "STEP: 101100 - loss: 0.023308299448614025\n",
      "STEP: 101200 - loss: 0.023302873564096632\n",
      "STEP: 101300 - loss: 0.02329744936031596\n",
      "STEP: 101400 - loss: 0.02329202683632535\n",
      "STEP: 101500 - loss: 0.023286605991183065\n",
      "STEP: 101600 - loss: 0.023281186823953005\n",
      "STEP: 101700 - loss: 0.02327576933370431\n",
      "STEP: 101800 - loss: 0.023270353519511282\n",
      "STEP: 101900 - loss: 0.02326493938045333\n",
      "STEP: 102000 - loss: 0.023259526915615225\n",
      "STEP: 102100 - loss: 0.02325411612408625\n",
      "STEP: 102200 - loss: 0.023248707004961292\n",
      "STEP: 102300 - loss: 0.023243299557339628\n",
      "STEP: 102400 - loss: 0.023237893780326\n",
      "STEP: 102500 - loss: 0.023232489673029523\n",
      "STEP: 102600 - loss: 0.02322708723456384\n",
      "STEP: 102700 - loss: 0.023221686464048083\n",
      "STEP: 102800 - loss: 0.02321628736060539\n",
      "STEP: 102900 - loss: 0.02321088992336357\n",
      "STEP: 103000 - loss: 0.023205494151455442\n",
      "STEP: 103100 - loss: 0.023200100044017756\n",
      "STEP: 103200 - loss: 0.023194707600192117\n",
      "STEP: 103300 - loss: 0.02318931681912409\n",
      "STEP: 103400 - loss: 0.023183927699964087\n",
      "STEP: 103500 - loss: 0.023178540241866562\n",
      "STEP: 103600 - loss: 0.023173154443990256\n",
      "STEP: 103700 - loss: 0.023167770305497805\n",
      "STEP: 103800 - loss: 0.023162387825556675\n",
      "STEP: 103900 - loss: 0.023157007003337757\n",
      "STEP: 104000 - loss: 0.02315162783801661\n",
      "STEP: 104100 - loss: 0.023146250328772103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 104200 - loss: 0.02314087447478768\n",
      "STEP: 104300 - loss: 0.023135500275250385\n",
      "STEP: 104400 - loss: 0.0231301277293513\n",
      "STEP: 104500 - loss: 0.023124756836285343\n",
      "STEP: 104600 - loss: 0.02311938759525092\n",
      "STEP: 104700 - loss: 0.023114020005450785\n",
      "STEP: 104800 - loss: 0.02310865406609058\n",
      "STEP: 104900 - loss: 0.023103289776380397\n",
      "STEP: 105000 - loss: 0.023097927135533658\n",
      "STEP: 105100 - loss: 0.023092566142766977\n",
      "STEP: 105200 - loss: 0.023087206797301273\n",
      "STEP: 105300 - loss: 0.0230818490983604\n",
      "STEP: 105400 - loss: 0.023076493045171675\n",
      "STEP: 105500 - loss: 0.0230711386369661\n",
      "STEP: 105600 - loss: 0.02306578587297812\n",
      "STEP: 105700 - loss: 0.0230604347524452\n",
      "STEP: 105800 - loss: 0.02305508527460818\n",
      "STEP: 105900 - loss: 0.02304973743871139\n",
      "STEP: 106000 - loss: 0.023044391244002364\n",
      "STEP: 106100 - loss: 0.023039046689731744\n",
      "STEP: 106200 - loss: 0.023033703775153246\n",
      "STEP: 106300 - loss: 0.023028362499523582\n",
      "STEP: 106400 - loss: 0.023023022862103417\n",
      "STEP: 106500 - loss: 0.02301768486215509\n",
      "STEP: 106600 - loss: 0.023012348498945283\n",
      "STEP: 106700 - loss: 0.02300701377174272\n",
      "STEP: 106800 - loss: 0.023001680679819746\n",
      "STEP: 106900 - loss: 0.022996349222451234\n",
      "STEP: 107000 - loss: 0.02299101939891497\n",
      "STEP: 107100 - loss: 0.02298569120849194\n",
      "STEP: 107200 - loss: 0.022980364650465372\n",
      "STEP: 107300 - loss: 0.022975039724121814\n",
      "STEP: 107400 - loss: 0.02296971642875046\n",
      "STEP: 107500 - loss: 0.02296439476364306\n",
      "STEP: 107600 - loss: 0.022959074728094046\n",
      "STEP: 107700 - loss: 0.02295375632140092\n",
      "STEP: 107800 - loss: 0.022948439542863498\n",
      "STEP: 107900 - loss: 0.02294312439178422\n",
      "STEP: 108000 - loss: 0.022937810867468125\n",
      "STEP: 108100 - loss: 0.02293249896922294\n",
      "STEP: 108200 - loss: 0.022927188696358767\n",
      "STEP: 108300 - loss: 0.022921880048188306\n",
      "STEP: 108400 - loss: 0.022916573024026776\n",
      "STEP: 108500 - loss: 0.02291126762319151\n",
      "STEP: 108600 - loss: 0.0229059638450028\n",
      "STEP: 108700 - loss: 0.02290066168878283\n",
      "STEP: 108800 - loss: 0.022895361153856632\n",
      "STEP: 108900 - loss: 0.02289006223955089\n",
      "STEP: 109000 - loss: 0.02288476494519521\n",
      "STEP: 109100 - loss: 0.022879469270121765\n",
      "STEP: 109200 - loss: 0.022874175213663715\n",
      "STEP: 109300 - loss: 0.022868882775157737\n",
      "STEP: 109400 - loss: 0.022863591953942306\n",
      "STEP: 109500 - loss: 0.022858302749357746\n",
      "STEP: 109600 - loss: 0.02285301516074698\n",
      "STEP: 109700 - loss: 0.022847729187454972\n",
      "STEP: 109800 - loss: 0.022842444828828804\n",
      "STEP: 109900 - loss: 0.02283716208421733\n",
      "STEP: 110000 - loss: 0.02283188095297202\n",
      "STEP: 110100 - loss: 0.022826601434445946\n",
      "STEP: 110200 - loss: 0.022821323527994656\n",
      "STEP: 110300 - loss: 0.022816047232975176\n",
      "STEP: 110400 - loss: 0.02281077254874693\n",
      "STEP: 110500 - loss: 0.022805499474670844\n",
      "STEP: 110600 - loss: 0.022800228010110467\n",
      "STEP: 110700 - loss: 0.022794958154430663\n",
      "STEP: 110800 - loss: 0.02278968990699861\n",
      "STEP: 110900 - loss: 0.022784423267182906\n",
      "STEP: 111000 - loss: 0.02277915823435452\n",
      "STEP: 111100 - loss: 0.02277389480788604\n",
      "STEP: 111200 - loss: 0.022768632987151524\n",
      "STEP: 111300 - loss: 0.02276337277152731\n",
      "STEP: 111400 - loss: 0.022758114160391515\n",
      "STEP: 111500 - loss: 0.02275285715312359\n",
      "STEP: 111600 - loss: 0.02274760174910532\n",
      "STEP: 111700 - loss: 0.02274234794771951\n",
      "STEP: 111800 - loss: 0.022737095748351234\n",
      "STEP: 111900 - loss: 0.022731845150387057\n",
      "STEP: 112000 - loss: 0.022726596153215126\n",
      "STEP: 112100 - loss: 0.022721348756225387\n",
      "STEP: 112200 - loss: 0.022716102958809513\n",
      "STEP: 112300 - loss: 0.02271085876036028\n",
      "STEP: 112400 - loss: 0.022705616160272533\n",
      "STEP: 112500 - loss: 0.022700375157942647\n",
      "STEP: 112600 - loss: 0.022695135752768415\n",
      "STEP: 112700 - loss: 0.022689897944149388\n",
      "STEP: 112800 - loss: 0.022684661731486165\n",
      "STEP: 112900 - loss: 0.022679427114181683\n",
      "STEP: 113000 - loss: 0.022674194091639333\n",
      "STEP: 113100 - loss: 0.02266896266326503\n",
      "STEP: 113200 - loss: 0.02266373282846543\n",
      "STEP: 113300 - loss: 0.02265850458664886\n",
      "STEP: 113400 - loss: 0.02265327793722512\n",
      "STEP: 113500 - loss: 0.022648052879605453\n",
      "STEP: 113600 - loss: 0.02264282941320249\n",
      "STEP: 113700 - loss: 0.022637607537430243\n",
      "STEP: 113800 - loss: 0.022632387251703856\n",
      "STEP: 113900 - loss: 0.022627168555440266\n",
      "STEP: 114000 - loss: 0.02262195144805741\n",
      "STEP: 114100 - loss: 0.022616735928974847\n",
      "STEP: 114200 - loss: 0.022611521997612967\n",
      "STEP: 114300 - loss: 0.022606309653394236\n",
      "STEP: 114400 - loss: 0.022601098895741793\n",
      "STEP: 114500 - loss: 0.02259588972408009\n",
      "STEP: 114600 - loss: 0.022590682137835237\n",
      "STEP: 114700 - loss: 0.022585476136434086\n",
      "STEP: 114800 - loss: 0.022580271719305062\n",
      "STEP: 114900 - loss: 0.022575068885877795\n",
      "STEP: 115000 - loss: 0.02256986763558328\n",
      "STEP: 115100 - loss: 0.02256466796785302\n",
      "STEP: 115200 - loss: 0.02255946988212036\n",
      "STEP: 115300 - loss: 0.02255427337781988\n",
      "STEP: 115400 - loss: 0.022549078454386893\n",
      "STEP: 115500 - loss: 0.022543885111257943\n",
      "STEP: 115600 - loss: 0.022538693347870908\n",
      "STEP: 115700 - loss: 0.022533503163664904\n",
      "STEP: 115800 - loss: 0.022528314558079736\n",
      "STEP: 115900 - loss: 0.022523127530556734\n",
      "STEP: 116000 - loss: 0.022517942080538\n",
      "STEP: 116100 - loss: 0.02251275820746699\n",
      "STEP: 116200 - loss: 0.022507575910787718\n",
      "STEP: 116300 - loss: 0.022502395189945987\n",
      "STEP: 116400 - loss: 0.022497216044388157\n",
      "STEP: 116500 - loss: 0.022492038473561904\n",
      "STEP: 116600 - loss: 0.022486862476915635\n",
      "STEP: 116700 - loss: 0.022481688053898993\n",
      "STEP: 116800 - loss: 0.02247651520396255\n",
      "STEP: 116900 - loss: 0.022471343926557998\n",
      "STEP: 117000 - loss: 0.022466174221137704\n",
      "STEP: 117100 - loss: 0.022461006087155257\n",
      "STEP: 117200 - loss: 0.022455839524065224\n",
      "STEP: 117300 - loss: 0.022450674531323118\n",
      "STEP: 117400 - loss: 0.02244551110838548\n",
      "STEP: 117500 - loss: 0.022440349254709342\n",
      "STEP: 117600 - loss: 0.02243518896975317\n",
      "STEP: 117700 - loss: 0.02243003025297628\n",
      "STEP: 117800 - loss: 0.022424873103838675\n",
      "STEP: 117900 - loss: 0.022419717521801146\n",
      "STEP: 118000 - loss: 0.022414563506325928\n",
      "STEP: 118100 - loss: 0.022409411056875637\n",
      "STEP: 118200 - loss: 0.022404260172914255\n",
      "STEP: 118300 - loss: 0.022399110853905807\n",
      "STEP: 118400 - loss: 0.02239396309931625\n",
      "STEP: 118500 - loss: 0.022388816908611376\n",
      "STEP: 118600 - loss: 0.02238367228125858\n",
      "STEP: 118700 - loss: 0.022378529216725427\n",
      "STEP: 118800 - loss: 0.022373387714481028\n",
      "STEP: 118900 - loss: 0.022368247773994807\n",
      "STEP: 119000 - loss: 0.022363109394737124\n",
      "STEP: 119100 - loss: 0.022357972576179073\n",
      "STEP: 119200 - loss: 0.022352837317792636\n",
      "STEP: 119300 - loss: 0.022347703619050786\n",
      "STEP: 119400 - loss: 0.022342571479426775\n",
      "STEP: 119500 - loss: 0.02233744089839509\n",
      "STEP: 119600 - loss: 0.0223323118754307\n",
      "STEP: 119700 - loss: 0.022327184410009356\n",
      "STEP: 119800 - loss: 0.02232205850160771\n",
      "STEP: 119900 - loss: 0.022316934149703267\n",
      "STEP: 120000 - loss: 0.02231181135377362\n",
      "STEP: 120100 - loss: 0.022306690113298007\n",
      "STEP: 120200 - loss: 0.02230157042775559\n",
      "STEP: 120300 - loss: 0.022296452296626648\n",
      "STEP: 120400 - loss: 0.022291335719392062\n",
      "STEP: 120500 - loss: 0.022286220695533706\n",
      "STEP: 120600 - loss: 0.02228110722453353\n",
      "STEP: 120700 - loss: 0.02227599530587491\n",
      "STEP: 120800 - loss: 0.022270884939041065\n",
      "STEP: 120900 - loss: 0.02226577612351651\n",
      "STEP: 121000 - loss: 0.022260668858786473\n",
      "STEP: 121100 - loss: 0.02225556314433651\n",
      "STEP: 121200 - loss: 0.022250458979652783\n",
      "STEP: 121300 - loss: 0.022245356364222552\n",
      "STEP: 121400 - loss: 0.022240255297533363\n",
      "STEP: 121500 - loss: 0.022235155779073264\n",
      "STEP: 121600 - loss: 0.02223005780833142\n",
      "STEP: 121700 - loss: 0.02222496138479719\n",
      "STEP: 121800 - loss: 0.022219866507960918\n",
      "STEP: 121900 - loss: 0.022214773177313126\n",
      "STEP: 122000 - loss: 0.022209681392345255\n",
      "STEP: 122100 - loss: 0.022204591152549403\n",
      "STEP: 122200 - loss: 0.022199502457418113\n",
      "STEP: 122300 - loss: 0.022194415306444303\n",
      "STEP: 122400 - loss: 0.02218932969912186\n",
      "STEP: 122500 - loss: 0.022184245634945414\n",
      "STEP: 122600 - loss: 0.02217916311340932\n",
      "STEP: 122700 - loss: 0.022174082134009662\n",
      "STEP: 122800 - loss: 0.022169002696242106\n",
      "STEP: 122900 - loss: 0.022163924799603163\n",
      "STEP: 123000 - loss: 0.022158848443590444\n",
      "STEP: 123100 - loss: 0.02215377362770131\n",
      "STEP: 123200 - loss: 0.022148700351434167\n",
      "STEP: 123300 - loss: 0.022143628614287716\n",
      "STEP: 123400 - loss: 0.022138558415761445\n",
      "STEP: 123500 - loss: 0.0221334897553551\n",
      "STEP: 123600 - loss: 0.02212842263256924\n",
      "STEP: 123700 - loss: 0.02212335704690468\n",
      "STEP: 123800 - loss: 0.0221182929978629\n",
      "STEP: 123900 - loss: 0.022113230484945935\n",
      "STEP: 124000 - loss: 0.02210816950765612\n",
      "STEP: 124100 - loss: 0.02210311006549664\n",
      "STEP: 124200 - loss: 0.02209805215797079\n",
      "STEP: 124300 - loss: 0.022092995784582535\n",
      "STEP: 124400 - loss: 0.02208794094483659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 124500 - loss: 0.022082887638237757\n",
      "STEP: 124600 - loss: 0.022077835864291503\n",
      "STEP: 124700 - loss: 0.022072785622503734\n",
      "STEP: 124800 - loss: 0.02206773691238078\n",
      "STEP: 124900 - loss: 0.0220626897334297\n",
      "STEP: 125000 - loss: 0.022057644085157816\n",
      "STEP: 125100 - loss: 0.022052599967072883\n",
      "STEP: 125200 - loss: 0.022047557378683087\n",
      "STEP: 125300 - loss: 0.022042516319497295\n",
      "STEP: 125400 - loss: 0.02203747678902461\n",
      "STEP: 125500 - loss: 0.022032438786774612\n",
      "STEP: 125600 - loss: 0.02202740231225776\n",
      "STEP: 125700 - loss: 0.022022367364984096\n",
      "STEP: 125800 - loss: 0.022017333944464746\n",
      "STEP: 125900 - loss: 0.02201230205021126\n",
      "STEP: 126000 - loss: 0.022007271681735323\n",
      "STEP: 126100 - loss: 0.02200224283854909\n",
      "STEP: 126200 - loss: 0.0219972155201656\n",
      "STEP: 126300 - loss: 0.02199218972609757\n",
      "STEP: 126400 - loss: 0.021987165455858826\n",
      "STEP: 126500 - loss: 0.021982142708963023\n",
      "STEP: 126600 - loss: 0.02197712148492487\n",
      "STEP: 126700 - loss: 0.02197210178325855\n",
      "STEP: 126800 - loss: 0.02196708360347988\n",
      "STEP: 126900 - loss: 0.0219620669451042\n",
      "STEP: 127000 - loss: 0.02195705180764727\n",
      "STEP: 127100 - loss: 0.021952038190625786\n",
      "STEP: 127200 - loss: 0.02194702609355612\n",
      "STEP: 127300 - loss: 0.021942015515955628\n",
      "STEP: 127400 - loss: 0.02193700645734194\n",
      "STEP: 127500 - loss: 0.021931998917232735\n",
      "STEP: 127600 - loss: 0.021926992895146616\n",
      "STEP: 127700 - loss: 0.021921988390601978\n",
      "STEP: 127800 - loss: 0.0219169854031181\n",
      "STEP: 127900 - loss: 0.02191198393221418\n",
      "STEP: 128000 - loss: 0.021906983977410203\n",
      "STEP: 128100 - loss: 0.021901985538226167\n",
      "STEP: 128200 - loss: 0.02189698861418262\n",
      "STEP: 128300 - loss: 0.02189199320480078\n",
      "STEP: 128400 - loss: 0.021886999309601324\n",
      "STEP: 128500 - loss: 0.021882006928106323\n",
      "STEP: 128600 - loss: 0.021877016059837714\n",
      "STEP: 128700 - loss: 0.021872026704317475\n",
      "STEP: 128800 - loss: 0.0218670388610684\n",
      "STEP: 128900 - loss: 0.021862052529613617\n",
      "STEP: 129000 - loss: 0.021857067709476524\n",
      "STEP: 129100 - loss: 0.0218520844001805\n",
      "STEP: 129200 - loss: 0.021847102601249987\n",
      "STEP: 129300 - loss: 0.021842122312209056\n",
      "STEP: 129400 - loss: 0.021837143532582343\n",
      "STEP: 129500 - loss: 0.021832166261894912\n",
      "STEP: 129600 - loss: 0.021827190499672668\n",
      "STEP: 129700 - loss: 0.021822216245440562\n",
      "STEP: 129800 - loss: 0.021817243498724867\n",
      "STEP: 129900 - loss: 0.02181227225905214\n",
      "STEP: 130000 - loss: 0.021807302525948834\n",
      "STEP: 130100 - loss: 0.021802334298941982\n",
      "STEP: 130200 - loss: 0.021797367577558676\n",
      "STEP: 130300 - loss: 0.021792402361326818\n",
      "STEP: 130400 - loss: 0.021787438649774327\n",
      "STEP: 130500 - loss: 0.02178247644242908\n",
      "STEP: 130600 - loss: 0.021777515738820034\n",
      "STEP: 130700 - loss: 0.021772556538475756\n",
      "STEP: 130800 - loss: 0.02176759884092572\n",
      "STEP: 130900 - loss: 0.021762642645698837\n",
      "STEP: 131000 - loss: 0.02175768795232538\n",
      "STEP: 131100 - loss: 0.021752734760335114\n",
      "STEP: 131200 - loss: 0.021747783069258255\n",
      "STEP: 131300 - loss: 0.021742832878625978\n",
      "STEP: 131400 - loss: 0.021737884187968702\n",
      "STEP: 131500 - loss: 0.02173293699681792\n",
      "STEP: 131600 - loss: 0.02172799130470503\n",
      "STEP: 131700 - loss: 0.021723047111161846\n",
      "STEP: 131800 - loss: 0.02171810441572038\n",
      "STEP: 131900 - loss: 0.021713163217913077\n",
      "STEP: 132000 - loss: 0.02170822351727261\n",
      "STEP: 132100 - loss: 0.021703285313332116\n",
      "STEP: 132200 - loss: 0.021698348605624248\n",
      "STEP: 132300 - loss: 0.021693413393683046\n",
      "STEP: 132400 - loss: 0.021688479677041923\n",
      "STEP: 132500 - loss: 0.021683547455235035\n",
      "STEP: 132600 - loss: 0.021678616727796786\n",
      "STEP: 132700 - loss: 0.02167368749426152\n",
      "STEP: 132800 - loss: 0.021668759754164244\n",
      "STEP: 132900 - loss: 0.02166383350704012\n",
      "STEP: 133000 - loss: 0.021658908752424277\n",
      "STEP: 133100 - loss: 0.021653985489852638\n",
      "STEP: 133200 - loss: 0.021649063718861057\n",
      "STEP: 133300 - loss: 0.02164414343898542\n",
      "STEP: 133400 - loss: 0.021639224649762423\n",
      "STEP: 133500 - loss: 0.021634307350728688\n",
      "STEP: 133600 - loss: 0.021629391541421135\n",
      "STEP: 133700 - loss: 0.02162447722137689\n",
      "STEP: 133800 - loss: 0.021619564390133555\n",
      "STEP: 133900 - loss: 0.021614653047228732\n",
      "STEP: 134000 - loss: 0.021609743192200347\n",
      "STEP: 134100 - loss: 0.021604834824586656\n",
      "STEP: 134200 - loss: 0.02159992794392595\n",
      "STEP: 134300 - loss: 0.02159502254975726\n",
      "STEP: 134400 - loss: 0.02159011864161921\n",
      "STEP: 134500 - loss: 0.021585216219051042\n",
      "STEP: 134600 - loss: 0.02158031528159206\n",
      "STEP: 134700 - loss: 0.021575415828782387\n",
      "STEP: 134800 - loss: 0.021570517860161547\n",
      "STEP: 134900 - loss: 0.02156562137526966\n",
      "STEP: 135000 - loss: 0.021560726373647386\n",
      "STEP: 135100 - loss: 0.02155583285483501\n",
      "STEP: 135200 - loss: 0.021550940818373648\n",
      "STEP: 135300 - loss: 0.021546050263804302\n",
      "STEP: 135400 - loss: 0.021541161190668272\n",
      "STEP: 135500 - loss: 0.021536273598507225\n",
      "STEP: 135600 - loss: 0.021531387486862657\n",
      "STEP: 135700 - loss: 0.021526502855276873\n",
      "STEP: 135800 - loss: 0.02152161970329193\n",
      "STEP: 135900 - loss: 0.021516738030450566\n",
      "STEP: 136000 - loss: 0.02151185783629512\n",
      "STEP: 136100 - loss: 0.021506979120369\n",
      "STEP: 136200 - loss: 0.021502101882214554\n",
      "STEP: 136300 - loss: 0.021497226121375994\n",
      "STEP: 136400 - loss: 0.02149235183739652\n",
      "STEP: 136500 - loss: 0.021487479029820043\n",
      "STEP: 136600 - loss: 0.021482607698190475\n",
      "STEP: 136700 - loss: 0.021477737842052\n",
      "STEP: 136800 - loss: 0.02147286946094937\n",
      "STEP: 136900 - loss: 0.0214680025544269\n",
      "STEP: 137000 - loss: 0.021463137122029817\n",
      "STEP: 137100 - loss: 0.021458273163303322\n",
      "STEP: 137200 - loss: 0.02145341067779226\n",
      "STEP: 137300 - loss: 0.021448549665042487\n",
      "STEP: 137400 - loss: 0.02144369012459981\n",
      "STEP: 137500 - loss: 0.021438832056010024\n",
      "STEP: 137600 - loss: 0.021433975458819372\n",
      "STEP: 137700 - loss: 0.021429120332574467\n",
      "STEP: 137800 - loss: 0.021424266676821548\n",
      "STEP: 137900 - loss: 0.021419414491107604\n",
      "STEP: 138000 - loss: 0.02141456377497946\n",
      "STEP: 138100 - loss: 0.021409714527984514\n",
      "STEP: 138200 - loss: 0.021404866749670175\n",
      "STEP: 138300 - loss: 0.021400020439584073\n",
      "STEP: 138400 - loss: 0.021395175597273917\n",
      "STEP: 138500 - loss: 0.021390332222287863\n",
      "STEP: 138600 - loss: 0.021385490314173847\n",
      "STEP: 138700 - loss: 0.021380649872480888\n",
      "STEP: 138800 - loss: 0.021375810896756917\n",
      "STEP: 138900 - loss: 0.021370973386551202\n",
      "STEP: 139000 - loss: 0.021366137341412756\n",
      "STEP: 139100 - loss: 0.021361302760890693\n",
      "STEP: 139200 - loss: 0.02135646964453444\n",
      "STEP: 139300 - loss: 0.021351637991893664\n",
      "STEP: 139400 - loss: 0.02134680780251806\n",
      "STEP: 139500 - loss: 0.021341979075957744\n",
      "STEP: 139600 - loss: 0.021337151811762774\n",
      "STEP: 139700 - loss: 0.02133232600948379\n",
      "STEP: 139800 - loss: 0.021327501668671162\n",
      "STEP: 139900 - loss: 0.02132267878887588\n",
      "STEP: 140000 - loss: 0.021317857369648745\n",
      "STEP: 140100 - loss: 0.021313037410540765\n",
      "STEP: 140200 - loss: 0.021308218911103745\n",
      "STEP: 140300 - loss: 0.021303401870888663\n",
      "STEP: 140400 - loss: 0.021298586289447745\n",
      "STEP: 140500 - loss: 0.021293772166332624\n",
      "STEP: 140600 - loss: 0.02128895950109537\n",
      "STEP: 140700 - loss: 0.02128414829328843\n",
      "STEP: 140800 - loss: 0.02127933854246427\n",
      "STEP: 140900 - loss: 0.021274530248175366\n",
      "STEP: 141000 - loss: 0.021269723409974746\n",
      "STEP: 141100 - loss: 0.021264918027415418\n",
      "STEP: 141200 - loss: 0.021260114100050596\n",
      "STEP: 141300 - loss: 0.021255311627433302\n",
      "STEP: 141400 - loss: 0.02125051060911782\n",
      "STEP: 141500 - loss: 0.021245711044657275\n",
      "STEP: 141600 - loss: 0.021240912933605725\n",
      "STEP: 141700 - loss: 0.021236116275517542\n",
      "STEP: 141800 - loss: 0.021231321069946758\n",
      "STEP: 141900 - loss: 0.02122652731644802\n",
      "STEP: 142000 - loss: 0.02122173501457582\n",
      "STEP: 142100 - loss: 0.021216944163885077\n",
      "STEP: 142200 - loss: 0.021212154763930822\n",
      "STEP: 142300 - loss: 0.021207366814268165\n",
      "STEP: 142400 - loss: 0.02120258031445254\n",
      "STEP: 142500 - loss: 0.02119779526403934\n",
      "STEP: 142600 - loss: 0.021193011662584547\n",
      "STEP: 142700 - loss: 0.02118822950964369\n",
      "STEP: 142800 - loss: 0.021183448804773172\n",
      "STEP: 142900 - loss: 0.021178669547528856\n",
      "STEP: 143000 - loss: 0.021173891737467705\n",
      "STEP: 143100 - loss: 0.021169115374145696\n",
      "STEP: 143200 - loss: 0.021164340457119825\n",
      "STEP: 143300 - loss: 0.021159566985947006\n",
      "STEP: 143400 - loss: 0.02115479496018421\n",
      "STEP: 143500 - loss: 0.02115002437938923\n",
      "STEP: 143600 - loss: 0.021145255243118638\n",
      "STEP: 143700 - loss: 0.02114048755093072\n",
      "STEP: 143800 - loss: 0.021135721302383066\n",
      "STEP: 143900 - loss: 0.02113095649703342\n",
      "STEP: 144000 - loss: 0.021126193134440294\n",
      "STEP: 144100 - loss: 0.021121431214161372\n",
      "STEP: 144200 - loss: 0.021116670735755697\n",
      "STEP: 144300 - loss: 0.021111911698781398\n",
      "STEP: 144400 - loss: 0.021107154102797603\n",
      "STEP: 144500 - loss: 0.02110239794736315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 144600 - loss: 0.02109764323203707\n",
      "STEP: 144700 - loss: 0.021092889956378667\n",
      "STEP: 144800 - loss: 0.02108813811994753\n",
      "STEP: 144900 - loss: 0.02108338772230287\n",
      "STEP: 145000 - loss: 0.021078638763004965\n",
      "STEP: 145100 - loss: 0.02107389124161346\n",
      "STEP: 145200 - loss: 0.021069145157688398\n",
      "STEP: 145300 - loss: 0.021064400510790185\n",
      "STEP: 145400 - loss: 0.021059657300479123\n",
      "STEP: 145500 - loss: 0.021054915526316043\n",
      "STEP: 145600 - loss: 0.021050175187861227\n",
      "STEP: 145700 - loss: 0.021045436284676075\n",
      "STEP: 145800 - loss: 0.021040698816321347\n",
      "STEP: 145900 - loss: 0.02103596278235848\n",
      "STEP: 146000 - loss: 0.02103122818234844\n",
      "STEP: 146100 - loss: 0.021026495015853363\n",
      "STEP: 146200 - loss: 0.021021763282434627\n",
      "STEP: 146300 - loss: 0.02101703298165395\n",
      "STEP: 146400 - loss: 0.02101230411307376\n",
      "STEP: 146500 - loss: 0.02100757667625582\n",
      "STEP: 146600 - loss: 0.021002850670762706\n",
      "STEP: 146700 - loss: 0.02099812609615682\n",
      "STEP: 146800 - loss: 0.02099340295200093\n",
      "STEP: 146900 - loss: 0.020988681237857878\n",
      "STEP: 147000 - loss: 0.020983960953290456\n",
      "STEP: 147100 - loss: 0.020979242097861863\n",
      "STEP: 147200 - loss: 0.020974524671135496\n",
      "STEP: 147300 - loss: 0.02096980867267478\n",
      "STEP: 147400 - loss: 0.0209650941020431\n",
      "STEP: 147500 - loss: 0.020960380958804252\n",
      "STEP: 147600 - loss: 0.02095566924252227\n",
      "STEP: 147700 - loss: 0.020950958952761296\n",
      "STEP: 147800 - loss: 0.020946250089085298\n",
      "STEP: 147900 - loss: 0.020941542651058673\n",
      "STEP: 148000 - loss: 0.02093683663824622\n",
      "STEP: 148100 - loss: 0.0209321320502121\n",
      "STEP: 148200 - loss: 0.020927428886521608\n",
      "STEP: 148300 - loss: 0.020922727146739466\n",
      "STEP: 148400 - loss: 0.020918026830431015\n",
      "STEP: 148500 - loss: 0.020913327937161323\n",
      "STEP: 148600 - loss: 0.02090863046649602\n",
      "STEP: 148700 - loss: 0.02090393441800052\n",
      "STEP: 148800 - loss: 0.02089923979124067\n",
      "STEP: 148900 - loss: 0.0208945465857823\n",
      "STEP: 149000 - loss: 0.020889854801191463\n",
      "STEP: 149100 - loss: 0.02088516443703449\n",
      "STEP: 149200 - loss: 0.020880475492877595\n",
      "STEP: 149300 - loss: 0.020875787968287034\n",
      "STEP: 149400 - loss: 0.020871101862829873\n",
      "STEP: 149500 - loss: 0.020866417176072634\n",
      "STEP: 149600 - loss: 0.020861733907582343\n",
      "STEP: 149700 - loss: 0.02085705205692608\n",
      "STEP: 149800 - loss: 0.020852371623670967\n",
      "STEP: 149900 - loss: 0.020847692607384692\n",
      "STEP: 150000 - loss: 0.020843015007634493\n",
      "STEP: 150100 - loss: 0.020838338823988257\n",
      "STEP: 150200 - loss: 0.02083366405601366\n",
      "STEP: 150300 - loss: 0.0208289907032786\n",
      "STEP: 150400 - loss: 0.02082431876535141\n",
      "STEP: 150500 - loss: 0.020819648241800347\n",
      "STEP: 150600 - loss: 0.020814979132193795\n",
      "STEP: 150700 - loss: 0.020810311436100133\n",
      "STEP: 150800 - loss: 0.02080564515308843\n",
      "STEP: 150900 - loss: 0.020800980282727186\n",
      "STEP: 151000 - loss: 0.020796316824585666\n",
      "STEP: 151100 - loss: 0.020791654778232787\n",
      "STEP: 151200 - loss: 0.02078699414323818\n",
      "STEP: 151300 - loss: 0.020782334919171006\n",
      "STEP: 151400 - loss: 0.02077767710560074\n",
      "STEP: 151500 - loss: 0.020773020702097536\n",
      "STEP: 151600 - loss: 0.02076836570823093\n",
      "STEP: 151700 - loss: 0.020763712123570994\n",
      "STEP: 151800 - loss: 0.020759059947688042\n",
      "STEP: 151900 - loss: 0.020754409180152155\n",
      "STEP: 152000 - loss: 0.020749759820534043\n",
      "STEP: 152100 - loss: 0.020745111868404116\n",
      "STEP: 152200 - loss: 0.020740465323333207\n",
      "STEP: 152300 - loss: 0.020735820184891975\n",
      "STEP: 152400 - loss: 0.02073117645265179\n",
      "STEP: 152500 - loss: 0.02072653412618371\n",
      "STEP: 152600 - loss: 0.020721893205058968\n",
      "STEP: 152700 - loss: 0.02071725368884892\n",
      "STEP: 152800 - loss: 0.020712615577125375\n",
      "STEP: 152900 - loss: 0.020707978869459947\n",
      "STEP: 153000 - loss: 0.020703343565424328\n",
      "STEP: 153100 - loss: 0.02069870966459115\n",
      "STEP: 153200 - loss: 0.020694077166532005\n",
      "STEP: 153300 - loss: 0.02068944607081939\n",
      "STEP: 153400 - loss: 0.020684816377025777\n",
      "STEP: 153500 - loss: 0.020680188084723775\n",
      "STEP: 153600 - loss: 0.020675561193485806\n",
      "STEP: 153700 - loss: 0.020670935702885158\n",
      "STEP: 153800 - loss: 0.02066631161249457\n",
      "STEP: 153900 - loss: 0.020661688921887494\n",
      "STEP: 154000 - loss: 0.020657067630636843\n",
      "STEP: 154100 - loss: 0.02065244773831628\n",
      "STEP: 154200 - loss: 0.02064782924449932\n",
      "STEP: 154300 - loss: 0.020643212148759707\n",
      "STEP: 154400 - loss: 0.02063859645067121\n",
      "STEP: 154500 - loss: 0.02063398214980778\n",
      "STEP: 154600 - loss: 0.020629369245743837\n",
      "STEP: 154700 - loss: 0.020624757738053265\n",
      "STEP: 154800 - loss: 0.020620147626310543\n",
      "STEP: 154900 - loss: 0.020615538910090637\n",
      "STEP: 155000 - loss: 0.020610931588967724\n",
      "STEP: 155100 - loss: 0.020606325662516804\n",
      "STEP: 155200 - loss: 0.020601721130312826\n",
      "STEP: 155300 - loss: 0.020597117991930848\n",
      "STEP: 155400 - loss: 0.020592516246946364\n",
      "STEP: 155500 - loss: 0.020587915894934493\n",
      "STEP: 155600 - loss: 0.02058331693547073\n",
      "STEP: 155700 - loss: 0.02057871936813086\n",
      "STEP: 155800 - loss: 0.020574123192490655\n",
      "STEP: 155900 - loss: 0.020569528408125797\n",
      "STEP: 156000 - loss: 0.020564935014612727\n",
      "STEP: 156100 - loss: 0.020560343011527357\n",
      "STEP: 156200 - loss: 0.020555752398446193\n",
      "STEP: 156300 - loss: 0.020551163174945777\n",
      "STEP: 156400 - loss: 0.02054657534060229\n",
      "STEP: 156500 - loss: 0.020541988894993\n",
      "STEP: 156600 - loss: 0.02053740383769454\n",
      "STEP: 156700 - loss: 0.02053282016828393\n",
      "STEP: 156800 - loss: 0.02052823788633839\n",
      "STEP: 156900 - loss: 0.02052365699143532\n",
      "STEP: 157000 - loss: 0.020519077483151637\n",
      "STEP: 157100 - loss: 0.02051449936106547\n",
      "STEP: 157200 - loss: 0.02050992262475446\n",
      "STEP: 157300 - loss: 0.020505347273796083\n",
      "STEP: 157400 - loss: 0.02050077330776854\n",
      "STEP: 157500 - loss: 0.020496200726250056\n",
      "STEP: 157600 - loss: 0.020491629528818688\n",
      "STEP: 157700 - loss: 0.020487059715052686\n",
      "STEP: 157800 - loss: 0.020482491284530946\n",
      "STEP: 157900 - loss: 0.020477924236831813\n",
      "STEP: 158000 - loss: 0.02047335857153415\n",
      "STEP: 158100 - loss: 0.02046879428821683\n",
      "STEP: 158200 - loss: 0.02046423138645893\n",
      "STEP: 158300 - loss: 0.020459669865839823\n",
      "STEP: 158400 - loss: 0.020455109725938607\n",
      "STEP: 158500 - loss: 0.020450550966334656\n",
      "STEP: 158600 - loss: 0.0204459935866076\n",
      "STEP: 158700 - loss: 0.02044143758633756\n",
      "STEP: 158800 - loss: 0.020436882965103875\n",
      "STEP: 158900 - loss: 0.020432329722486674\n",
      "STEP: 159000 - loss: 0.020427777858066095\n",
      "STEP: 159100 - loss: 0.02042322737142248\n",
      "STEP: 159200 - loss: 0.02041867826213613\n",
      "STEP: 159300 - loss: 0.02041413052978755\n",
      "STEP: 159400 - loss: 0.020409584173957464\n",
      "STEP: 159500 - loss: 0.020405039194226586\n",
      "STEP: 159600 - loss: 0.02040049559017596\n",
      "STEP: 159700 - loss: 0.020395953361386675\n",
      "STEP: 159800 - loss: 0.020391412507439435\n",
      "STEP: 159900 - loss: 0.020386873027916065\n",
      "STEP: 160000 - loss: 0.020382334922397796\n",
      "STEP: 160100 - loss: 0.02037779819046651\n",
      "STEP: 160200 - loss: 0.020373262831703518\n",
      "STEP: 160300 - loss: 0.020368728845690923\n",
      "STEP: 160400 - loss: 0.02036419623201041\n",
      "STEP: 160500 - loss: 0.020359664990244514\n",
      "STEP: 160600 - loss: 0.020355135119975148\n",
      "STEP: 160700 - loss: 0.020350606620784764\n",
      "STEP: 160800 - loss: 0.020346079492255968\n",
      "STEP: 160900 - loss: 0.020341553733971292\n",
      "STEP: 161000 - loss: 0.02033702934551359\n",
      "STEP: 161100 - loss: 0.0203325063264657\n",
      "STEP: 161200 - loss: 0.020327984676410688\n",
      "STEP: 161300 - loss: 0.020323464394931594\n",
      "STEP: 161400 - loss: 0.020318945481611936\n",
      "STEP: 161500 - loss: 0.020314427936034864\n",
      "STEP: 161600 - loss: 0.020309911757784196\n",
      "STEP: 161700 - loss: 0.02030539694644345\n",
      "STEP: 161800 - loss: 0.02030088350159644\n",
      "STEP: 161900 - loss: 0.020296371422827475\n",
      "STEP: 162000 - loss: 0.020291860709720003\n",
      "STEP: 162100 - loss: 0.020287351361858684\n",
      "STEP: 162200 - loss: 0.020282843378827874\n",
      "STEP: 162300 - loss: 0.02027833676021163\n",
      "STEP: 162400 - loss: 0.020273831505595065\n",
      "STEP: 162500 - loss: 0.02026932761456265\n",
      "STEP: 162600 - loss: 0.020264825086699228\n",
      "STEP: 162700 - loss: 0.020260323921589846\n",
      "STEP: 162800 - loss: 0.02025582411881944\n",
      "STEP: 162900 - loss: 0.020251325677973664\n",
      "STEP: 163000 - loss: 0.020246828598637717\n",
      "STEP: 163100 - loss: 0.020242332880396785\n",
      "STEP: 163200 - loss: 0.020237838522837028\n",
      "STEP: 163300 - loss: 0.020233345525543876\n",
      "STEP: 163400 - loss: 0.020228853888103394\n",
      "STEP: 163500 - loss: 0.020224363610101458\n",
      "STEP: 163600 - loss: 0.020219874691124348\n",
      "STEP: 163700 - loss: 0.02021538713075829\n",
      "STEP: 163800 - loss: 0.020210900928589723\n",
      "STEP: 163900 - loss: 0.020206416084205227\n",
      "STEP: 164000 - loss: 0.020201932597191368\n",
      "STEP: 164100 - loss: 0.020197450467135048\n",
      "STEP: 164200 - loss: 0.020192969693623164\n",
      "STEP: 164300 - loss: 0.020188490276242855\n",
      "STEP: 164400 - loss: 0.020184012214581087\n",
      "STEP: 164500 - loss: 0.02017953550822534\n",
      "STEP: 164600 - loss: 0.02017506015676329\n",
      "STEP: 164700 - loss: 0.02017058615978201\n",
      "STEP: 164800 - loss: 0.02016611351686956\n",
      "STEP: 164900 - loss: 0.02016164222761358\n",
      "STEP: 165000 - loss: 0.020157172291602257\n",
      "STEP: 165100 - loss: 0.02015270370842344\n",
      "STEP: 165200 - loss: 0.020148236477665612\n",
      "STEP: 165300 - loss: 0.020143770598916796\n",
      "STEP: 165400 - loss: 0.020139306071765787\n",
      "STEP: 165500 - loss: 0.020134842895800687\n",
      "STEP: 165600 - loss: 0.020130381070610794\n",
      "STEP: 165700 - loss: 0.02012592059578497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 165800 - loss: 0.020121461470911543\n",
      "STEP: 165900 - loss: 0.020117003695580074\n",
      "STEP: 166000 - loss: 0.020112547269379933\n",
      "STEP: 166100 - loss: 0.02010809219190022\n",
      "STEP: 166200 - loss: 0.02010363846273048\n",
      "STEP: 166300 - loss: 0.020099186081460493\n",
      "STEP: 166400 - loss: 0.020094735047679772\n",
      "STEP: 166500 - loss: 0.02009028536097828\n",
      "STEP: 166600 - loss: 0.020085837020946122\n",
      "STEP: 166700 - loss: 0.020081390027173546\n",
      "STEP: 166800 - loss: 0.02007694437925032\n",
      "STEP: 166900 - loss: 0.020072500076767124\n",
      "STEP: 167000 - loss: 0.020068057119314576\n",
      "STEP: 167100 - loss: 0.020063615506483216\n",
      "STEP: 167200 - loss: 0.02005917523786375\n",
      "STEP: 167300 - loss: 0.020054736313047182\n",
      "STEP: 167400 - loss: 0.020050298731624196\n",
      "STEP: 167500 - loss: 0.02004586249318635\n",
      "STEP: 167600 - loss: 0.020041427597324805\n",
      "STEP: 167700 - loss: 0.020036994043630803\n",
      "STEP: 167800 - loss: 0.02003256183169601\n",
      "STEP: 167900 - loss: 0.020028130961112087\n",
      "STEP: 168000 - loss: 0.020023701431470635\n",
      "STEP: 168100 - loss: 0.020019273242363834\n",
      "STEP: 168200 - loss: 0.020014846393383474\n",
      "STEP: 168300 - loss: 0.020010420884121687\n",
      "STEP: 168400 - loss: 0.02000599671417098\n",
      "STEP: 168500 - loss: 0.020001573883123454\n",
      "STEP: 168600 - loss: 0.019997152390571947\n",
      "STEP: 168700 - loss: 0.01999273223610903\n",
      "STEP: 168800 - loss: 0.01998831341932751\n",
      "STEP: 168900 - loss: 0.01998389593982013\n",
      "STEP: 169000 - loss: 0.019979479797179827\n",
      "STEP: 169100 - loss: 0.01997506499100022\n",
      "STEP: 169200 - loss: 0.019970651520874152\n",
      "STEP: 169300 - loss: 0.01996623938639524\n",
      "STEP: 169400 - loss: 0.019961828587156994\n",
      "STEP: 169500 - loss: 0.019957419122753048\n",
      "STEP: 169600 - loss: 0.01995301099277731\n",
      "STEP: 169700 - loss: 0.01994860419682363\n",
      "STEP: 169800 - loss: 0.019944198734485635\n",
      "STEP: 169900 - loss: 0.01993979460535824\n",
      "STEP: 170000 - loss: 0.019935391809035086\n",
      "STEP: 170100 - loss: 0.019930990345110887\n",
      "STEP: 170200 - loss: 0.019926590213180127\n",
      "STEP: 170300 - loss: 0.019922191412837638\n",
      "STEP: 170400 - loss: 0.019917793943677993\n",
      "STEP: 170500 - loss: 0.01991339780529603\n",
      "STEP: 170600 - loss: 0.01990900299728702\n",
      "STEP: 170700 - loss: 0.01990460951924582\n",
      "STEP: 170800 - loss: 0.019900217370768043\n",
      "STEP: 170900 - loss: 0.019895826551449024\n",
      "STEP: 171000 - loss: 0.01989143706088399\n",
      "STEP: 171100 - loss: 0.01988704889866906\n",
      "STEP: 171200 - loss: 0.019882662064399776\n",
      "STEP: 171300 - loss: 0.019878276557671958\n",
      "STEP: 171400 - loss: 0.019873892378081792\n",
      "STEP: 171500 - loss: 0.019869509525225367\n",
      "STEP: 171600 - loss: 0.01986512799869906\n",
      "STEP: 171700 - loss: 0.019860747798099144\n",
      "STEP: 171800 - loss: 0.01985636892302229\n",
      "STEP: 171900 - loss: 0.019851991373065068\n",
      "STEP: 172000 - loss: 0.019847615147824033\n",
      "STEP: 172100 - loss: 0.019843240246896446\n",
      "STEP: 172200 - loss: 0.019838866669879277\n",
      "STEP: 172300 - loss: 0.019834494416369433\n",
      "STEP: 172400 - loss: 0.019830123485964298\n",
      "STEP: 172500 - loss: 0.019825753878261347\n",
      "STEP: 172600 - loss: 0.019821385592858017\n",
      "STEP: 172700 - loss: 0.019817018629351986\n",
      "STEP: 172800 - loss: 0.019812652987341018\n",
      "STEP: 172900 - loss: 0.019808288666423125\n",
      "STEP: 173000 - loss: 0.019803925666195984\n",
      "STEP: 173100 - loss: 0.01979956398625817\n",
      "STEP: 173200 - loss: 0.019795203626207514\n",
      "STEP: 173300 - loss: 0.01979084458564275\n",
      "STEP: 173400 - loss: 0.019786486864162282\n",
      "STEP: 173500 - loss: 0.01978213046136449\n",
      "STEP: 173600 - loss: 0.01977777537684841\n",
      "STEP: 173700 - loss: 0.019773421610212893\n",
      "STEP: 173800 - loss: 0.019769069161056883\n",
      "STEP: 173900 - loss: 0.01976471802897927\n",
      "STEP: 174000 - loss: 0.019760368213579703\n",
      "STEP: 174100 - loss: 0.019756019714457375\n",
      "STEP: 174200 - loss: 0.019751672531211864\n",
      "STEP: 174300 - loss: 0.019747326663442456\n",
      "STEP: 174400 - loss: 0.01974298211074931\n",
      "STEP: 174500 - loss: 0.01973863887273207\n",
      "STEP: 174600 - loss: 0.01973429694899062\n",
      "STEP: 174700 - loss: 0.019729956339125203\n",
      "STEP: 174800 - loss: 0.019725617042736076\n",
      "STEP: 174900 - loss: 0.019721279059423644\n",
      "STEP: 175000 - loss: 0.019716942388788083\n",
      "STEP: 175100 - loss: 0.01971260703043009\n",
      "STEP: 175200 - loss: 0.01970827298395085\n",
      "STEP: 175300 - loss: 0.019703940248950426\n",
      "STEP: 175400 - loss: 0.01969960882503042\n",
      "STEP: 175500 - loss: 0.01969527871179167\n",
      "STEP: 175600 - loss: 0.019690949908835055\n",
      "STEP: 175700 - loss: 0.019686622415762575\n",
      "STEP: 175800 - loss: 0.019682296232175232\n",
      "STEP: 175900 - loss: 0.019677971357674483\n",
      "STEP: 176000 - loss: 0.019673647791862616\n",
      "STEP: 176100 - loss: 0.01966932553434067\n",
      "STEP: 176200 - loss: 0.01966500458471115\n",
      "STEP: 176300 - loss: 0.019660684942575935\n",
      "STEP: 176400 - loss: 0.019656366607537182\n",
      "STEP: 176500 - loss: 0.0196520495791972\n",
      "STEP: 176600 - loss: 0.019647733857158352\n",
      "STEP: 176700 - loss: 0.01964341944102348\n",
      "STEP: 176800 - loss: 0.019639106330394777\n",
      "STEP: 176900 - loss: 0.019634794524875526\n",
      "STEP: 177000 - loss: 0.0196304840240681\n",
      "STEP: 177100 - loss: 0.019626174827576094\n",
      "STEP: 177200 - loss: 0.01962186693500232\n",
      "STEP: 177300 - loss: 0.019617560345950035\n",
      "STEP: 177400 - loss: 0.01961325506002282\n",
      "STEP: 177500 - loss: 0.01960895107682405\n",
      "STEP: 177600 - loss: 0.01960464839595743\n",
      "STEP: 177700 - loss: 0.01960034701702678\n",
      "STEP: 177800 - loss: 0.019596046939635643\n",
      "STEP: 177900 - loss: 0.01959174816338858\n",
      "STEP: 178000 - loss: 0.019587450687889373\n",
      "STEP: 178100 - loss: 0.019583154512742194\n",
      "STEP: 178200 - loss: 0.019578859637551585\n",
      "STEP: 178300 - loss: 0.019574566061921934\n",
      "STEP: 178400 - loss: 0.019570273785457882\n",
      "STEP: 178500 - loss: 0.019565982807764292\n",
      "STEP: 178600 - loss: 0.019561693128445627\n",
      "STEP: 178700 - loss: 0.019557404747107224\n",
      "STEP: 178800 - loss: 0.019553117663354137\n",
      "STEP: 178900 - loss: 0.01954883187679155\n",
      "STEP: 179000 - loss: 0.019544547387024536\n",
      "STEP: 179100 - loss: 0.019540264193658886\n",
      "STEP: 179200 - loss: 0.01953598229629997\n",
      "STEP: 179300 - loss: 0.019531701694553545\n",
      "STEP: 179400 - loss: 0.019527422388025528\n",
      "STEP: 179500 - loss: 0.019523144376321678\n",
      "STEP: 179600 - loss: 0.019518867659048172\n",
      "STEP: 179700 - loss: 0.019514592235811222\n",
      "STEP: 179800 - loss: 0.019510318106216833\n",
      "STEP: 179900 - loss: 0.019506045269871847\n",
      "STEP: 180000 - loss: 0.019501773726382458\n",
      "STEP: 180100 - loss: 0.01949750347535538\n",
      "STEP: 180200 - loss: 0.01949323451639752\n",
      "STEP: 180300 - loss: 0.019488966849115878\n",
      "STEP: 180400 - loss: 0.01948470047311713\n",
      "STEP: 180500 - loss: 0.01948043538800869\n",
      "STEP: 180600 - loss: 0.019476171593397797\n",
      "STEP: 180700 - loss: 0.019471909088891484\n",
      "STEP: 180800 - loss: 0.019467647874097632\n",
      "STEP: 180900 - loss: 0.019463387948623716\n",
      "STEP: 181000 - loss: 0.019459129312077574\n",
      "STEP: 181100 - loss: 0.019454871964066672\n",
      "STEP: 181200 - loss: 0.01945061590419967\n",
      "STEP: 181300 - loss: 0.019446361132084082\n",
      "STEP: 181400 - loss: 0.019442107647328476\n",
      "STEP: 181500 - loss: 0.019437855449540752\n",
      "STEP: 181600 - loss: 0.01943360453832992\n",
      "STEP: 181700 - loss: 0.01942935491330403\n",
      "STEP: 181800 - loss: 0.019425106574072294\n",
      "STEP: 181900 - loss: 0.019420859520243\n",
      "STEP: 182000 - loss: 0.01941661375142542\n",
      "STEP: 182100 - loss: 0.019412369267228467\n",
      "STEP: 182200 - loss: 0.019408126067261548\n",
      "STEP: 182300 - loss: 0.019403884151133574\n",
      "STEP: 182400 - loss: 0.01939964351845411\n",
      "STEP: 182500 - loss: 0.019395404168833008\n",
      "STEP: 182600 - loss: 0.019391166101879333\n",
      "STEP: 182700 - loss: 0.019386929317203275\n",
      "STEP: 182800 - loss: 0.019382693814414563\n",
      "STEP: 182900 - loss: 0.019378459593123114\n",
      "STEP: 183000 - loss: 0.0193742266529394\n",
      "STEP: 183100 - loss: 0.0193699949934732\n",
      "STEP: 183200 - loss: 0.01936576461433519\n",
      "STEP: 183300 - loss: 0.019361535515135912\n",
      "STEP: 183400 - loss: 0.019357307695485514\n",
      "STEP: 183500 - loss: 0.019353081154995157\n",
      "STEP: 183600 - loss: 0.01934885589327557\n",
      "STEP: 183700 - loss: 0.019344631909937744\n",
      "STEP: 183800 - loss: 0.019340409204592575\n",
      "STEP: 183900 - loss: 0.01933618777685144\n",
      "STEP: 184000 - loss: 0.019331967626325355\n",
      "STEP: 184100 - loss: 0.019327748752626224\n",
      "STEP: 184200 - loss: 0.019323531155365236\n",
      "STEP: 184300 - loss: 0.019319314834154087\n",
      "STEP: 184400 - loss: 0.019315099788604743\n",
      "STEP: 184500 - loss: 0.01931088601832902\n",
      "STEP: 184600 - loss: 0.01930667352293876\n",
      "STEP: 184700 - loss: 0.019302462302046425\n",
      "STEP: 184800 - loss: 0.01929825235526398\n",
      "STEP: 184900 - loss: 0.019294043682204047\n",
      "STEP: 185000 - loss: 0.019289836282479062\n",
      "STEP: 185100 - loss: 0.01928563015570135\n",
      "STEP: 185200 - loss: 0.019281425301484082\n",
      "STEP: 185300 - loss: 0.01927722171943981\n",
      "STEP: 185400 - loss: 0.019273019409181684\n",
      "STEP: 185500 - loss: 0.019268818370322516\n",
      "STEP: 185600 - loss: 0.019264618602475955\n",
      "STEP: 185700 - loss: 0.0192604201052549\n",
      "STEP: 185800 - loss: 0.019256222878273003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 185900 - loss: 0.01925202692114363\n",
      "STEP: 186000 - loss: 0.019247832233480647\n",
      "STEP: 186100 - loss: 0.019243638814897882\n",
      "STEP: 186200 - loss: 0.01923944666500897\n",
      "STEP: 186300 - loss: 0.01923525578342835\n",
      "STEP: 186400 - loss: 0.01923106616976957\n",
      "STEP: 186500 - loss: 0.019226877823647785\n",
      "STEP: 186600 - loss: 0.019222690744676522\n",
      "STEP: 186700 - loss: 0.019218504932470543\n",
      "STEP: 186800 - loss: 0.019214320386644702\n",
      "STEP: 186900 - loss: 0.01921013710681355\n",
      "STEP: 187000 - loss: 0.019205955092591993\n",
      "STEP: 187100 - loss: 0.019201774343594685\n",
      "STEP: 187200 - loss: 0.019197594859437304\n",
      "STEP: 187300 - loss: 0.01919341663973458\n",
      "STEP: 187400 - loss: 0.019189239684102015\n",
      "STEP: 187500 - loss: 0.019185063992154985\n",
      "STEP: 187600 - loss: 0.019180889563509128\n",
      "STEP: 187700 - loss: 0.01917671639777997\n",
      "STEP: 187800 - loss: 0.019172544494583336\n",
      "STEP: 187900 - loss: 0.01916837385353528\n",
      "STEP: 188000 - loss: 0.01916420447425187\n",
      "STEP: 188100 - loss: 0.01916003635634887\n",
      "STEP: 188200 - loss: 0.019155869499442934\n",
      "STEP: 188300 - loss: 0.01915170390315022\n",
      "STEP: 188400 - loss: 0.01914753956708731\n",
      "STEP: 188500 - loss: 0.019143376490870792\n",
      "STEP: 188600 - loss: 0.019139214674117305\n",
      "STEP: 188700 - loss: 0.019135054116443822\n",
      "STEP: 188800 - loss: 0.01913089481746711\n",
      "STEP: 188900 - loss: 0.019126736776804463\n",
      "STEP: 189000 - loss: 0.01912257999407312\n",
      "STEP: 189100 - loss: 0.01911842446888997\n",
      "STEP: 189200 - loss: 0.019114270200872845\n",
      "STEP: 189300 - loss: 0.01911011718963935\n",
      "STEP: 189400 - loss: 0.019105965434806693\n",
      "STEP: 189500 - loss: 0.0191018149359928\n",
      "STEP: 189600 - loss: 0.019097665692815992\n",
      "STEP: 189700 - loss: 0.01909351770489367\n",
      "STEP: 189800 - loss: 0.019089370971844453\n",
      "STEP: 189900 - loss: 0.019085225493286208\n",
      "STEP: 190000 - loss: 0.019081081268837608\n",
      "STEP: 190100 - loss: 0.01907693829811681\n",
      "STEP: 190200 - loss: 0.019072796580742533\n",
      "STEP: 190300 - loss: 0.019068656116333765\n",
      "STEP: 190400 - loss: 0.019064516904508864\n",
      "STEP: 190500 - loss: 0.019060378944887022\n",
      "STEP: 190600 - loss: 0.01905624223708709\n",
      "STEP: 190700 - loss: 0.01905210678072864\n",
      "STEP: 190800 - loss: 0.019047972575430467\n",
      "STEP: 190900 - loss: 0.019043839620812443\n",
      "STEP: 191000 - loss: 0.01903970791649361\n",
      "STEP: 191100 - loss: 0.019035577462093993\n",
      "STEP: 191200 - loss: 0.01903144825723314\n",
      "STEP: 191300 - loss: 0.01902732030153097\n",
      "STEP: 191400 - loss: 0.019023193594607447\n",
      "STEP: 191500 - loss: 0.01901906813608275\n",
      "STEP: 191600 - loss: 0.019014943925576917\n",
      "STEP: 191700 - loss: 0.01901082096271041\n",
      "STEP: 191800 - loss: 0.019006699247103652\n",
      "STEP: 191900 - loss: 0.019002578778377136\n",
      "STEP: 192000 - loss: 0.018998459556151738\n",
      "STEP: 192100 - loss: 0.01899434158004782\n",
      "STEP: 192200 - loss: 0.01899022484968676\n",
      "STEP: 192300 - loss: 0.018986109364689387\n",
      "STEP: 192400 - loss: 0.018981995124676743\n",
      "STEP: 192500 - loss: 0.018977882129270207\n",
      "STEP: 192600 - loss: 0.01897377037809107\n",
      "STEP: 192700 - loss: 0.018969659870760984\n",
      "STEP: 192800 - loss: 0.01896555060690123\n",
      "STEP: 192900 - loss: 0.01896144258613386\n",
      "STEP: 193000 - loss: 0.018957335808080435\n",
      "STEP: 193100 - loss: 0.018953230272363033\n",
      "STEP: 193200 - loss: 0.018949125978603647\n",
      "STEP: 193300 - loss: 0.01894502292642444\n",
      "STEP: 193400 - loss: 0.018940921115447887\n",
      "STEP: 193500 - loss: 0.018936820545296284\n",
      "STEP: 193600 - loss: 0.018932721215592075\n",
      "STEP: 193700 - loss: 0.01892862312595787\n",
      "STEP: 193800 - loss: 0.01892452627601637\n",
      "STEP: 193900 - loss: 0.018920430665390838\n",
      "STEP: 194000 - loss: 0.018916336293703654\n",
      "STEP: 194100 - loss: 0.018912243160578417\n",
      "STEP: 194200 - loss: 0.018908151265638155\n",
      "STEP: 194300 - loss: 0.018904060608506086\n",
      "STEP: 194400 - loss: 0.01889997118880572\n",
      "STEP: 194500 - loss: 0.018895883006160706\n",
      "STEP: 194600 - loss: 0.018891796060194612\n",
      "STEP: 194700 - loss: 0.018887710350531214\n",
      "STEP: 194800 - loss: 0.018883625876794367\n",
      "STEP: 194900 - loss: 0.01887954263860806\n",
      "STEP: 195000 - loss: 0.018875460635596635\n",
      "STEP: 195100 - loss: 0.018871379867384158\n",
      "STEP: 195200 - loss: 0.018867300333594926\n",
      "STEP: 195300 - loss: 0.018863222033853523\n",
      "STEP: 195400 - loss: 0.01885914496778468\n",
      "STEP: 195500 - loss: 0.018855069135012687\n",
      "STEP: 195600 - loss: 0.018850994535162664\n",
      "STEP: 195700 - loss: 0.01884692116785946\n",
      "STEP: 195800 - loss: 0.01884284903272796\n",
      "STEP: 195900 - loss: 0.018838778129393566\n",
      "STEP: 196000 - loss: 0.01883470845748162\n",
      "STEP: 196100 - loss: 0.01883064001661704\n",
      "STEP: 196200 - loss: 0.018826572806425704\n",
      "STEP: 196300 - loss: 0.018822506826533185\n",
      "STEP: 196400 - loss: 0.01881844207656514\n",
      "STEP: 196500 - loss: 0.018814378556147426\n",
      "STEP: 196600 - loss: 0.018810316264905966\n",
      "STEP: 196700 - loss: 0.018806255202466572\n",
      "STEP: 196800 - loss: 0.01880219536845594\n",
      "STEP: 196900 - loss: 0.018798136762500074\n",
      "STEP: 197000 - loss: 0.018794079384225358\n",
      "STEP: 197100 - loss: 0.01879002323325832\n",
      "STEP: 197200 - loss: 0.018785968309225694\n",
      "STEP: 197300 - loss: 0.018781914611754234\n",
      "STEP: 197400 - loss: 0.01877786214047062\n",
      "STEP: 197500 - loss: 0.018773810895002074\n",
      "STEP: 197600 - loss: 0.018769760874975342\n",
      "STEP: 197700 - loss: 0.018765712080017975\n",
      "STEP: 197800 - loss: 0.018761664509757036\n",
      "STEP: 197900 - loss: 0.018757618163820294\n",
      "STEP: 198000 - loss: 0.018753573041834858\n",
      "STEP: 198100 - loss: 0.0187495291434288\n",
      "STEP: 198200 - loss: 0.01874548646822937\n",
      "STEP: 198300 - loss: 0.01874144501586492\n",
      "STEP: 198400 - loss: 0.018737404785963217\n",
      "STEP: 198500 - loss: 0.018733365778152558\n",
      "STEP: 198600 - loss: 0.01872932799206089\n",
      "STEP: 198700 - loss: 0.018725291427316854\n",
      "STEP: 198800 - loss: 0.018721256083548563\n",
      "STEP: 198900 - loss: 0.01871722196038495\n",
      "STEP: 199000 - loss: 0.01871318905745439\n",
      "STEP: 199100 - loss: 0.01870915737438574\n",
      "STEP: 199200 - loss: 0.01870512691080785\n",
      "STEP: 199300 - loss: 0.018701097666349895\n",
      "STEP: 199400 - loss: 0.018697069640640877\n",
      "STEP: 199500 - loss: 0.01869304283331023\n",
      "STEP: 199600 - loss: 0.018689017243986993\n",
      "STEP: 199700 - loss: 0.018684992872300778\n",
      "STEP: 199800 - loss: 0.018680969717881298\n",
      "STEP: 199900 - loss: 0.018676947780357975\n",
      "STEP: 200000 - loss: 0.01867292705936096\n",
      "STEP: 200100 - loss: 0.018668907554519797\n",
      "STEP: 200200 - loss: 0.01866488926546469\n",
      "STEP: 200300 - loss: 0.018660872191825863\n",
      "STEP: 200400 - loss: 0.01865685633323331\n",
      "STEP: 200500 - loss: 0.01865284168931764\n",
      "STEP: 200600 - loss: 0.0186488282597093\n",
      "STEP: 200700 - loss: 0.01864481604403893\n",
      "STEP: 200800 - loss: 0.018640805041936773\n",
      "STEP: 200900 - loss: 0.01863679525303434\n",
      "STEP: 201000 - loss: 0.01863278667696211\n",
      "STEP: 201100 - loss: 0.01862877931335121\n",
      "STEP: 201200 - loss: 0.0186247731618329\n",
      "STEP: 201300 - loss: 0.018620768222038148\n",
      "STEP: 201400 - loss: 0.01861676449359875\n",
      "STEP: 201500 - loss: 0.018612761976145982\n",
      "STEP: 201600 - loss: 0.018608760669311414\n",
      "STEP: 201700 - loss: 0.018604760572726736\n",
      "STEP: 201800 - loss: 0.018600761686023853\n",
      "STEP: 201900 - loss: 0.01859676400883469\n",
      "STEP: 202000 - loss: 0.018592767540791387\n",
      "STEP: 202100 - loss: 0.0185887722815259\n",
      "STEP: 202200 - loss: 0.018584778230670757\n",
      "STEP: 202300 - loss: 0.01858078538785807\n",
      "STEP: 202400 - loss: 0.018576793752720536\n",
      "STEP: 202500 - loss: 0.018572803324890717\n",
      "STEP: 202600 - loss: 0.018568814104001304\n",
      "STEP: 202700 - loss: 0.018564826089684888\n",
      "STEP: 202800 - loss: 0.01856083928157496\n",
      "STEP: 202900 - loss: 0.018556853679304024\n",
      "STEP: 203000 - loss: 0.018552869282505664\n",
      "STEP: 203100 - loss: 0.018548886090812854\n",
      "STEP: 203200 - loss: 0.01854490410385929\n",
      "STEP: 203300 - loss: 0.01854092332127835\n",
      "STEP: 203400 - loss: 0.018536943742703406\n",
      "STEP: 203500 - loss: 0.018532965367768526\n",
      "STEP: 203600 - loss: 0.018528988196107272\n",
      "STEP: 203700 - loss: 0.018525012227353782\n",
      "STEP: 203800 - loss: 0.018521037461142024\n",
      "STEP: 203900 - loss: 0.018517063897106355\n",
      "STEP: 204000 - loss: 0.018513091534880753\n",
      "STEP: 204100 - loss: 0.018509120374099887\n",
      "STEP: 204200 - loss: 0.018505150414397934\n",
      "STEP: 204300 - loss: 0.01850118165540985\n",
      "STEP: 204400 - loss: 0.018497214096770197\n",
      "STEP: 204500 - loss: 0.018493247738113977\n",
      "STEP: 204600 - loss: 0.018489282579075842\n",
      "STEP: 204700 - loss: 0.018485318619291136\n",
      "STEP: 204800 - loss: 0.018481355858394694\n",
      "STEP: 204900 - loss: 0.018477394296022343\n",
      "STEP: 205000 - loss: 0.018473433931808798\n",
      "STEP: 205100 - loss: 0.018469474765390258\n",
      "STEP: 205200 - loss: 0.01846551679640177\n",
      "STEP: 205300 - loss: 0.018461560024479285\n",
      "STEP: 205400 - loss: 0.018457604449258782\n",
      "STEP: 205500 - loss: 0.018453650070375943\n",
      "STEP: 205600 - loss: 0.018449696887467056\n",
      "STEP: 205700 - loss: 0.018445744900168072\n",
      "STEP: 205800 - loss: 0.018441794108115476\n",
      "STEP: 205900 - loss: 0.018437844510945652\n",
      "STEP: 206000 - loss: 0.0184338961082947\n",
      "STEP: 206100 - loss: 0.018429948899799868\n",
      "STEP: 206200 - loss: 0.018426002885097447\n",
      "STEP: 206300 - loss: 0.018422058063824416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 206400 - loss: 0.018418114435617607\n",
      "STEP: 206500 - loss: 0.018414172000114255\n",
      "STEP: 206600 - loss: 0.018410230756951314\n",
      "STEP: 206700 - loss: 0.01840629070576632\n",
      "STEP: 206800 - loss: 0.018402351846196464\n",
      "STEP: 206900 - loss: 0.018398414177879234\n",
      "STEP: 207000 - loss: 0.018394477700452362\n",
      "STEP: 207100 - loss: 0.0183905424135535\n",
      "STEP: 207200 - loss: 0.018386608316820333\n",
      "STEP: 207300 - loss: 0.018382675409891087\n",
      "STEP: 207400 - loss: 0.01837874369240348\n",
      "STEP: 207500 - loss: 0.01837481316399606\n",
      "STEP: 207600 - loss: 0.018370883824306652\n",
      "STEP: 207700 - loss: 0.01836695567297423\n",
      "STEP: 207800 - loss: 0.018363028709636527\n",
      "STEP: 207900 - loss: 0.018359102933932663\n",
      "STEP: 208000 - loss: 0.018355178345501228\n",
      "STEP: 208100 - loss: 0.018351254943981002\n",
      "STEP: 208200 - loss: 0.018347332729011047\n",
      "STEP: 208300 - loss: 0.01834341170023021\n",
      "STEP: 208400 - loss: 0.018339491857277686\n",
      "STEP: 208500 - loss: 0.018335573199792673\n",
      "STEP: 208600 - loss: 0.018331655727414752\n",
      "STEP: 208700 - loss: 0.018327739439783313\n",
      "STEP: 208800 - loss: 0.018323824336537738\n",
      "STEP: 208900 - loss: 0.01831991041731803\n",
      "STEP: 209000 - loss: 0.018315997681763806\n",
      "STEP: 209100 - loss: 0.018312086129514953\n",
      "STEP: 209200 - loss: 0.018308175760211636\n",
      "STEP: 209300 - loss: 0.018304266573493856\n",
      "STEP: 209400 - loss: 0.01830035856900203\n",
      "STEP: 209500 - loss: 0.018296451746376355\n",
      "STEP: 209600 - loss: 0.01829254610525731\n",
      "STEP: 209700 - loss: 0.01828864164528552\n",
      "STEP: 209800 - loss: 0.018284738366101706\n",
      "STEP: 209900 - loss: 0.018280836267346567\n",
      "STEP: 210000 - loss: 0.018276935348661035\n",
      "STEP: 210100 - loss: 0.018273035609686027\n",
      "STEP: 210200 - loss: 0.018269137050062677\n",
      "STEP: 210300 - loss: 0.018265239669432484\n",
      "STEP: 210400 - loss: 0.018261343467436623\n",
      "STEP: 210500 - loss: 0.018257448443716438\n",
      "STEP: 210600 - loss: 0.018253554597913443\n",
      "STEP: 210700 - loss: 0.018249661929669526\n",
      "STEP: 210800 - loss: 0.018245770438626303\n",
      "STEP: 210900 - loss: 0.018241880124425788\n",
      "STEP: 211000 - loss: 0.01823799098670968\n",
      "STEP: 211100 - loss: 0.018234103025120463\n",
      "STEP: 211200 - loss: 0.01823021623930002\n",
      "STEP: 211300 - loss: 0.01822633062889096\n",
      "STEP: 211400 - loss: 0.01822244619353545\n",
      "STEP: 211500 - loss: 0.018218562932876305\n",
      "STEP: 211600 - loss: 0.01821468084655583\n",
      "STEP: 211700 - loss: 0.01821079993421692\n",
      "STEP: 211800 - loss: 0.018206920195502616\n",
      "STEP: 211900 - loss: 0.01820304163005562\n",
      "STEP: 212000 - loss: 0.018199164237519053\n",
      "STEP: 212100 - loss: 0.018195288017536383\n",
      "STEP: 212200 - loss: 0.018191412969750655\n",
      "STEP: 212300 - loss: 0.018187539093805192\n",
      "STEP: 212400 - loss: 0.018183666389343885\n",
      "STEP: 212500 - loss: 0.018179794856009956\n",
      "STEP: 212600 - loss: 0.018175924493447244\n",
      "STEP: 212700 - loss: 0.018172055301299717\n",
      "STEP: 212800 - loss: 0.01816818727921126\n",
      "STEP: 212900 - loss: 0.01816432042682582\n",
      "STEP: 213000 - loss: 0.018160454743787777\n",
      "STEP: 213100 - loss: 0.01815659022974082\n",
      "STEP: 213200 - loss: 0.018152726884330273\n",
      "STEP: 213300 - loss: 0.01814886470719986\n",
      "STEP: 213400 - loss: 0.018145003697994648\n",
      "STEP: 213500 - loss: 0.01814114385635906\n",
      "STEP: 213600 - loss: 0.01813728518193799\n",
      "STEP: 213700 - loss: 0.01813342767437619\n",
      "STEP: 213800 - loss: 0.01812957133331914\n",
      "STEP: 213900 - loss: 0.01812571615841163\n",
      "STEP: 214000 - loss: 0.01812186214929888\n",
      "STEP: 214100 - loss: 0.01811800930562639\n",
      "STEP: 214200 - loss: 0.01811415762703942\n",
      "STEP: 214300 - loss: 0.018110307113183892\n",
      "STEP: 214400 - loss: 0.018106457763705182\n",
      "STEP: 214500 - loss: 0.018102609578249128\n",
      "STEP: 214600 - loss: 0.018098762556461555\n",
      "STEP: 214700 - loss: 0.018094916697988725\n",
      "STEP: 214800 - loss: 0.018091072002476553\n",
      "STEP: 214900 - loss: 0.018087228469571268\n",
      "STEP: 215000 - loss: 0.018083386098919168\n",
      "STEP: 215100 - loss: 0.018079544890166832\n",
      "STEP: 215200 - loss: 0.018075704842960626\n",
      "STEP: 215300 - loss: 0.018071865956947236\n",
      "STEP: 215400 - loss: 0.01806802823177353\n",
      "STEP: 215500 - loss: 0.018064191667086133\n",
      "STEP: 215600 - loss: 0.01806035626253224\n",
      "STEP: 215700 - loss: 0.018056522017758845\n",
      "STEP: 215800 - loss: 0.01805268893241311\n",
      "STEP: 215900 - loss: 0.018048857006142453\n",
      "STEP: 216000 - loss: 0.01804502623859406\n",
      "STEP: 216100 - loss: 0.01804119662941546\n",
      "STEP: 216200 - loss: 0.018037368178254416\n",
      "STEP: 216300 - loss: 0.018033540884758587\n",
      "STEP: 216400 - loss: 0.018029714748575773\n",
      "STEP: 216500 - loss: 0.01802588976935396\n",
      "STEP: 216600 - loss: 0.018022065946741016\n",
      "STEP: 216700 - loss: 0.018018243280385347\n",
      "STEP: 216800 - loss: 0.01801442176993507\n",
      "STEP: 216900 - loss: 0.018010601415038616\n",
      "STEP: 217000 - loss: 0.018006782215344234\n",
      "STEP: 217100 - loss: 0.018002964170500728\n",
      "STEP: 217200 - loss: 0.017999147280156776\n",
      "STEP: 217300 - loss: 0.017995331543961005\n",
      "STEP: 217400 - loss: 0.017991516961562558\n",
      "STEP: 217500 - loss: 0.01798770353261012\n",
      "STEP: 217600 - loss: 0.01798389125675306\n",
      "STEP: 217700 - loss: 0.017980080133640528\n",
      "STEP: 217800 - loss: 0.017976270162921873\n",
      "STEP: 217900 - loss: 0.017972461344246425\n",
      "STEP: 218000 - loss: 0.017968653677263746\n",
      "STEP: 218100 - loss: 0.017964847161623457\n",
      "STEP: 218200 - loss: 0.017961041796975497\n",
      "STEP: 218300 - loss: 0.017957237582969417\n",
      "STEP: 218400 - loss: 0.017953434519255383\n",
      "STEP: 218500 - loss: 0.017949632605483348\n",
      "STEP: 218600 - loss: 0.017945831841303817\n",
      "STEP: 218700 - loss: 0.017942032226366776\n",
      "STEP: 218800 - loss: 0.017938233760322504\n",
      "STEP: 218900 - loss: 0.017934436442821805\n",
      "STEP: 219000 - loss: 0.01793064027351523\n",
      "STEP: 219100 - loss: 0.017926845252053007\n",
      "STEP: 219200 - loss: 0.017923051378086582\n",
      "STEP: 219300 - loss: 0.01791925865126657\n",
      "STEP: 219400 - loss: 0.017915467071244053\n",
      "STEP: 219500 - loss: 0.017911676637670227\n",
      "STEP: 219600 - loss: 0.017907887350196165\n",
      "STEP: 219700 - loss: 0.017904099208473336\n",
      "STEP: 219800 - loss: 0.017900312212153168\n",
      "STEP: 219900 - loss: 0.01789652636088728\n",
      "STEP: 220000 - loss: 0.017892741654327186\n",
      "STEP: 220100 - loss: 0.017888958092124912\n",
      "STEP: 220200 - loss: 0.017885175673931913\n",
      "STEP: 220300 - loss: 0.01788139439940065\n",
      "STEP: 220400 - loss: 0.01787761426818296\n",
      "STEP: 220500 - loss: 0.01787383527993085\n",
      "STEP: 220600 - loss: 0.01787005743429706\n",
      "STEP: 220700 - loss: 0.017866280730933472\n",
      "STEP: 220800 - loss: 0.0178625051694931\n",
      "STEP: 220900 - loss: 0.017858730749628314\n",
      "STEP: 221000 - loss: 0.017854957470991854\n",
      "STEP: 221100 - loss: 0.017851185333236683\n",
      "STEP: 221200 - loss: 0.017847414336015353\n",
      "STEP: 221300 - loss: 0.017843644478981374\n",
      "STEP: 221400 - loss: 0.01783987576178756\n",
      "STEP: 221500 - loss: 0.01783610818408719\n",
      "STEP: 221600 - loss: 0.01783234174553392\n",
      "STEP: 221700 - loss: 0.017828576445780845\n",
      "STEP: 221800 - loss: 0.017824812284481815\n",
      "STEP: 221900 - loss: 0.017821049261290328\n",
      "STEP: 222000 - loss: 0.017817287375860056\n",
      "STEP: 222100 - loss: 0.017813526627845116\n",
      "STEP: 222200 - loss: 0.017809767016899333\n",
      "STEP: 222300 - loss: 0.017806008542676992\n",
      "STEP: 222400 - loss: 0.017802251204832066\n",
      "STEP: 222500 - loss: 0.01779849500301905\n",
      "STEP: 222600 - loss: 0.017794739936892283\n",
      "STEP: 222700 - loss: 0.01779098600610605\n",
      "STEP: 222800 - loss: 0.017787233210315376\n",
      "STEP: 222900 - loss: 0.017783481549174883\n",
      "STEP: 223000 - loss: 0.017779731022339002\n",
      "STEP: 223100 - loss: 0.01777598162946324\n",
      "STEP: 223200 - loss: 0.017772233370202156\n",
      "STEP: 223300 - loss: 0.017768486244211214\n",
      "STEP: 223400 - loss: 0.01776474025114549\n",
      "STEP: 223500 - loss: 0.017760995390660384\n",
      "STEP: 223600 - loss: 0.017757251662411537\n",
      "STEP: 223700 - loss: 0.017753509066054243\n",
      "STEP: 223800 - loss: 0.01774976760124414\n",
      "STEP: 223900 - loss: 0.017746027267637274\n",
      "STEP: 224000 - loss: 0.017742288064889276\n",
      "STEP: 224100 - loss: 0.017738549992656526\n",
      "STEP: 224200 - loss: 0.017734813050594427\n",
      "STEP: 224300 - loss: 0.017731077238359844\n",
      "STEP: 224400 - loss: 0.01772734255560852\n",
      "STEP: 224500 - loss: 0.017723609001997365\n",
      "STEP: 224600 - loss: 0.01771987657718265\n",
      "STEP: 224700 - loss: 0.017716145280820734\n",
      "STEP: 224800 - loss: 0.017712415112568873\n",
      "STEP: 224900 - loss: 0.01770868607208337\n",
      "STEP: 225000 - loss: 0.01770495815902138\n",
      "STEP: 225100 - loss: 0.017701231373039958\n",
      "STEP: 225200 - loss: 0.017697505713796205\n",
      "STEP: 225300 - loss: 0.017693781180947284\n",
      "STEP: 225400 - loss: 0.01769005777415073\n",
      "STEP: 225500 - loss: 0.017686335493063775\n",
      "STEP: 225600 - loss: 0.01768261433734402\n",
      "STEP: 225700 - loss: 0.017678894306649137\n",
      "STEP: 225800 - loss: 0.01767517540063703\n",
      "STEP: 225900 - loss: 0.017671457618965297\n",
      "STEP: 226000 - loss: 0.01766774096129197\n",
      "STEP: 226100 - loss: 0.017664025427275357\n",
      "STEP: 226200 - loss: 0.017660311016573282\n",
      "STEP: 226300 - loss: 0.017656597728844266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 226400 - loss: 0.017652885563746598\n",
      "STEP: 226500 - loss: 0.017649174520938723\n",
      "STEP: 226600 - loss: 0.01764546460007927\n",
      "STEP: 226700 - loss: 0.017641755800827077\n",
      "STEP: 226800 - loss: 0.017638048122840855\n",
      "STEP: 226900 - loss: 0.017634341565779234\n",
      "STEP: 227000 - loss: 0.017630636129301606\n",
      "STEP: 227100 - loss: 0.017626931813066984\n",
      "STEP: 227200 - loss: 0.01762322861673437\n",
      "STEP: 227300 - loss: 0.017619526539963296\n",
      "STEP: 227400 - loss: 0.017615825582413154\n",
      "STEP: 227500 - loss: 0.017612125743743327\n",
      "STEP: 227600 - loss: 0.017608427023613715\n",
      "STEP: 227700 - loss: 0.01760472942168369\n",
      "STEP: 227800 - loss: 0.017601032937613406\n",
      "STEP: 227900 - loss: 0.01759733757106252\n",
      "STEP: 228000 - loss: 0.01759364332169127\n",
      "STEP: 228100 - loss: 0.017589950189159743\n",
      "STEP: 228200 - loss: 0.017586258173128234\n",
      "STEP: 228300 - loss: 0.017582567273256973\n",
      "STEP: 228400 - loss: 0.01757887748920656\n",
      "STEP: 228500 - loss: 0.017575188820637473\n",
      "STEP: 228600 - loss: 0.017571501267210417\n",
      "STEP: 228700 - loss: 0.017567814828586027\n",
      "STEP: 228800 - loss: 0.017564129504425277\n",
      "STEP: 228900 - loss: 0.01756044529438914\n",
      "STEP: 229000 - loss: 0.017556762198138638\n",
      "STEP: 229100 - loss: 0.01755308021533499\n",
      "STEP: 229200 - loss: 0.01754939934563962\n",
      "STEP: 229300 - loss: 0.01754571958871367\n",
      "STEP: 229400 - loss: 0.017542040944218743\n",
      "STEP: 229500 - loss: 0.017538363411816273\n",
      "STEP: 229600 - loss: 0.01753468699116819\n",
      "STEP: 229700 - loss: 0.017531011681936055\n",
      "STEP: 229800 - loss: 0.017527337483781984\n",
      "STEP: 229900 - loss: 0.017523664396368016\n",
      "STEP: 230000 - loss: 0.01751999241935596\n",
      "STEP: 230100 - loss: 0.017516321552408228\n",
      "STEP: 230200 - loss: 0.017512651795186917\n",
      "STEP: 230300 - loss: 0.01750898314735483\n",
      "STEP: 230400 - loss: 0.017505315608574105\n",
      "STEP: 230500 - loss: 0.017501649178507564\n",
      "STEP: 230600 - loss: 0.017497983856818007\n",
      "STEP: 230700 - loss: 0.01749431964316787\n",
      "STEP: 230800 - loss: 0.017490656537220633\n",
      "STEP: 230900 - loss: 0.017486994538638726\n",
      "STEP: 231000 - loss: 0.01748333364708575\n",
      "STEP: 231100 - loss: 0.017479673862224657\n",
      "STEP: 231200 - loss: 0.017476015183718958\n",
      "STEP: 231300 - loss: 0.017472357611231885\n",
      "STEP: 231400 - loss: 0.017468701144427258\n",
      "STEP: 231500 - loss: 0.017465045782968577\n",
      "STEP: 231600 - loss: 0.01746139152651956\n",
      "STEP: 231700 - loss: 0.017457738374744035\n",
      "STEP: 231800 - loss: 0.017454086327306037\n",
      "STEP: 231900 - loss: 0.017450435383869495\n",
      "STEP: 232000 - loss: 0.017446785544098613\n",
      "STEP: 232100 - loss: 0.017443136807657758\n",
      "STEP: 232200 - loss: 0.017439489174211145\n",
      "STEP: 232300 - loss: 0.01743584264342337\n",
      "STEP: 232400 - loss: 0.01743219721495886\n",
      "STEP: 232500 - loss: 0.017428552888482284\n",
      "STEP: 232600 - loss: 0.01742490966365847\n",
      "STEP: 232700 - loss: 0.01742126754015244\n",
      "STEP: 232800 - loss: 0.01741762651762877\n",
      "STEP: 232900 - loss: 0.01741398659575296\n",
      "STEP: 233000 - loss: 0.01741034777418978\n",
      "STEP: 233100 - loss: 0.017406710052604815\n",
      "STEP: 233200 - loss: 0.017403073430663395\n",
      "STEP: 233300 - loss: 0.017399437908030828\n",
      "STEP: 233400 - loss: 0.01739580348437278\n",
      "STEP: 233500 - loss: 0.017392170159355094\n",
      "STEP: 233600 - loss: 0.01738853793264338\n",
      "STEP: 233700 - loss: 0.01738490680390353\n",
      "STEP: 233800 - loss: 0.017381276772801633\n",
      "STEP: 233900 - loss: 0.017377647839003574\n",
      "STEP: 234000 - loss: 0.01737402000217579\n",
      "STEP: 234100 - loss: 0.017370393261984536\n",
      "STEP: 234200 - loss: 0.017366767618096073\n",
      "STEP: 234300 - loss: 0.017363143070176897\n",
      "STEP: 234400 - loss: 0.017359519617893754\n",
      "STEP: 234500 - loss: 0.017355897260913123\n",
      "STEP: 234600 - loss: 0.01735227599890212\n",
      "STEP: 234700 - loss: 0.017348655831527452\n",
      "STEP: 234800 - loss: 0.017345036758455952\n",
      "STEP: 234900 - loss: 0.017341418779355116\n",
      "STEP: 235000 - loss: 0.017337801893891877\n",
      "STEP: 235100 - loss: 0.01733418610173347\n",
      "STEP: 235200 - loss: 0.0173305714025476\n",
      "STEP: 235300 - loss: 0.017326957796001487\n",
      "STEP: 235400 - loss: 0.017323345281762954\n",
      "STEP: 235500 - loss: 0.017319733859499593\n",
      "STEP: 235600 - loss: 0.017316123528879283\n",
      "STEP: 235700 - loss: 0.01731251428956986\n",
      "STEP: 235800 - loss: 0.01730890614123943\n",
      "STEP: 235900 - loss: 0.017305299083556193\n",
      "STEP: 236000 - loss: 0.017301693116187827\n",
      "STEP: 236100 - loss: 0.017298088238803397\n",
      "STEP: 236200 - loss: 0.017294484451070836\n",
      "STEP: 236300 - loss: 0.017290881752658922\n",
      "STEP: 236400 - loss: 0.017287280143235974\n",
      "STEP: 236500 - loss: 0.017283679622471133\n",
      "STEP: 236600 - loss: 0.01728008019003268\n",
      "STEP: 236700 - loss: 0.01727648184559017\n",
      "STEP: 236800 - loss: 0.017272884588812078\n",
      "STEP: 236900 - loss: 0.017269288419367895\n",
      "STEP: 237000 - loss: 0.017265693336926596\n",
      "STEP: 237100 - loss: 0.01726209934115778\n",
      "STEP: 237200 - loss: 0.017258506431730596\n",
      "STEP: 237300 - loss: 0.01725491460831451\n",
      "STEP: 237400 - loss: 0.01725132387057956\n",
      "STEP: 237500 - loss: 0.01724773421819511\n",
      "STEP: 237600 - loss: 0.017244145650831023\n",
      "STEP: 237700 - loss: 0.01724055816815754\n",
      "STEP: 237800 - loss: 0.0172369717698443\n",
      "STEP: 237900 - loss: 0.017233386455561515\n",
      "STEP: 238000 - loss: 0.01722980222497968\n",
      "STEP: 238100 - loss: 0.01722621907776877\n",
      "STEP: 238200 - loss: 0.01722263701359959\n",
      "STEP: 238300 - loss: 0.01721905603214225\n",
      "STEP: 238400 - loss: 0.01721547613306782\n",
      "STEP: 238500 - loss: 0.01721189731604651\n",
      "STEP: 238600 - loss: 0.017208319580749733\n",
      "STEP: 238700 - loss: 0.01720474292684795\n",
      "STEP: 238800 - loss: 0.017201167354012423\n",
      "STEP: 238900 - loss: 0.01719759286191423\n",
      "STEP: 239000 - loss: 0.017194019450224644\n",
      "STEP: 239100 - loss: 0.017190447118614883\n",
      "STEP: 239200 - loss: 0.017186875866756556\n",
      "STEP: 239300 - loss: 0.017183305694321048\n",
      "STEP: 239400 - loss: 0.017179736600979975\n",
      "STEP: 239500 - loss: 0.017176168586405253\n",
      "STEP: 239600 - loss: 0.017172601650268484\n",
      "STEP: 239700 - loss: 0.017169035792241744\n",
      "STEP: 239800 - loss: 0.017165471011996912\n",
      "STEP: 239900 - loss: 0.017161907309206435\n",
      "STEP: 240000 - loss: 0.017158344683542246\n",
      "STEP: 240100 - loss: 0.01715478313467659\n",
      "STEP: 240200 - loss: 0.017151222662282452\n",
      "STEP: 240300 - loss: 0.017147663266031624\n",
      "STEP: 240400 - loss: 0.01714410494559726\n",
      "STEP: 240500 - loss: 0.017140547700652015\n",
      "STEP: 240600 - loss: 0.017136991530868464\n",
      "STEP: 240700 - loss: 0.017133436435919677\n",
      "STEP: 240800 - loss: 0.017129882415478786\n",
      "STEP: 240900 - loss: 0.01712632946921876\n",
      "STEP: 241000 - loss: 0.01712277759681313\n",
      "STEP: 241100 - loss: 0.01711922679793474\n",
      "STEP: 241200 - loss: 0.017115677072257453\n",
      "STEP: 241300 - loss: 0.017112128419454357\n",
      "STEP: 241400 - loss: 0.017108580839199448\n",
      "STEP: 241500 - loss: 0.01710503433116637\n",
      "STEP: 241600 - loss: 0.017101488895028744\n",
      "STEP: 241700 - loss: 0.017097944530460766\n",
      "STEP: 241800 - loss: 0.017094401237136212\n",
      "STEP: 241900 - loss: 0.017090859014729617\n",
      "STEP: 242000 - loss: 0.017087317862914637\n",
      "STEP: 242100 - loss: 0.017083777781365807\n",
      "STEP: 242200 - loss: 0.01708023876975779\n",
      "STEP: 242300 - loss: 0.017076700827764815\n",
      "STEP: 242400 - loss: 0.01707316395506172\n",
      "STEP: 242500 - loss: 0.017069628151322933\n",
      "STEP: 242600 - loss: 0.017066093416223567\n",
      "STEP: 242700 - loss: 0.017062559749438353\n",
      "STEP: 242800 - loss: 0.01705902715064233\n",
      "STEP: 242900 - loss: 0.017055495619510634\n",
      "STEP: 243000 - loss: 0.01705196515571821\n",
      "STEP: 243100 - loss: 0.01704843575894088\n",
      "STEP: 243200 - loss: 0.017044907428853864\n",
      "STEP: 243300 - loss: 0.017041380165132507\n",
      "STEP: 243400 - loss: 0.017037853967452533\n",
      "STEP: 243500 - loss: 0.017034328835489436\n",
      "STEP: 243600 - loss: 0.017030804768919355\n",
      "STEP: 243700 - loss: 0.017027281767417864\n",
      "STEP: 243800 - loss: 0.017023759830661178\n",
      "STEP: 243900 - loss: 0.0170202389583255\n",
      "STEP: 244000 - loss: 0.017016719150086658\n",
      "STEP: 244100 - loss: 0.017013200405621244\n",
      "STEP: 244200 - loss: 0.017009682724605544\n",
      "STEP: 244300 - loss: 0.017006166106715882\n",
      "STEP: 244400 - loss: 0.017002650551629212\n",
      "STEP: 244500 - loss: 0.01699913605902211\n",
      "STEP: 244600 - loss: 0.016995622628571263\n",
      "STEP: 244700 - loss: 0.01699211025995354\n",
      "STEP: 244800 - loss: 0.01698859895284584\n",
      "STEP: 244900 - loss: 0.016985088706925534\n",
      "STEP: 245000 - loss: 0.016981579521869523\n",
      "STEP: 245100 - loss: 0.016978071397355406\n",
      "STEP: 245200 - loss: 0.016974564333060098\n",
      "STEP: 245300 - loss: 0.016971058328661404\n",
      "STEP: 245400 - loss: 0.016967553383836875\n",
      "STEP: 245500 - loss: 0.016964049498264015\n",
      "STEP: 245600 - loss: 0.016960546671620752\n",
      "STEP: 245700 - loss: 0.016957044903584968\n",
      "STEP: 245800 - loss: 0.01695354419383436\n",
      "STEP: 245900 - loss: 0.01695004454204732\n",
      "STEP: 246000 - loss: 0.01694654594790206\n",
      "STEP: 246100 - loss: 0.016943048411076436\n",
      "STEP: 246200 - loss: 0.016939551931249096\n",
      "STEP: 246300 - loss: 0.016936056508098353\n",
      "STEP: 246400 - loss: 0.01693256214130293\n",
      "STEP: 246500 - loss: 0.016929068830541422\n",
      "STEP: 246600 - loss: 0.0169255765754925\n",
      "STEP: 246700 - loss: 0.016922085375835\n",
      "STEP: 246800 - loss: 0.01691859523124782\n",
      "STEP: 246900 - loss: 0.016915106141410126\n",
      "STEP: 247000 - loss: 0.016911618106001045\n",
      "STEP: 247100 - loss: 0.016908131124699737\n",
      "STEP: 247200 - loss: 0.016904645197185694\n",
      "STEP: 247300 - loss: 0.01690116032313812\n",
      "STEP: 247400 - loss: 0.01689767650223667\n",
      "STEP: 247500 - loss: 0.01689419373416098\n",
      "STEP: 247600 - loss: 0.016890712018590624\n",
      "STEP: 247700 - loss: 0.016887231355205812\n",
      "STEP: 247800 - loss: 0.016883751743685803\n",
      "STEP: 247900 - loss: 0.01688027318371145\n",
      "STEP: 248000 - loss: 0.016876795674962308\n",
      "STEP: 248100 - loss: 0.01687331921711848\n",
      "STEP: 248200 - loss: 0.016869843809860875\n",
      "STEP: 248300 - loss: 0.016866369452868984\n",
      "STEP: 248400 - loss: 0.016862896145824112\n",
      "STEP: 248500 - loss: 0.016859423888406597\n",
      "STEP: 248600 - loss: 0.016855952680297267\n",
      "STEP: 248700 - loss: 0.01685248252117673\n",
      "STEP: 248800 - loss: 0.016849013410725706\n",
      "STEP: 248900 - loss: 0.016845545348625625\n",
      "STEP: 249000 - loss: 0.016842078334557295\n",
      "STEP: 249100 - loss: 0.016838612368201923\n",
      "STEP: 249200 - loss: 0.016835147449240643\n",
      "STEP: 249300 - loss: 0.01683168357735531\n",
      "STEP: 249400 - loss: 0.016828220752226952\n",
      "STEP: 249500 - loss: 0.016824758973537142\n",
      "STEP: 249600 - loss: 0.016821298240967907\n",
      "STEP: 249700 - loss: 0.016817838554200708\n",
      "STEP: 249800 - loss: 0.016814379912917492\n",
      "STEP: 249900 - loss: 0.01681092231680006\n",
      "STEP: 250000 - loss: 0.01680746576553085\n",
      "STEP: 250100 - loss: 0.016804010258791507\n",
      "STEP: 250200 - loss: 0.016800555796264792\n",
      "STEP: 250300 - loss: 0.016797102377632482\n",
      "STEP: 250400 - loss: 0.01679365000257757\n",
      "STEP: 250500 - loss: 0.016790198670782245\n",
      "STEP: 250600 - loss: 0.016786748381929346\n",
      "STEP: 250700 - loss: 0.016783299135701524\n",
      "STEP: 250800 - loss: 0.016779850931781626\n",
      "STEP: 250900 - loss: 0.016776403769852514\n",
      "STEP: 251000 - loss: 0.016772957649597152\n",
      "STEP: 251100 - loss: 0.016769512570698766\n",
      "STEP: 251200 - loss: 0.01676606853284067\n",
      "STEP: 251300 - loss: 0.016762625535705893\n",
      "STEP: 251400 - loss: 0.01675918357897801\n",
      "STEP: 251500 - loss: 0.016755742662340416\n",
      "STEP: 251600 - loss: 0.016752302785477104\n",
      "STEP: 251700 - loss: 0.016748863948071215\n",
      "STEP: 251800 - loss: 0.016745426149806786\n",
      "STEP: 251900 - loss: 0.016741989390367758\n",
      "STEP: 252000 - loss: 0.016738553669438126\n",
      "STEP: 252100 - loss: 0.016735118986701953\n",
      "STEP: 252200 - loss: 0.01673168534184323\n",
      "STEP: 252300 - loss: 0.016728252734546242\n",
      "STEP: 252400 - loss: 0.016724821164495536\n",
      "STEP: 252500 - loss: 0.016721390631375708\n",
      "STEP: 252600 - loss: 0.01671796113487115\n",
      "STEP: 252700 - loss: 0.016714532674666313\n",
      "STEP: 252800 - loss: 0.016711105250446123\n",
      "STEP: 252900 - loss: 0.016707678861895513\n",
      "STEP: 253000 - loss: 0.016704253508699484\n",
      "STEP: 253100 - loss: 0.01670082919054262\n",
      "STEP: 253200 - loss: 0.016697405907110517\n",
      "STEP: 253300 - loss: 0.016693983658088322\n",
      "STEP: 253400 - loss: 0.016690562443161187\n",
      "STEP: 253500 - loss: 0.01668714226201485\n",
      "STEP: 253600 - loss: 0.016683723114334297\n",
      "STEP: 253700 - loss: 0.01668030499980571\n",
      "STEP: 253800 - loss: 0.016676887918114443\n",
      "STEP: 253900 - loss: 0.016673471868946507\n",
      "STEP: 254000 - loss: 0.01667005685198764\n",
      "STEP: 254100 - loss: 0.01666664286692391\n",
      "STEP: 254200 - loss: 0.016663229913441418\n",
      "STEP: 254300 - loss: 0.016659817991226337\n",
      "STEP: 254400 - loss: 0.01665640709996511\n",
      "STEP: 254500 - loss: 0.016652997239343707\n",
      "STEP: 254600 - loss: 0.016649588409048944\n",
      "STEP: 254700 - loss: 0.016646180608767224\n",
      "STEP: 254800 - loss: 0.016642773838185375\n",
      "STEP: 254900 - loss: 0.01663936809698993\n",
      "STEP: 255000 - loss: 0.016635963384868183\n",
      "STEP: 255100 - loss: 0.016632559701506518\n",
      "STEP: 255200 - loss: 0.016629157046592187\n",
      "STEP: 255300 - loss: 0.016625755419812618\n",
      "STEP: 255400 - loss: 0.016622354820854696\n",
      "STEP: 255500 - loss: 0.016618955249405744\n",
      "STEP: 255600 - loss: 0.01661555670515346\n",
      "STEP: 255700 - loss: 0.016612159187785133\n",
      "STEP: 255800 - loss: 0.016608762696988473\n",
      "STEP: 255900 - loss: 0.016605367232451176\n",
      "STEP: 256000 - loss: 0.016601972793861226\n",
      "STEP: 256100 - loss: 0.016598579380906156\n",
      "STEP: 256200 - loss: 0.016595186993274065\n",
      "STEP: 256300 - loss: 0.01659179563065332\n",
      "STEP: 256400 - loss: 0.01658840529273202\n",
      "STEP: 256500 - loss: 0.016585015979198113\n",
      "STEP: 256600 - loss: 0.016581627689740135\n",
      "STEP: 256700 - loss: 0.016578240424046854\n",
      "STEP: 256800 - loss: 0.016574854181806396\n",
      "STEP: 256900 - loss: 0.01657146896270807\n",
      "STEP: 257000 - loss: 0.0165680847664396\n",
      "STEP: 257100 - loss: 0.016564701592690655\n",
      "STEP: 257200 - loss: 0.016561319441150073\n",
      "STEP: 257300 - loss: 0.016557938311506693\n",
      "STEP: 257400 - loss: 0.016554558203449803\n",
      "STEP: 257500 - loss: 0.01655117911666857\n",
      "STEP: 257600 - loss: 0.01654780105085209\n",
      "STEP: 257700 - loss: 0.016544424005690138\n",
      "STEP: 257800 - loss: 0.016541047980872074\n",
      "STEP: 257900 - loss: 0.01653767297608745\n",
      "STEP: 258000 - loss: 0.016534298991025938\n",
      "STEP: 258100 - loss: 0.016530926025377646\n",
      "STEP: 258200 - loss: 0.016527554078831972\n",
      "STEP: 258300 - loss: 0.016524183151079445\n",
      "STEP: 258400 - loss: 0.016520813241809866\n",
      "STEP: 258500 - loss: 0.01651744435071311\n",
      "STEP: 258600 - loss: 0.01651407647747989\n",
      "STEP: 258700 - loss: 0.016510709621800276\n",
      "STEP: 258800 - loss: 0.016507343783365023\n",
      "STEP: 258900 - loss: 0.016503978961864445\n",
      "STEP: 259000 - loss: 0.016500615156989203\n",
      "STEP: 259100 - loss: 0.016497252368430287\n",
      "STEP: 259200 - loss: 0.016493890595878084\n",
      "STEP: 259300 - loss: 0.016490529839023757\n",
      "STEP: 259400 - loss: 0.01648717009755842\n",
      "STEP: 259500 - loss: 0.016483811371173232\n",
      "STEP: 259600 - loss: 0.016480453659559156\n",
      "STEP: 259700 - loss: 0.01647709696240752\n",
      "STEP: 259800 - loss: 0.016473741279409735\n",
      "STEP: 259900 - loss: 0.016470386610257657\n",
      "STEP: 260000 - loss: 0.016467032954642195\n",
      "STEP: 260100 - loss: 0.016463680312255622\n",
      "STEP: 260200 - loss: 0.01646032868278941\n",
      "STEP: 260300 - loss: 0.01645697806593564\n",
      "STEP: 260400 - loss: 0.01645362846138587\n",
      "STEP: 260500 - loss: 0.016450279868832404\n",
      "STEP: 260600 - loss: 0.016446932287967543\n",
      "STEP: 260700 - loss: 0.016443585718483394\n",
      "STEP: 260800 - loss: 0.016440240160072127\n",
      "STEP: 260900 - loss: 0.0164368956124265\n",
      "STEP: 261000 - loss: 0.016433552075238684\n",
      "STEP: 261100 - loss: 0.01643020954820149\n",
      "STEP: 261200 - loss: 0.01642686803100763\n",
      "STEP: 261300 - loss: 0.016423527523349763\n",
      "STEP: 261400 - loss: 0.01642018802492092\n",
      "STEP: 261500 - loss: 0.016416849535414044\n",
      "STEP: 261600 - loss: 0.016413512054522208\n",
      "STEP: 261700 - loss: 0.016410175581938725\n",
      "STEP: 261800 - loss: 0.0164068401173566\n",
      "STEP: 261900 - loss: 0.01640350566046918\n",
      "STEP: 262000 - loss: 0.016400172210970333\n",
      "STEP: 262100 - loss: 0.01639683976855321\n",
      "STEP: 262200 - loss: 0.01639350833291167\n",
      "STEP: 262300 - loss: 0.016390177903739204\n",
      "STEP: 262400 - loss: 0.016386848480729826\n",
      "STEP: 262500 - loss: 0.016383520063577652\n",
      "STEP: 262600 - loss: 0.016380192651976346\n",
      "STEP: 262700 - loss: 0.016376866245619993\n",
      "STEP: 262800 - loss: 0.016373540844203026\n",
      "STEP: 262900 - loss: 0.016370216447419685\n",
      "STEP: 263000 - loss: 0.016366893054964162\n",
      "STEP: 263100 - loss: 0.01636357066653132\n",
      "STEP: 263200 - loss: 0.016360249281815252\n",
      "STEP: 263300 - loss: 0.016356928900511117\n",
      "STEP: 263400 - loss: 0.016353609522313257\n",
      "STEP: 263500 - loss: 0.016350291146916756\n",
      "STEP: 263600 - loss: 0.016346973774016345\n",
      "STEP: 263700 - loss: 0.016343657403307354\n",
      "STEP: 263800 - loss: 0.01634034203448471\n",
      "STEP: 263900 - loss: 0.016337027667243684\n",
      "STEP: 264000 - loss: 0.016333714301279754\n",
      "STEP: 264100 - loss: 0.01633040193628791\n",
      "STEP: 264200 - loss: 0.016327090571964003\n",
      "STEP: 264300 - loss: 0.01632378020800344\n",
      "STEP: 264400 - loss: 0.016320470844102174\n",
      "STEP: 264500 - loss: 0.01631716247995578\n",
      "STEP: 264600 - loss: 0.016313855115259988\n",
      "STEP: 264700 - loss: 0.016310548749711055\n",
      "STEP: 264800 - loss: 0.016307243383004873\n",
      "STEP: 264900 - loss: 0.016303939014837624\n",
      "STEP: 265000 - loss: 0.01630063564490558\n",
      "STEP: 265100 - loss: 0.016297333272904904\n",
      "STEP: 265200 - loss: 0.016294031898532135\n",
      "STEP: 265300 - loss: 0.016290731521483963\n",
      "STEP: 265400 - loss: 0.016287432141456764\n",
      "STEP: 265500 - loss: 0.016284133758147277\n",
      "STEP: 265600 - loss: 0.016280836371252563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 265700 - loss: 0.016277539980468894\n",
      "STEP: 265800 - loss: 0.016274244585493928\n",
      "STEP: 265900 - loss: 0.01627095018602432\n",
      "STEP: 266000 - loss: 0.01626765678175735\n",
      "STEP: 266100 - loss: 0.016264364372390323\n",
      "STEP: 266200 - loss: 0.016261072957620578\n",
      "STEP: 266300 - loss: 0.0162577825371454\n",
      "STEP: 266400 - loss: 0.016254493110662516\n",
      "STEP: 266500 - loss: 0.01625120467786944\n",
      "STEP: 266600 - loss: 0.016247917238463843\n",
      "STEP: 266700 - loss: 0.01624463079214365\n",
      "STEP: 266800 - loss: 0.016241345338606616\n",
      "STEP: 266900 - loss: 0.01623806087755109\n",
      "STEP: 267000 - loss: 0.016234777408674586\n",
      "STEP: 267100 - loss: 0.01623149493167584\n",
      "STEP: 267200 - loss: 0.016228213446252644\n",
      "STEP: 267300 - loss: 0.016224932952103775\n",
      "STEP: 267400 - loss: 0.016221653448927443\n",
      "STEP: 267500 - loss: 0.016218374936422163\n",
      "STEP: 267600 - loss: 0.016215097414286596\n",
      "STEP: 267700 - loss: 0.016211820882219553\n",
      "STEP: 267800 - loss: 0.016208545339919854\n",
      "STEP: 267900 - loss: 0.016205270787086213\n",
      "STEP: 268000 - loss: 0.016201997223418023\n",
      "STEP: 268100 - loss: 0.016198724648613976\n",
      "STEP: 268200 - loss: 0.016195453062373348\n",
      "STEP: 268300 - loss: 0.01619218246439561\n",
      "STEP: 268400 - loss: 0.016188912854379775\n",
      "STEP: 268500 - loss: 0.016185644232025682\n",
      "STEP: 268600 - loss: 0.01618237659703279\n",
      "STEP: 268700 - loss: 0.01617910994910053\n",
      "STEP: 268800 - loss: 0.01617584428792872\n",
      "STEP: 268900 - loss: 0.01617257961321723\n",
      "STEP: 269000 - loss: 0.016169315924666187\n",
      "STEP: 269100 - loss: 0.016166053221975046\n",
      "STEP: 269200 - loss: 0.016162791504844426\n",
      "STEP: 269300 - loss: 0.016159530772974395\n",
      "STEP: 269400 - loss: 0.016156271026065008\n",
      "STEP: 269500 - loss: 0.016153012263816945\n",
      "STEP: 269600 - loss: 0.01614975448593051\n",
      "STEP: 269700 - loss: 0.016146497692106045\n",
      "STEP: 269800 - loss: 0.01614324188204471\n",
      "STEP: 269900 - loss: 0.01613998705544674\n",
      "STEP: 270000 - loss: 0.016136733212013296\n",
      "STEP: 270100 - loss: 0.01613348035144511\n",
      "STEP: 270200 - loss: 0.01613022847344318\n",
      "STEP: 270300 - loss: 0.01612697757770875\n",
      "STEP: 270400 - loss: 0.016123727663942874\n",
      "STEP: 270500 - loss: 0.016120478731846893\n",
      "STEP: 270600 - loss: 0.016117230781122347\n",
      "STEP: 270700 - loss: 0.016113983811470348\n",
      "STEP: 270800 - loss: 0.016110737822592452\n",
      "STEP: 270900 - loss: 0.016107492814190758\n",
      "STEP: 271000 - loss: 0.016104248785966457\n",
      "STEP: 271100 - loss: 0.016101005737621955\n",
      "STEP: 271200 - loss: 0.016097763668858782\n",
      "STEP: 271300 - loss: 0.01609452257937875\n",
      "STEP: 271400 - loss: 0.016091282468884548\n",
      "STEP: 271500 - loss: 0.016088043337077935\n",
      "STEP: 271600 - loss: 0.01608480518366115\n",
      "STEP: 271700 - loss: 0.016081568008336846\n",
      "STEP: 271800 - loss: 0.016078331810807395\n",
      "STEP: 271900 - loss: 0.016075096590775186\n",
      "STEP: 272000 - loss: 0.01607186234794286\n",
      "STEP: 272100 - loss: 0.016068629082013546\n",
      "STEP: 272200 - loss: 0.016065396792689663\n",
      "STEP: 272300 - loss: 0.016062165479674324\n",
      "STEP: 272400 - loss: 0.016058935142670195\n",
      "STEP: 272500 - loss: 0.016055705781380863\n",
      "STEP: 272600 - loss: 0.016052477395509235\n",
      "STEP: 272700 - loss: 0.01604924998475849\n",
      "STEP: 272800 - loss: 0.01604602354883215\n",
      "STEP: 272900 - loss: 0.016042798087433675\n",
      "STEP: 273000 - loss: 0.01603957360026661\n",
      "STEP: 273100 - loss: 0.016036350087034506\n",
      "STEP: 273200 - loss: 0.016033127547441173\n",
      "STEP: 273300 - loss: 0.016029905981190414\n",
      "STEP: 273400 - loss: 0.016026685387986187\n",
      "STEP: 273500 - loss: 0.016023465767532483\n",
      "STEP: 273600 - loss: 0.01602024711953299\n",
      "STEP: 273700 - loss: 0.01601702944369226\n",
      "STEP: 273800 - loss: 0.016013812739714726\n",
      "STEP: 273900 - loss: 0.016010597007304426\n",
      "STEP: 274000 - loss: 0.016007382246165942\n",
      "STEP: 274100 - loss: 0.01600416845600377\n",
      "STEP: 274200 - loss: 0.016000955636522592\n",
      "STEP: 274300 - loss: 0.015997743787427122\n",
      "STEP: 274400 - loss: 0.0159945329084217\n",
      "STEP: 274500 - loss: 0.0159913229992121\n",
      "STEP: 274600 - loss: 0.015988114059502595\n",
      "STEP: 274700 - loss: 0.015984906088998614\n",
      "STEP: 274800 - loss: 0.015981699087405238\n",
      "STEP: 274900 - loss: 0.015978493054427678\n",
      "STEP: 275000 - loss: 0.015975287989771287\n",
      "STEP: 275100 - loss: 0.015972083893141358\n",
      "STEP: 275200 - loss: 0.01596888076424381\n",
      "STEP: 275300 - loss: 0.015965678602783792\n",
      "STEP: 275400 - loss: 0.01596247740846738\n",
      "STEP: 275500 - loss: 0.01595927718100017\n",
      "STEP: 275600 - loss: 0.01595607792008801\n",
      "STEP: 275700 - loss: 0.015952879625436962\n",
      "STEP: 275800 - loss: 0.015949682296753007\n",
      "STEP: 275900 - loss: 0.01594648593374264\n",
      "STEP: 276000 - loss: 0.01594329053611154\n",
      "STEP: 276100 - loss: 0.015940096103566385\n",
      "STEP: 276200 - loss: 0.015936902635813585\n",
      "STEP: 276300 - loss: 0.015933710132559422\n",
      "STEP: 276400 - loss: 0.015930518593510728\n",
      "STEP: 276500 - loss: 0.015927328018374074\n",
      "STEP: 276600 - loss: 0.01592413840685632\n",
      "STEP: 276700 - loss: 0.015920949758664264\n",
      "STEP: 276800 - loss: 0.015917762073504746\n",
      "STEP: 276900 - loss: 0.015914575351085203\n",
      "STEP: 277000 - loss: 0.015911389591112327\n",
      "STEP: 277100 - loss: 0.01590820479329358\n",
      "STEP: 277200 - loss: 0.015905020957336063\n",
      "STEP: 277300 - loss: 0.01590183808294746\n",
      "STEP: 277400 - loss: 0.015898656169835025\n",
      "STEP: 277500 - loss: 0.015895475217706213\n",
      "STEP: 277600 - loss: 0.015892295226269233\n",
      "STEP: 277700 - loss: 0.015889116195231416\n",
      "STEP: 277800 - loss: 0.015885938124300662\n",
      "STEP: 277900 - loss: 0.015882761013184792\n",
      "STEP: 278000 - loss: 0.01587958486159228\n",
      "STEP: 278100 - loss: 0.015876409669230665\n",
      "STEP: 278200 - loss: 0.0158732354358085\n",
      "STEP: 278300 - loss: 0.015870062161033725\n",
      "STEP: 278400 - loss: 0.015866889844615522\n",
      "STEP: 278500 - loss: 0.015863718486261356\n",
      "STEP: 278600 - loss: 0.015860548085680375\n",
      "STEP: 278700 - loss: 0.01585737864258107\n",
      "STEP: 278800 - loss: 0.01585421015667212\n",
      "STEP: 278900 - loss: 0.01585104262766247\n",
      "STEP: 279000 - loss: 0.015847876055260817\n",
      "STEP: 279100 - loss: 0.015844710439176558\n",
      "STEP: 279200 - loss: 0.015841545779118307\n",
      "STEP: 279300 - loss: 0.01583838207479551\n",
      "STEP: 279400 - loss: 0.015835219325917232\n",
      "STEP: 279500 - loss: 0.015832057532193227\n",
      "STEP: 279600 - loss: 0.015828896693332362\n",
      "STEP: 279700 - loss: 0.015825736809044873\n",
      "STEP: 279800 - loss: 0.01582257787903961\n",
      "STEP: 279900 - loss: 0.015819419903026842\n",
      "STEP: 280000 - loss: 0.015816262880716078\n",
      "STEP: 280100 - loss: 0.01581310681181729\n",
      "STEP: 280200 - loss: 0.015809951696040235\n",
      "STEP: 280300 - loss: 0.01580679753309528\n",
      "STEP: 280400 - loss: 0.015803644322692566\n",
      "STEP: 280500 - loss: 0.015800492064542527\n",
      "STEP: 280600 - loss: 0.015797340758354726\n",
      "STEP: 280700 - loss: 0.015794190403840135\n",
      "STEP: 280800 - loss: 0.015791041000708977\n",
      "STEP: 280900 - loss: 0.01578789254867238\n",
      "STEP: 281000 - loss: 0.01578474504744041\n",
      "STEP: 281100 - loss: 0.015781598496724163\n",
      "STEP: 281200 - loss: 0.01577845289623426\n",
      "STEP: 281300 - loss: 0.015775308245681855\n",
      "STEP: 281400 - loss: 0.015772164544777867\n",
      "STEP: 281500 - loss: 0.01576902179323352\n",
      "STEP: 281600 - loss: 0.015765879990759766\n",
      "STEP: 281700 - loss: 0.015762739137068174\n",
      "STEP: 281800 - loss: 0.015759599231869946\n",
      "STEP: 281900 - loss: 0.01575646027487671\n",
      "STEP: 282000 - loss: 0.015753322265799802\n",
      "STEP: 282100 - loss: 0.015750185204351046\n",
      "STEP: 282200 - loss: 0.015747049090241924\n",
      "STEP: 282300 - loss: 0.015743913923184746\n",
      "STEP: 282400 - loss: 0.015740779702890832\n",
      "STEP: 282500 - loss: 0.01573764642907256\n",
      "STEP: 282600 - loss: 0.015734514101441936\n",
      "STEP: 282700 - loss: 0.01573138271971098\n",
      "STEP: 282800 - loss: 0.015728252283592006\n",
      "STEP: 282900 - loss: 0.015725122792797502\n",
      "STEP: 283000 - loss: 0.015721994247039992\n",
      "STEP: 283100 - loss: 0.01571886664603145\n",
      "STEP: 283200 - loss: 0.015715739989484875\n",
      "STEP: 283300 - loss: 0.015712614277113006\n",
      "STEP: 283400 - loss: 0.015709489508628514\n",
      "STEP: 283500 - loss: 0.01570636568374422\n",
      "STEP: 283600 - loss: 0.01570324280217336\n",
      "STEP: 283700 - loss: 0.015700120863628323\n",
      "STEP: 283800 - loss: 0.015696999867822917\n",
      "STEP: 283900 - loss: 0.01569387981447019\n",
      "STEP: 284000 - loss: 0.015690760703283305\n",
      "STEP: 284100 - loss: 0.015687642533975688\n",
      "STEP: 284200 - loss: 0.015684525306260806\n",
      "STEP: 284300 - loss: 0.015681409019852356\n",
      "STEP: 284400 - loss: 0.015678293674463765\n",
      "STEP: 284500 - loss: 0.015675179269808954\n",
      "STEP: 284600 - loss: 0.015672065805601758\n",
      "STEP: 284700 - loss: 0.015668953281555965\n",
      "STEP: 284800 - loss: 0.01566584169738567\n",
      "STEP: 284900 - loss: 0.015662731052804964\n",
      "STEP: 285000 - loss: 0.015659621347527904\n",
      "STEP: 285100 - loss: 0.015656512581268836\n",
      "STEP: 285200 - loss: 0.015653404753742106\n",
      "STEP: 285300 - loss: 0.01565029786466219\n",
      "STEP: 285400 - loss: 0.015647191913743776\n",
      "STEP: 285500 - loss: 0.015644086900700908\n",
      "STEP: 285600 - loss: 0.015640982825248614\n",
      "STEP: 285700 - loss: 0.01563787968710205\n",
      "STEP: 285800 - loss: 0.015634777485975426\n",
      "STEP: 285900 - loss: 0.01563167622158425\n",
      "STEP: 286000 - loss: 0.015628575893643113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 286100 - loss: 0.0156254765018674\n",
      "STEP: 286200 - loss: 0.015622378045972485\n",
      "STEP: 286300 - loss: 0.015619280525673254\n",
      "STEP: 286400 - loss: 0.015616183940685355\n",
      "STEP: 286500 - loss: 0.015613088290724182\n",
      "STEP: 286600 - loss: 0.015609993575505503\n",
      "STEP: 286700 - loss: 0.015606899794744652\n",
      "STEP: 286800 - loss: 0.015603806948157564\n",
      "STEP: 286900 - loss: 0.015600715035460053\n",
      "STEP: 287000 - loss: 0.01559762405636784\n",
      "STEP: 287100 - loss: 0.015594534010597311\n",
      "STEP: 287200 - loss: 0.015591444897864098\n",
      "STEP: 287300 - loss: 0.015588356717884552\n",
      "STEP: 287400 - loss: 0.015585269470375068\n",
      "STEP: 287500 - loss: 0.015582183155051877\n",
      "STEP: 287600 - loss: 0.015579097771631334\n",
      "STEP: 287700 - loss: 0.015576013319830006\n",
      "STEP: 287800 - loss: 0.01557292979936454\n",
      "STEP: 287900 - loss: 0.015569847209951712\n",
      "STEP: 288000 - loss: 0.015566765551307888\n",
      "STEP: 288100 - loss: 0.01556368482315024\n",
      "STEP: 288200 - loss: 0.01556060502519579\n",
      "STEP: 288300 - loss: 0.015557526157161581\n",
      "STEP: 288400 - loss: 0.015554448218764182\n",
      "STEP: 288500 - loss: 0.015551371209721578\n",
      "STEP: 288600 - loss: 0.015548295129750502\n",
      "STEP: 288700 - loss: 0.015545219978568604\n",
      "STEP: 288800 - loss: 0.015542145755893286\n",
      "STEP: 288900 - loss: 0.015539072461442042\n",
      "STEP: 289000 - loss: 0.015536000094932431\n",
      "STEP: 289100 - loss: 0.015532928656082459\n",
      "STEP: 289200 - loss: 0.015529858144609627\n",
      "STEP: 289300 - loss: 0.015526788560232036\n",
      "STEP: 289400 - loss: 0.01552371990266744\n",
      "STEP: 289500 - loss: 0.01552065217163412\n",
      "STEP: 289600 - loss: 0.015517585366850046\n",
      "STEP: 289700 - loss: 0.015514519488033595\n",
      "STEP: 289800 - loss: 0.015511454534902883\n",
      "STEP: 289900 - loss: 0.015508390507176773\n",
      "STEP: 290000 - loss: 0.015505327404573156\n",
      "STEP: 290100 - loss: 0.015502265226810872\n",
      "STEP: 290200 - loss: 0.015499203973608863\n",
      "STEP: 290300 - loss: 0.015496143644685378\n",
      "STEP: 290400 - loss: 0.015493084239759466\n",
      "STEP: 290500 - loss: 0.015490025758550251\n",
      "STEP: 290600 - loss: 0.015486968200776571\n",
      "STEP: 290700 - loss: 0.015483911566157221\n",
      "STEP: 290800 - loss: 0.015480855854411854\n",
      "STEP: 290900 - loss: 0.015477801065259523\n",
      "STEP: 291000 - loss: 0.015474747198419403\n",
      "STEP: 291100 - loss: 0.015471694253611227\n",
      "STEP: 291200 - loss: 0.015468642230554478\n",
      "STEP: 291300 - loss: 0.015465591128968509\n",
      "STEP: 291400 - loss: 0.015462540948573304\n",
      "STEP: 291500 - loss: 0.01545949168908833\n",
      "STEP: 291600 - loss: 0.015456443350233787\n",
      "STEP: 291700 - loss: 0.015453395931729175\n",
      "STEP: 291800 - loss: 0.015450349433295051\n",
      "STEP: 291900 - loss: 0.015447303854651106\n",
      "STEP: 292000 - loss: 0.015444259195517965\n",
      "STEP: 292100 - loss: 0.015441215455615031\n",
      "STEP: 292200 - loss: 0.01543817263466376\n",
      "STEP: 292300 - loss: 0.015435130732384034\n",
      "STEP: 292400 - loss: 0.015432089748496585\n",
      "STEP: 292500 - loss: 0.015429049682721619\n",
      "STEP: 292600 - loss: 0.015426010534780422\n",
      "STEP: 292700 - loss: 0.015422972304393509\n",
      "STEP: 292800 - loss: 0.015419934991281742\n",
      "STEP: 292900 - loss: 0.015416898595166059\n",
      "STEP: 293000 - loss: 0.015413863115767562\n",
      "STEP: 293100 - loss: 0.01541082855280737\n",
      "STEP: 293200 - loss: 0.015407794906006893\n",
      "STEP: 293300 - loss: 0.015404762175086953\n",
      "STEP: 293400 - loss: 0.015401730359769628\n",
      "STEP: 293500 - loss: 0.01539869945977578\n",
      "STEP: 293600 - loss: 0.015395669474827137\n",
      "STEP: 293700 - loss: 0.015392640404645561\n",
      "STEP: 293800 - loss: 0.01538961224895249\n",
      "STEP: 293900 - loss: 0.015386585007469897\n",
      "STEP: 294000 - loss: 0.015383558679919886\n",
      "STEP: 294100 - loss: 0.015380533266023792\n",
      "STEP: 294200 - loss: 0.01537750876550439\n",
      "STEP: 294300 - loss: 0.015374485178083411\n",
      "STEP: 294400 - loss: 0.01537146250348311\n",
      "STEP: 294500 - loss: 0.015368440741425977\n",
      "STEP: 294600 - loss: 0.015365419891634344\n",
      "STEP: 294700 - loss: 0.015362399953830657\n",
      "STEP: 294800 - loss: 0.015359380927737504\n",
      "STEP: 294900 - loss: 0.015356362813077475\n",
      "STEP: 295000 - loss: 0.015353345609573573\n",
      "STEP: 295100 - loss: 0.015350329316948297\n",
      "STEP: 295200 - loss: 0.015347313934924712\n",
      "STEP: 295300 - loss: 0.01534429946322589\n",
      "STEP: 295400 - loss: 0.015341285901574736\n",
      "STEP: 295500 - loss: 0.01533827324969464\n",
      "STEP: 295600 - loss: 0.01533526150730849\n",
      "STEP: 295700 - loss: 0.015332250674139855\n",
      "STEP: 295800 - loss: 0.015329240749912274\n",
      "STEP: 295900 - loss: 0.01532623173434903\n",
      "STEP: 296000 - loss: 0.01532322362717391\n",
      "STEP: 296100 - loss: 0.015320216428110161\n",
      "STEP: 296200 - loss: 0.015317210136882028\n",
      "STEP: 296300 - loss: 0.015314204753213148\n",
      "STEP: 296400 - loss: 0.015311200276827305\n",
      "STEP: 296500 - loss: 0.015308196707448716\n",
      "STEP: 296600 - loss: 0.015305194044801362\n",
      "STEP: 296700 - loss: 0.015302192288609573\n",
      "STEP: 296800 - loss: 0.015299191438597185\n",
      "STEP: 296900 - loss: 0.015296191494488992\n",
      "STEP: 297000 - loss: 0.015293192456009078\n",
      "STEP: 297100 - loss: 0.0152901943228821\n",
      "STEP: 297200 - loss: 0.015287197094832944\n",
      "STEP: 297300 - loss: 0.015284200771585729\n",
      "STEP: 297400 - loss: 0.015281205352865678\n",
      "STEP: 297500 - loss: 0.015278210838397328\n",
      "STEP: 297600 - loss: 0.015275217227905827\n",
      "STEP: 297700 - loss: 0.015272224521115828\n",
      "STEP: 297800 - loss: 0.015269232717752792\n",
      "STEP: 297900 - loss: 0.015266241817541878\n",
      "STEP: 298000 - loss: 0.015263251820208143\n",
      "STEP: 298100 - loss: 0.01526026272547707\n",
      "STEP: 298200 - loss: 0.015257274533074097\n",
      "STEP: 298300 - loss: 0.015254287242724643\n",
      "STEP: 298400 - loss: 0.01525130085415434\n",
      "STEP: 298500 - loss: 0.015248315367088861\n",
      "STEP: 298600 - loss: 0.015245330781253948\n",
      "STEP: 298700 - loss: 0.015242347096375378\n",
      "STEP: 298800 - loss: 0.015239364312179437\n",
      "STEP: 298900 - loss: 0.015236382428391626\n",
      "STEP: 299000 - loss: 0.015233401444738452\n",
      "STEP: 299100 - loss: 0.015230421360945915\n",
      "STEP: 299200 - loss: 0.015227442176740092\n",
      "STEP: 299300 - loss: 0.015224463891847742\n",
      "STEP: 299400 - loss: 0.015221486505995085\n",
      "STEP: 299500 - loss: 0.015218510018908471\n",
      "STEP: 299600 - loss: 0.01521553443031477\n",
      "STEP: 299700 - loss: 0.015212559739940424\n",
      "STEP: 299800 - loss: 0.015209585947512493\n",
      "STEP: 299900 - loss: 0.015206613052757384\n",
      "STEP: 300000 - loss: 0.015203641055402487\n",
      "STEP: 300100 - loss: 0.015200669955174573\n",
      "STEP: 300200 - loss: 0.015197699751800658\n",
      "STEP: 300300 - loss: 0.015194730445008084\n",
      "STEP: 300400 - loss: 0.015191762034524154\n",
      "STEP: 300500 - loss: 0.01518879452007609\n",
      "STEP: 300600 - loss: 0.015185827901391159\n",
      "STEP: 300700 - loss: 0.015182862178197334\n",
      "STEP: 300800 - loss: 0.015179897350221683\n",
      "STEP: 300900 - loss: 0.015176933417192163\n",
      "STEP: 301000 - loss: 0.015173970378836488\n",
      "STEP: 301100 - loss: 0.015171008234882492\n",
      "STEP: 301200 - loss: 0.01516804698505789\n",
      "STEP: 301300 - loss: 0.015165086629091318\n",
      "STEP: 301400 - loss: 0.015162127166710171\n",
      "STEP: 301500 - loss: 0.0151591685976428\n",
      "STEP: 301600 - loss: 0.015156210921617935\n",
      "STEP: 301700 - loss: 0.015153254138363367\n",
      "STEP: 301800 - loss: 0.015150298247607438\n",
      "STEP: 301900 - loss: 0.01514734324907913\n",
      "STEP: 302000 - loss: 0.015144389142506811\n",
      "STEP: 302100 - loss: 0.015141435927618832\n",
      "STEP: 302200 - loss: 0.015138483604144302\n",
      "STEP: 302300 - loss: 0.015135532171812332\n",
      "STEP: 302400 - loss: 0.015132581630351055\n",
      "STEP: 302500 - loss: 0.015129631979490046\n",
      "STEP: 302600 - loss: 0.015126683218958428\n",
      "STEP: 302700 - loss: 0.015123735348484924\n",
      "STEP: 302800 - loss: 0.015120788367799088\n",
      "STEP: 302900 - loss: 0.015117842276629994\n",
      "STEP: 303000 - loss: 0.015114897074707706\n",
      "STEP: 303100 - loss: 0.015111952761760852\n",
      "STEP: 303200 - loss: 0.015109009337519446\n",
      "STEP: 303300 - loss: 0.015106066801713234\n",
      "STEP: 303400 - loss: 0.015103125154071742\n",
      "STEP: 303500 - loss: 0.015100184394324687\n",
      "STEP: 303600 - loss: 0.015097244522202569\n",
      "STEP: 303700 - loss: 0.015094305537434677\n",
      "STEP: 303800 - loss: 0.01509136743975133\n",
      "STEP: 303900 - loss: 0.01508843022888296\n",
      "STEP: 304000 - loss: 0.015085493904559174\n",
      "STEP: 304100 - loss: 0.01508255846651095\n",
      "STEP: 304200 - loss: 0.015079623914468366\n",
      "STEP: 304300 - loss: 0.015076690248161561\n",
      "STEP: 304400 - loss: 0.015073757467321891\n",
      "STEP: 304500 - loss: 0.015070825571679259\n",
      "STEP: 304600 - loss: 0.015067894560964662\n",
      "STEP: 304700 - loss: 0.015064964434909114\n",
      "STEP: 304800 - loss: 0.015062035193243142\n",
      "STEP: 304900 - loss: 0.015059106835698005\n",
      "STEP: 305000 - loss: 0.015056179362004563\n",
      "STEP: 305100 - loss: 0.015053252771894135\n",
      "STEP: 305200 - loss: 0.015050327065097685\n",
      "STEP: 305300 - loss: 0.015047402241346614\n",
      "STEP: 305400 - loss: 0.015044478300372467\n",
      "STEP: 305500 - loss: 0.015041555241906581\n",
      "STEP: 305600 - loss: 0.015038633065680268\n",
      "STEP: 305700 - loss: 0.015035711771425537\n",
      "STEP: 305800 - loss: 0.015032791358873723\n",
      "STEP: 305900 - loss: 0.015029871827757035\n",
      "STEP: 306000 - loss: 0.01502695317780709\n",
      "STEP: 306100 - loss: 0.015024035408755871\n",
      "STEP: 306200 - loss: 0.01502111852033539\n",
      "STEP: 306300 - loss: 0.015018202512277559\n",
      "STEP: 306400 - loss: 0.015015287384314768\n",
      "STEP: 306500 - loss: 0.0150123731361796\n",
      "STEP: 306600 - loss: 0.015009459767603938\n",
      "STEP: 306700 - loss: 0.015006547278320423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 306800 - loss: 0.015003635668061602\n",
      "STEP: 306900 - loss: 0.01500072493656\n",
      "STEP: 307000 - loss: 0.014997815083548314\n",
      "STEP: 307100 - loss: 0.014994906108759188\n",
      "STEP: 307200 - loss: 0.014991998011925666\n",
      "STEP: 307300 - loss: 0.014989090792780666\n",
      "STEP: 307400 - loss: 0.014986184451057044\n",
      "STEP: 307500 - loss: 0.014983278986488208\n",
      "STEP: 307600 - loss: 0.014980374398806956\n",
      "STEP: 307700 - loss: 0.014977470687746458\n",
      "STEP: 307800 - loss: 0.014974567853040624\n",
      "STEP: 307900 - loss: 0.014971665894422295\n",
      "STEP: 308000 - loss: 0.014968764811625101\n",
      "STEP: 308100 - loss: 0.014965864604382956\n",
      "STEP: 308200 - loss: 0.014962965272428862\n",
      "STEP: 308300 - loss: 0.014960066815497292\n",
      "STEP: 308400 - loss: 0.014957169233321705\n",
      "STEP: 308500 - loss: 0.014954272525636076\n",
      "STEP: 308600 - loss: 0.014951376692173932\n",
      "STEP: 308700 - loss: 0.014948481732670105\n",
      "STEP: 308800 - loss: 0.014945587646858289\n",
      "STEP: 308900 - loss: 0.014942694434472803\n",
      "STEP: 309000 - loss: 0.01493980209524771\n",
      "STEP: 309100 - loss: 0.014936910628917851\n",
      "STEP: 309200 - loss: 0.014934020035217394\n",
      "STEP: 309300 - loss: 0.014931130313880894\n",
      "STEP: 309400 - loss: 0.01492824146464317\n",
      "STEP: 309500 - loss: 0.014925353487238755\n",
      "STEP: 309600 - loss: 0.014922466381402247\n",
      "STEP: 309700 - loss: 0.014919580146868842\n",
      "STEP: 309800 - loss: 0.01491669478337355\n",
      "STEP: 309900 - loss: 0.014913810290651013\n",
      "STEP: 310000 - loss: 0.014910926668436773\n",
      "STEP: 310100 - loss: 0.014908043916465866\n",
      "STEP: 310200 - loss: 0.01490516203447335\n",
      "STEP: 310300 - loss: 0.014902281022194758\n",
      "STEP: 310400 - loss: 0.01489940087936561\n",
      "STEP: 310500 - loss: 0.014896521605721167\n",
      "STEP: 310600 - loss: 0.014893643200997416\n",
      "STEP: 310700 - loss: 0.01489076566492947\n",
      "STEP: 310800 - loss: 0.01488788899725366\n",
      "STEP: 310900 - loss: 0.014885013197705651\n",
      "STEP: 311000 - loss: 0.014882138266021009\n",
      "STEP: 311100 - loss: 0.014879264201935994\n",
      "STEP: 311200 - loss: 0.014876391005186807\n",
      "STEP: 311300 - loss: 0.014873518675509578\n",
      "STEP: 311400 - loss: 0.0148706472126404\n",
      "STEP: 311500 - loss: 0.014867776616315543\n",
      "STEP: 311600 - loss: 0.014864906886271623\n",
      "STEP: 311700 - loss: 0.014862038022244894\n",
      "STEP: 311800 - loss: 0.014859170023972296\n",
      "STEP: 311900 - loss: 0.014856302891189863\n",
      "STEP: 312000 - loss: 0.014853436623634652\n",
      "STEP: 312100 - loss: 0.014850571221043597\n",
      "STEP: 312200 - loss: 0.014847706683153402\n",
      "STEP: 312300 - loss: 0.014844843009700945\n",
      "STEP: 312400 - loss: 0.01484198020042363\n",
      "STEP: 312500 - loss: 0.01483911825505802\n",
      "STEP: 312600 - loss: 0.014836257173341744\n",
      "STEP: 312700 - loss: 0.014833396955012118\n",
      "STEP: 312800 - loss: 0.014830537599806088\n",
      "STEP: 312900 - loss: 0.014827679107461448\n",
      "STEP: 313000 - loss: 0.014824821477715739\n",
      "STEP: 313100 - loss: 0.014821964710306154\n",
      "STEP: 313200 - loss: 0.01481910880497076\n",
      "STEP: 313300 - loss: 0.014816253761447198\n",
      "STEP: 313400 - loss: 0.014813399579473387\n",
      "STEP: 313500 - loss: 0.014810546258787093\n",
      "STEP: 313600 - loss: 0.014807693799126304\n",
      "STEP: 313700 - loss: 0.014804842200229531\n",
      "STEP: 313800 - loss: 0.014801991461834082\n",
      "STEP: 313900 - loss: 0.01479914158367894\n",
      "STEP: 314000 - loss: 0.014796292565502115\n",
      "STEP: 314100 - loss: 0.01479344440704213\n",
      "STEP: 314200 - loss: 0.01479059710803709\n",
      "STEP: 314300 - loss: 0.014787750668226133\n",
      "STEP: 314400 - loss: 0.01478490508734769\n",
      "STEP: 314500 - loss: 0.014782060365140143\n",
      "STEP: 314600 - loss: 0.01477921650134271\n",
      "STEP: 314700 - loss: 0.014776373495694115\n",
      "STEP: 314800 - loss: 0.014773531347933185\n",
      "STEP: 314900 - loss: 0.014770690057799084\n",
      "STEP: 315000 - loss: 0.01476784962503091\n",
      "STEP: 315100 - loss: 0.014765010049367705\n",
      "STEP: 315200 - loss: 0.014762171330549034\n",
      "STEP: 315300 - loss: 0.014759333468313785\n",
      "STEP: 315400 - loss: 0.014756496462401871\n",
      "STEP: 315500 - loss: 0.014753660312552864\n",
      "STEP: 315600 - loss: 0.014750825018505628\n",
      "STEP: 315700 - loss: 0.014747990580000666\n",
      "STEP: 315800 - loss: 0.014745156996777259\n",
      "STEP: 315900 - loss: 0.014742324268575005\n",
      "STEP: 316000 - loss: 0.01473949239513465\n",
      "STEP: 316100 - loss: 0.01473666137619522\n",
      "STEP: 316200 - loss: 0.01473383121149724\n",
      "STEP: 316300 - loss: 0.0147310019007811\n",
      "STEP: 316400 - loss: 0.014728173443786496\n",
      "STEP: 316500 - loss: 0.014725345840254048\n",
      "STEP: 316600 - loss: 0.014722519089924238\n",
      "STEP: 316700 - loss: 0.014719693192537135\n",
      "STEP: 316800 - loss: 0.014716868147833453\n",
      "STEP: 316900 - loss: 0.014714043955553938\n",
      "STEP: 317000 - loss: 0.014711220615439308\n",
      "STEP: 317100 - loss: 0.01470839812723011\n",
      "STEP: 317200 - loss: 0.014705576490666972\n",
      "STEP: 317300 - loss: 0.014702755705491505\n",
      "STEP: 317400 - loss: 0.014699935771444229\n",
      "STEP: 317500 - loss: 0.014697116688266506\n",
      "STEP: 317600 - loss: 0.014694298455699176\n",
      "STEP: 317700 - loss: 0.014691481073483855\n",
      "STEP: 317800 - loss: 0.014688664541361663\n",
      "STEP: 317900 - loss: 0.014685848859073824\n",
      "STEP: 318000 - loss: 0.014683034026362247\n",
      "STEP: 318100 - loss: 0.014680220042968145\n",
      "STEP: 318200 - loss: 0.014677406908633086\n",
      "STEP: 318300 - loss: 0.014674594623099408\n",
      "STEP: 318400 - loss: 0.014671783186108192\n",
      "STEP: 318500 - loss: 0.014668972597401578\n",
      "STEP: 318600 - loss: 0.014666162856721754\n",
      "STEP: 318700 - loss: 0.014663353963810327\n",
      "STEP: 318800 - loss: 0.014660545918409599\n",
      "STEP: 318900 - loss: 0.014657738720261881\n",
      "STEP: 319000 - loss: 0.014654932369109374\n",
      "STEP: 319100 - loss: 0.014652126864694331\n",
      "STEP: 319200 - loss: 0.01464932220675926\n",
      "STEP: 319300 - loss: 0.014646518395046567\n",
      "STEP: 319400 - loss: 0.0146437154292989\n",
      "STEP: 319500 - loss: 0.014640913309258801\n",
      "STEP: 319600 - loss: 0.014638112034669281\n",
      "STEP: 319700 - loss: 0.014635311605272852\n",
      "STEP: 319800 - loss: 0.014632512020812912\n",
      "STEP: 319900 - loss: 0.014629713281031706\n",
      "STEP: 320000 - loss: 0.014626915385672767\n",
      "STEP: 320100 - loss: 0.01462411833447902\n",
      "STEP: 320200 - loss: 0.014621322127193876\n",
      "STEP: 320300 - loss: 0.014618526763560503\n",
      "STEP: 320400 - loss: 0.014615732243322244\n",
      "STEP: 320500 - loss: 0.014612938566222611\n",
      "STEP: 320600 - loss: 0.014610145732004956\n",
      "STEP: 320700 - loss: 0.014607353740413161\n",
      "STEP: 320800 - loss: 0.01460456259119059\n",
      "STEP: 320900 - loss: 0.014601772284081055\n",
      "STEP: 321000 - loss: 0.014598982818828607\n",
      "STEP: 321100 - loss: 0.014596194195177051\n",
      "STEP: 321200 - loss: 0.014593406412870227\n",
      "STEP: 321300 - loss: 0.014590619471652345\n",
      "STEP: 321400 - loss: 0.014587833371267452\n",
      "STEP: 321500 - loss: 0.014585048111459987\n",
      "STEP: 321600 - loss: 0.014582263691974014\n",
      "STEP: 321700 - loss: 0.014579480112554047\n",
      "STEP: 321800 - loss: 0.014576697372944295\n",
      "STEP: 321900 - loss: 0.014573915472889604\n",
      "STEP: 322000 - loss: 0.014571134412134406\n",
      "STEP: 322100 - loss: 0.01456835419042354\n",
      "STEP: 322200 - loss: 0.014565574807501447\n",
      "STEP: 322300 - loss: 0.014562796263113468\n",
      "STEP: 322400 - loss: 0.014560018557004136\n",
      "STEP: 322500 - loss: 0.01455724168891842\n",
      "STEP: 322600 - loss: 0.014554465658601645\n",
      "STEP: 322700 - loss: 0.01455169046579886\n",
      "STEP: 322800 - loss: 0.014548916110255111\n",
      "STEP: 322900 - loss: 0.01454614259171622\n",
      "STEP: 323000 - loss: 0.014543369909926964\n",
      "STEP: 323100 - loss: 0.014540598064633084\n",
      "STEP: 323200 - loss: 0.014537827055580136\n",
      "STEP: 323300 - loss: 0.014535056882513706\n",
      "STEP: 323400 - loss: 0.014532287545179809\n",
      "STEP: 323500 - loss: 0.014529519043323466\n",
      "STEP: 323600 - loss: 0.014526751376691006\n",
      "STEP: 323700 - loss: 0.014523984545028259\n",
      "STEP: 323800 - loss: 0.014521218548081648\n",
      "STEP: 323900 - loss: 0.014518453385596573\n",
      "STEP: 324000 - loss: 0.014515689057319656\n",
      "STEP: 324100 - loss: 0.014512925562996857\n",
      "STEP: 324200 - loss: 0.014510162902374663\n",
      "STEP: 324300 - loss: 0.014507401075199447\n",
      "STEP: 324400 - loss: 0.014504640081217579\n",
      "STEP: 324500 - loss: 0.014501879920175589\n",
      "STEP: 324600 - loss: 0.014499120591820616\n",
      "STEP: 324700 - loss: 0.014496362095898501\n",
      "STEP: 324800 - loss: 0.014493604432156415\n",
      "STEP: 324900 - loss: 0.01449084760034158\n",
      "STEP: 325000 - loss: 0.014488091600200207\n",
      "STEP: 325100 - loss: 0.014485336431479878\n",
      "STEP: 325200 - loss: 0.01448258209392736\n",
      "STEP: 325300 - loss: 0.014479828587290102\n",
      "STEP: 325400 - loss: 0.014477075911315085\n",
      "STEP: 325500 - loss: 0.014474324065749639\n",
      "STEP: 325600 - loss: 0.014471573050341436\n",
      "STEP: 325700 - loss: 0.014468822864837685\n",
      "STEP: 325800 - loss: 0.014466073508985818\n",
      "STEP: 325900 - loss: 0.014463324982533571\n",
      "STEP: 326000 - loss: 0.014460577285229004\n",
      "STEP: 326100 - loss: 0.014457830416819301\n",
      "STEP: 326200 - loss: 0.014455084377052735\n",
      "STEP: 326300 - loss: 0.01445233916567704\n",
      "STEP: 326400 - loss: 0.014449594782440267\n",
      "STEP: 326500 - loss: 0.014446851227090562\n",
      "STEP: 326600 - loss: 0.014444108499375987\n",
      "STEP: 326700 - loss: 0.01444136659904482\n",
      "STEP: 326800 - loss: 0.014438625525845308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 326900 - loss: 0.014435885279525926\n",
      "STEP: 327000 - loss: 0.014433145859835326\n",
      "STEP: 327100 - loss: 0.014430407266521571\n",
      "STEP: 327200 - loss: 0.014427669499333676\n",
      "STEP: 327300 - loss: 0.014424932558020371\n",
      "STEP: 327400 - loss: 0.014422196442329931\n",
      "STEP: 327500 - loss: 0.01441946115201181\n",
      "STEP: 327600 - loss: 0.014416726686814391\n",
      "STEP: 327700 - loss: 0.014413993046487262\n",
      "STEP: 327800 - loss: 0.01441126023077897\n",
      "STEP: 327900 - loss: 0.01440852823943876\n",
      "STEP: 328000 - loss: 0.014405797072216254\n",
      "STEP: 328100 - loss: 0.014403066728860464\n",
      "STEP: 328200 - loss: 0.014400337209120723\n",
      "STEP: 328300 - loss: 0.014397608512746579\n",
      "STEP: 328400 - loss: 0.014394880639487503\n",
      "STEP: 328500 - loss: 0.014392153589093157\n",
      "STEP: 328600 - loss: 0.014389427361313259\n",
      "STEP: 328700 - loss: 0.014386701955897455\n",
      "STEP: 328800 - loss: 0.014383977372595776\n",
      "STEP: 328900 - loss: 0.01438125361115806\n",
      "STEP: 329000 - loss: 0.014378530671334027\n",
      "STEP: 329100 - loss: 0.014375808552874144\n",
      "STEP: 329200 - loss: 0.0143730872555284\n",
      "STEP: 329300 - loss: 0.014370366779047138\n",
      "STEP: 329400 - loss: 0.0143676471231803\n",
      "STEP: 329500 - loss: 0.014364928287678552\n",
      "STEP: 329600 - loss: 0.014362210272292291\n",
      "STEP: 329700 - loss: 0.014359493076772012\n",
      "STEP: 329800 - loss: 0.014356776700868212\n",
      "STEP: 329900 - loss: 0.014354061144331803\n",
      "STEP: 330000 - loss: 0.014351346406913401\n",
      "STEP: 330100 - loss: 0.014348632488363943\n",
      "STEP: 330200 - loss: 0.01434591938843389\n",
      "STEP: 330300 - loss: 0.014343207106874997\n",
      "STEP: 330400 - loss: 0.014340495643437645\n",
      "STEP: 330500 - loss: 0.014337784997873254\n",
      "STEP: 330600 - loss: 0.014335075169932713\n",
      "STEP: 330700 - loss: 0.014332366159367931\n",
      "STEP: 330800 - loss: 0.014329657965929868\n",
      "STEP: 330900 - loss: 0.01432695058936985\n",
      "STEP: 331000 - loss: 0.01432424402943945\n",
      "STEP: 331100 - loss: 0.014321538285890539\n",
      "STEP: 331200 - loss: 0.01431883335847449\n",
      "STEP: 331300 - loss: 0.014316129246943038\n",
      "STEP: 331400 - loss: 0.014313425951048111\n",
      "STEP: 331500 - loss: 0.014310723470541378\n",
      "STEP: 331600 - loss: 0.014308021805175403\n",
      "STEP: 331700 - loss: 0.014305320954701509\n",
      "STEP: 331800 - loss: 0.014302620918872097\n",
      "STEP: 331900 - loss: 0.014299921697439318\n",
      "STEP: 332000 - loss: 0.014297223290155658\n",
      "STEP: 332100 - loss: 0.014294525696773\n",
      "STEP: 332200 - loss: 0.014291828917044355\n",
      "STEP: 332300 - loss: 0.014289132950721876\n",
      "STEP: 332400 - loss: 0.014286437797557682\n",
      "STEP: 332500 - loss: 0.014283743457305162\n",
      "STEP: 332600 - loss: 0.01428104992971671\n",
      "STEP: 332700 - loss: 0.014278357214545245\n",
      "STEP: 332800 - loss: 0.014275665311543432\n",
      "STEP: 332900 - loss: 0.01427297422046431\n",
      "STEP: 333000 - loss: 0.014270283941060904\n",
      "STEP: 333100 - loss: 0.014267594473086098\n",
      "STEP: 333200 - loss: 0.014264905816293597\n",
      "STEP: 333300 - loss: 0.014262217970436013\n",
      "STEP: 333400 - loss: 0.014259530935267054\n",
      "STEP: 333500 - loss: 0.014256844710539975\n",
      "STEP: 333600 - loss: 0.014254159296008494\n",
      "STEP: 333700 - loss: 0.014251474691425731\n",
      "STEP: 333800 - loss: 0.014248790896545469\n",
      "STEP: 333900 - loss: 0.014246107911121662\n",
      "STEP: 334000 - loss: 0.014243425734907677\n",
      "STEP: 334100 - loss: 0.014240744367657623\n",
      "STEP: 334200 - loss: 0.014238063809125253\n",
      "STEP: 334300 - loss: 0.014235384059064434\n",
      "STEP: 334400 - loss: 0.014232705117229646\n",
      "STEP: 334500 - loss: 0.014230026983374864\n",
      "STEP: 334600 - loss: 0.014227349657254048\n",
      "STEP: 334700 - loss: 0.014224673138621902\n",
      "STEP: 334800 - loss: 0.014221997427232402\n",
      "STEP: 334900 - loss: 0.014219322522840255\n",
      "STEP: 335000 - loss: 0.014216648425199967\n",
      "STEP: 335100 - loss: 0.014213975134065826\n",
      "STEP: 335200 - loss: 0.01421130264919277\n",
      "STEP: 335300 - loss: 0.014208630970335607\n",
      "STEP: 335400 - loss: 0.014205960097248912\n",
      "STEP: 335500 - loss: 0.01420329002968779\n",
      "STEP: 335600 - loss: 0.014200620767407066\n",
      "STEP: 335700 - loss: 0.014197952310161749\n",
      "STEP: 335800 - loss: 0.014195284657707082\n",
      "STEP: 335900 - loss: 0.014192617809798042\n",
      "STEP: 336000 - loss: 0.014189951766190107\n",
      "STEP: 336100 - loss: 0.014187286526638487\n",
      "STEP: 336200 - loss: 0.014184622090898661\n",
      "STEP: 336300 - loss: 0.01418195845872634\n",
      "STEP: 336400 - loss: 0.014179295629876581\n",
      "STEP: 336500 - loss: 0.014176633604105352\n",
      "STEP: 336600 - loss: 0.01417397238116811\n",
      "STEP: 336700 - loss: 0.0141713119608208\n",
      "STEP: 336800 - loss: 0.014168652342819521\n",
      "STEP: 336900 - loss: 0.014165993526919756\n",
      "STEP: 337000 - loss: 0.01416333551287785\n",
      "STEP: 337100 - loss: 0.014160678300449685\n",
      "STEP: 337200 - loss: 0.014158021889391378\n",
      "STEP: 337300 - loss: 0.014155366279459505\n",
      "STEP: 337400 - loss: 0.014152711470409782\n",
      "STEP: 337500 - loss: 0.014150057461998964\n",
      "STEP: 337600 - loss: 0.014147404253983522\n",
      "STEP: 337700 - loss: 0.014144751846119944\n",
      "STEP: 337800 - loss: 0.014142100238164975\n",
      "STEP: 337900 - loss: 0.014139449429874756\n",
      "STEP: 338000 - loss: 0.014136799421006572\n",
      "STEP: 338100 - loss: 0.01413415021131711\n",
      "STEP: 338200 - loss: 0.01413150180056304\n",
      "STEP: 338300 - loss: 0.014128854188501433\n",
      "STEP: 338400 - loss: 0.0141262073748895\n",
      "STEP: 338500 - loss: 0.014123561359484314\n",
      "STEP: 338600 - loss: 0.014120916142042953\n",
      "STEP: 338700 - loss: 0.01411827172232254\n",
      "STEP: 338800 - loss: 0.014115628100080723\n",
      "STEP: 338900 - loss: 0.014112985275074841\n",
      "STEP: 339000 - loss: 0.014110343247062132\n",
      "STEP: 339100 - loss: 0.014107702015800494\n",
      "STEP: 339200 - loss: 0.014105061581047448\n",
      "STEP: 339300 - loss: 0.01410242194256058\n",
      "STEP: 339400 - loss: 0.014099783100097841\n",
      "STEP: 339500 - loss: 0.01409714505341681\n",
      "STEP: 339600 - loss: 0.014094507802275686\n",
      "STEP: 339700 - loss: 0.014091871346432295\n",
      "STEP: 339800 - loss: 0.014089235685644902\n",
      "STEP: 339900 - loss: 0.014086600819671502\n",
      "STEP: 340000 - loss: 0.014083966748270482\n",
      "STEP: 340100 - loss: 0.014081333471199766\n",
      "STEP: 340200 - loss: 0.014078700988218151\n",
      "STEP: 340300 - loss: 0.014076069299083869\n",
      "STEP: 340400 - loss: 0.014073438403555424\n",
      "STEP: 340500 - loss: 0.014070808301391591\n",
      "STEP: 340600 - loss: 0.01406817899235067\n",
      "STEP: 340700 - loss: 0.01406555047619171\n",
      "STEP: 340800 - loss: 0.014062922752673323\n",
      "STEP: 340900 - loss: 0.014060295821554719\n",
      "STEP: 341000 - loss: 0.014057669682594398\n",
      "STEP: 341100 - loss: 0.014055044335551747\n",
      "STEP: 341200 - loss: 0.014052419780185569\n",
      "STEP: 341300 - loss: 0.014049796016255434\n",
      "STEP: 341400 - loss: 0.014047173043520367\n",
      "STEP: 341500 - loss: 0.014044550861739512\n",
      "STEP: 341600 - loss: 0.014041929470672426\n",
      "STEP: 341700 - loss: 0.01403930887007882\n",
      "STEP: 341800 - loss: 0.014036689059717763\n",
      "STEP: 341900 - loss: 0.0140340700393492\n",
      "STEP: 342000 - loss: 0.014031451808732835\n",
      "STEP: 342100 - loss: 0.014028834367628215\n",
      "STEP: 342200 - loss: 0.014026217715795408\n",
      "STEP: 342300 - loss: 0.014023601852993983\n",
      "STEP: 342400 - loss: 0.014020986778984371\n",
      "STEP: 342500 - loss: 0.014018372493526318\n",
      "STEP: 342600 - loss: 0.01401575899638009\n",
      "STEP: 342700 - loss: 0.014013146287305664\n",
      "STEP: 342800 - loss: 0.014010534366063518\n",
      "STEP: 342900 - loss: 0.014007923232414005\n",
      "STEP: 343000 - loss: 0.014005312886117494\n",
      "STEP: 343100 - loss: 0.01400270332693443\n",
      "STEP: 343200 - loss: 0.014000094554625392\n",
      "STEP: 343300 - loss: 0.013997486568951199\n",
      "STEP: 343400 - loss: 0.013994879369672034\n",
      "STEP: 343500 - loss: 0.013992272956549503\n",
      "STEP: 343600 - loss: 0.01398966732934386\n",
      "STEP: 343700 - loss: 0.013987062487816056\n",
      "STEP: 343800 - loss: 0.013984458431727412\n",
      "STEP: 343900 - loss: 0.013981855160838599\n",
      "STEP: 344000 - loss: 0.013979252674910986\n",
      "STEP: 344100 - loss: 0.013976650973706062\n",
      "STEP: 344200 - loss: 0.013974050056984457\n",
      "STEP: 344300 - loss: 0.013971449924508073\n",
      "STEP: 344400 - loss: 0.013968850576038064\n",
      "STEP: 344500 - loss: 0.013966252011336328\n",
      "STEP: 344600 - loss: 0.013963654230164022\n",
      "STEP: 344700 - loss: 0.013961057232282961\n",
      "STEP: 344800 - loss: 0.013958461017454866\n",
      "STEP: 344900 - loss: 0.013955865585441488\n",
      "STEP: 345000 - loss: 0.013953270936004897\n",
      "STEP: 345100 - loss: 0.013950677068906705\n",
      "STEP: 345200 - loss: 0.013948083983909287\n",
      "STEP: 345300 - loss: 0.013945491680774375\n",
      "STEP: 345400 - loss: 0.013942900159264507\n",
      "STEP: 345500 - loss: 0.013940309419141619\n",
      "STEP: 345600 - loss: 0.013937719460168096\n",
      "STEP: 345700 - loss: 0.013935130282106412\n",
      "STEP: 345800 - loss: 0.013932541884718857\n",
      "STEP: 345900 - loss: 0.01392995426776809\n",
      "STEP: 346000 - loss: 0.013927367431016835\n",
      "STEP: 346100 - loss: 0.013924781374227358\n",
      "STEP: 346200 - loss: 0.013922196097162737\n",
      "STEP: 346300 - loss: 0.01391961159958557\n",
      "STEP: 346400 - loss: 0.013917027881259014\n",
      "STEP: 346500 - loss: 0.013914444941945884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 346600 - loss: 0.013911862781408959\n",
      "STEP: 346700 - loss: 0.013909281399411357\n",
      "STEP: 346800 - loss: 0.013906700795716731\n",
      "STEP: 346900 - loss: 0.013904120970088201\n",
      "STEP: 347000 - loss: 0.013901541922288893\n",
      "STEP: 347100 - loss: 0.0138989636520821\n",
      "STEP: 347200 - loss: 0.013896386159231482\n",
      "STEP: 347300 - loss: 0.013893809443500493\n",
      "STEP: 347400 - loss: 0.013891233504652756\n",
      "STEP: 347500 - loss: 0.013888658342451907\n",
      "STEP: 347600 - loss: 0.013886083956661788\n",
      "STEP: 347700 - loss: 0.013883510347046057\n",
      "STEP: 347800 - loss: 0.013880937513368819\n",
      "STEP: 347900 - loss: 0.013878365455394007\n",
      "STEP: 348000 - loss: 0.01387579417288525\n",
      "STEP: 348100 - loss: 0.013873223665607384\n",
      "STEP: 348200 - loss: 0.013870653933324007\n",
      "STEP: 348300 - loss: 0.013868084975799364\n",
      "STEP: 348400 - loss: 0.01386551679279821\n",
      "STEP: 348500 - loss: 0.013862949384084583\n",
      "STEP: 348600 - loss: 0.01386038274942305\n",
      "STEP: 348700 - loss: 0.013857816888578369\n",
      "STEP: 348800 - loss: 0.013855251801314892\n",
      "STEP: 348900 - loss: 0.013852687487397214\n",
      "STEP: 349000 - loss: 0.013850123946590362\n",
      "STEP: 349100 - loss: 0.01384756117865884\n",
      "STEP: 349200 - loss: 0.01384499918336805\n",
      "STEP: 349300 - loss: 0.013842437960482224\n",
      "STEP: 349400 - loss: 0.013839877509766969\n",
      "STEP: 349500 - loss: 0.013837317830987244\n",
      "STEP: 349600 - loss: 0.01383475892390838\n",
      "STEP: 349700 - loss: 0.013832200788295109\n",
      "STEP: 349800 - loss: 0.013829643423913431\n",
      "STEP: 349900 - loss: 0.013827086830528228\n",
      "STEP: 350000 - loss: 0.013824531007905438\n",
      "STEP: 350100 - loss: 0.013821975955810056\n",
      "STEP: 350200 - loss: 0.013819421674008209\n",
      "STEP: 350300 - loss: 0.013816868162265192\n",
      "STEP: 350400 - loss: 0.013814315420346824\n",
      "STEP: 350500 - loss: 0.013811763448019237\n",
      "STEP: 350600 - loss: 0.013809212245047827\n",
      "STEP: 350700 - loss: 0.013806661811199137\n",
      "STEP: 350800 - loss: 0.013804112146238692\n",
      "STEP: 350900 - loss: 0.013801563249932891\n",
      "STEP: 351000 - loss: 0.013799015122047726\n",
      "STEP: 351100 - loss: 0.01379646776234976\n",
      "STEP: 351200 - loss: 0.013793921170605006\n",
      "STEP: 351300 - loss: 0.013791375346579848\n",
      "STEP: 351400 - loss: 0.013788830290041094\n",
      "STEP: 351500 - loss: 0.013786286000754966\n",
      "STEP: 351600 - loss: 0.013783742478487872\n",
      "STEP: 351700 - loss: 0.013781199723006976\n",
      "STEP: 351800 - loss: 0.01377865773407896\n",
      "STEP: 351900 - loss: 0.013776116511470194\n",
      "STEP: 352000 - loss: 0.013773576054948125\n",
      "STEP: 352100 - loss: 0.0137710363642795\n",
      "STEP: 352200 - loss: 0.013768497439231222\n",
      "STEP: 352300 - loss: 0.013765959279570463\n",
      "STEP: 352400 - loss: 0.013763421885064468\n",
      "STEP: 352500 - loss: 0.01376088525548061\n",
      "STEP: 352600 - loss: 0.013758349390585687\n",
      "STEP: 352700 - loss: 0.013755814290147623\n",
      "STEP: 352800 - loss: 0.013753279953933445\n",
      "STEP: 352900 - loss: 0.0137507463817113\n",
      "STEP: 353000 - loss: 0.013748213573248158\n",
      "STEP: 353100 - loss: 0.013745681528312187\n",
      "STEP: 353200 - loss: 0.013743150246670522\n",
      "STEP: 353300 - loss: 0.013740619728091544\n",
      "STEP: 353400 - loss: 0.013738089972342868\n",
      "STEP: 353500 - loss: 0.013735560979192544\n",
      "STEP: 353600 - loss: 0.013733032748408605\n",
      "STEP: 353700 - loss: 0.013730505279758864\n",
      "STEP: 353800 - loss: 0.013727978573011898\n",
      "STEP: 353900 - loss: 0.013725452627935699\n",
      "STEP: 354000 - loss: 0.01372292744429868\n",
      "STEP: 354100 - loss: 0.013720403021868981\n",
      "STEP: 354200 - loss: 0.013717879360415314\n",
      "STEP: 354300 - loss: 0.013715356459706136\n",
      "STEP: 354400 - loss: 0.013712834319509926\n",
      "STEP: 354500 - loss: 0.013710312939595667\n",
      "STEP: 354600 - loss: 0.013707792319731576\n",
      "STEP: 354700 - loss: 0.013705272459686835\n",
      "STEP: 354800 - loss: 0.013702753359230008\n",
      "STEP: 354900 - loss: 0.01370023501813054\n",
      "STEP: 355000 - loss: 0.01369771743615679\n",
      "STEP: 355100 - loss: 0.013695200613078445\n",
      "STEP: 355200 - loss: 0.013692684548664093\n",
      "STEP: 355300 - loss: 0.013690169242683436\n",
      "STEP: 355400 - loss: 0.013687654694905524\n",
      "STEP: 355500 - loss: 0.013685140905099707\n",
      "STEP: 355600 - loss: 0.013682627873035567\n",
      "STEP: 355700 - loss: 0.013680115598482468\n",
      "STEP: 355800 - loss: 0.013677604081209849\n",
      "STEP: 355900 - loss: 0.013675093320987744\n",
      "STEP: 356000 - loss: 0.01367258331758536\n",
      "STEP: 356100 - loss: 0.01367007407077305\n",
      "STEP: 356200 - loss: 0.013667565580320024\n",
      "STEP: 356300 - loss: 0.01366505784599679\n",
      "STEP: 356400 - loss: 0.013662550867573005\n",
      "STEP: 356500 - loss: 0.013660044644818837\n",
      "STEP: 356600 - loss: 0.013657539177504252\n",
      "STEP: 356700 - loss: 0.0136550344653999\n",
      "STEP: 356800 - loss: 0.01365253050827544\n",
      "STEP: 356900 - loss: 0.013650027305901632\n",
      "STEP: 357000 - loss: 0.013647524858049001\n",
      "STEP: 357100 - loss: 0.013645023164487508\n",
      "STEP: 357200 - loss: 0.013642522224988077\n",
      "STEP: 357300 - loss: 0.013640022039321117\n",
      "STEP: 357400 - loss: 0.013637522607257743\n",
      "STEP: 357500 - loss: 0.013635023928568185\n",
      "STEP: 357600 - loss: 0.013632526003023403\n",
      "STEP: 357700 - loss: 0.01363002883039478\n",
      "STEP: 357800 - loss: 0.01362753241045268\n",
      "STEP: 357900 - loss: 0.013625036742968033\n",
      "STEP: 358000 - loss: 0.013622541827712584\n",
      "STEP: 358100 - loss: 0.01362004766445731\n",
      "STEP: 358200 - loss: 0.01361755425297335\n",
      "STEP: 358300 - loss: 0.013615061593031825\n",
      "STEP: 358400 - loss: 0.013612569684404243\n",
      "STEP: 358500 - loss: 0.01361007852686236\n",
      "STEP: 358600 - loss: 0.013607588120177538\n",
      "STEP: 358700 - loss: 0.013605098464120974\n",
      "STEP: 358800 - loss: 0.013602609558464812\n",
      "STEP: 358900 - loss: 0.013600121402980624\n",
      "STEP: 359000 - loss: 0.013597633997440347\n",
      "STEP: 359100 - loss: 0.013595147341615576\n",
      "STEP: 359200 - loss: 0.013592661435278486\n",
      "STEP: 359300 - loss: 0.013590176278201272\n",
      "STEP: 359400 - loss: 0.013587691870155277\n",
      "STEP: 359500 - loss: 0.013585208210913238\n",
      "STEP: 359600 - loss: 0.013582725300247445\n",
      "STEP: 359700 - loss: 0.013580243137929993\n",
      "STEP: 359800 - loss: 0.013577761723733206\n",
      "STEP: 359900 - loss: 0.013575281057429597\n",
      "STEP: 360000 - loss: 0.013572801138791534\n",
      "STEP: 360100 - loss: 0.013570321967591684\n",
      "STEP: 360200 - loss: 0.013567843543602775\n",
      "STEP: 360300 - loss: 0.013565365866597328\n",
      "STEP: 360400 - loss: 0.013562888936348123\n",
      "STEP: 360500 - loss: 0.01356041275262817\n",
      "STEP: 360600 - loss: 0.01355793731521028\n",
      "STEP: 360700 - loss: 0.013555462623867351\n",
      "STEP: 360800 - loss: 0.01355298867837261\n",
      "STEP: 360900 - loss: 0.013550515478499095\n",
      "STEP: 361000 - loss: 0.013548043024020028\n",
      "STEP: 361100 - loss: 0.013545571314708502\n",
      "STEP: 361200 - loss: 0.013543100350338058\n",
      "STEP: 361300 - loss: 0.013540630130681902\n",
      "STEP: 361400 - loss: 0.01353816065551372\n",
      "STEP: 361500 - loss: 0.01353569192460682\n",
      "STEP: 361600 - loss: 0.0135332239377349\n",
      "STEP: 361700 - loss: 0.013530756694671742\n",
      "STEP: 361800 - loss: 0.013528290195190798\n",
      "STEP: 361900 - loss: 0.013525824439066123\n",
      "STEP: 362000 - loss: 0.013523359426071488\n",
      "STEP: 362100 - loss: 0.013520895155980936\n",
      "STEP: 362200 - loss: 0.013518431628568353\n",
      "STEP: 362300 - loss: 0.013515968843607909\n",
      "STEP: 362400 - loss: 0.01351350680087383\n",
      "STEP: 362500 - loss: 0.013511045500140137\n",
      "STEP: 362600 - loss: 0.013508584941181234\n",
      "STEP: 362700 - loss: 0.013506125123771518\n",
      "STEP: 362800 - loss: 0.01350366604768534\n",
      "STEP: 362900 - loss: 0.013501207712697158\n",
      "STEP: 363000 - loss: 0.01349875011858157\n",
      "STEP: 363100 - loss: 0.01349629326511336\n",
      "STEP: 363200 - loss: 0.013493837152067239\n",
      "STEP: 363300 - loss: 0.013491381779217656\n",
      "STEP: 363400 - loss: 0.013488927146339608\n",
      "STEP: 363500 - loss: 0.013486473253207901\n",
      "STEP: 363600 - loss: 0.01348402009959784\n",
      "STEP: 363700 - loss: 0.013481567685284301\n",
      "STEP: 363800 - loss: 0.01347911601004231\n",
      "STEP: 363900 - loss: 0.013476665073647054\n",
      "STEP: 364000 - loss: 0.013474214875873806\n",
      "STEP: 364100 - loss: 0.013471765416497836\n",
      "STEP: 364200 - loss: 0.013469316695294706\n",
      "STEP: 364300 - loss: 0.01346686871203982\n",
      "STEP: 364400 - loss: 0.013464421466508409\n",
      "STEP: 364500 - loss: 0.013461974958476671\n",
      "STEP: 364600 - loss: 0.01345952918771974\n",
      "STEP: 364700 - loss: 0.013457084154013496\n",
      "STEP: 364800 - loss: 0.013454639857133549\n",
      "STEP: 364900 - loss: 0.013452196296856136\n",
      "STEP: 365000 - loss: 0.013449753472956872\n",
      "STEP: 365100 - loss: 0.013447311385212041\n",
      "STEP: 365200 - loss: 0.013444870033397418\n",
      "STEP: 365300 - loss: 0.013442429417289178\n",
      "STEP: 365400 - loss: 0.013439989536663782\n",
      "STEP: 365500 - loss: 0.013437550391297256\n",
      "STEP: 365600 - loss: 0.013435111980966108\n",
      "STEP: 365700 - loss: 0.013432674305446462\n",
      "STEP: 365800 - loss: 0.013430237364515329\n",
      "STEP: 365900 - loss: 0.013427801157948716\n",
      "STEP: 366000 - loss: 0.013425365685523489\n",
      "STEP: 366100 - loss: 0.013422930947016187\n",
      "STEP: 366200 - loss: 0.013420496942203736\n",
      "STEP: 366300 - loss: 0.01341806367086285\n",
      "STEP: 366400 - loss: 0.013415631132770323\n",
      "STEP: 366500 - loss: 0.013413199327703526\n",
      "STEP: 366600 - loss: 0.013410768255438738\n",
      "STEP: 366700 - loss: 0.013408337915753818\n",
      "STEP: 366800 - loss: 0.01340590830842547\n",
      "STEP: 366900 - loss: 0.013403479433230961\n",
      "STEP: 367000 - loss: 0.013401051289947619\n",
      "STEP: 367100 - loss: 0.013398623878352894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 367200 - loss: 0.013396197198224232\n",
      "STEP: 367300 - loss: 0.013393771249338899\n",
      "STEP: 367400 - loss: 0.01339134603147461\n",
      "STEP: 367500 - loss: 0.0133889215444092\n",
      "STEP: 367600 - loss: 0.013386497787919801\n",
      "STEP: 367700 - loss: 0.013384074761784822\n",
      "STEP: 367800 - loss: 0.013381652465781607\n",
      "STEP: 367900 - loss: 0.013379230899688321\n",
      "STEP: 368000 - loss: 0.013376810063283003\n",
      "STEP: 368100 - loss: 0.013374389956343224\n",
      "STEP: 368200 - loss: 0.01337197057864776\n",
      "STEP: 368300 - loss: 0.013369551929974221\n",
      "STEP: 368400 - loss: 0.013367134010101122\n",
      "STEP: 368500 - loss: 0.013364716818806646\n",
      "STEP: 368600 - loss: 0.013362300355869366\n",
      "STEP: 368700 - loss: 0.013359884621067433\n",
      "STEP: 368800 - loss: 0.013357469614179569\n",
      "STEP: 368900 - loss: 0.013355055334984118\n",
      "STEP: 369000 - loss: 0.013352641783260153\n",
      "STEP: 369100 - loss: 0.013350228958786\n",
      "STEP: 369200 - loss: 0.013347816861340645\n",
      "STEP: 369300 - loss: 0.013345405490702672\n",
      "STEP: 369400 - loss: 0.01334299484665121\n",
      "STEP: 369500 - loss: 0.013340584928965222\n",
      "STEP: 369600 - loss: 0.01333817573742362\n",
      "STEP: 369700 - loss: 0.01333576727180571\n",
      "STEP: 369800 - loss: 0.013333359531890526\n",
      "STEP: 369900 - loss: 0.013330952517457483\n",
      "STEP: 370000 - loss: 0.013328546228285613\n",
      "STEP: 370100 - loss: 0.013326140664154582\n",
      "STEP: 370200 - loss: 0.013323735824843672\n",
      "STEP: 370300 - loss: 0.013321331710132412\n",
      "STEP: 370400 - loss: 0.013318928319800485\n",
      "STEP: 370500 - loss: 0.013316525653627478\n",
      "STEP: 370600 - loss: 0.013314123711393145\n",
      "STEP: 370700 - loss: 0.013311722492877116\n",
      "STEP: 370800 - loss: 0.01330932199785948\n",
      "STEP: 370900 - loss: 0.01330692222612003\n",
      "STEP: 371000 - loss: 0.01330452317743871\n",
      "STEP: 371100 - loss: 0.013302124851595567\n",
      "STEP: 371200 - loss: 0.013299727248370785\n",
      "STEP: 371300 - loss: 0.013297330367544723\n",
      "STEP: 371400 - loss: 0.013294934208897175\n",
      "STEP: 371500 - loss: 0.013292538772208887\n",
      "STEP: 371600 - loss: 0.01329014405726027\n",
      "STEP: 371700 - loss: 0.013287750063831144\n",
      "STEP: 371800 - loss: 0.013285356791702821\n",
      "STEP: 371900 - loss: 0.013282964240655578\n",
      "STEP: 372000 - loss: 0.013280572410469852\n",
      "STEP: 372100 - loss: 0.013278181300926705\n",
      "STEP: 372200 - loss: 0.013275790911806857\n",
      "STEP: 372300 - loss: 0.013273401242890842\n",
      "STEP: 372400 - loss: 0.01327101229395999\n",
      "STEP: 372500 - loss: 0.013268624064794954\n",
      "STEP: 372600 - loss: 0.013266236555177344\n",
      "STEP: 372700 - loss: 0.013263849764887408\n",
      "STEP: 372800 - loss: 0.013261463693707112\n",
      "STEP: 372900 - loss: 0.013259078341417171\n",
      "STEP: 373000 - loss: 0.013256693707799383\n",
      "STEP: 373100 - loss: 0.013254309792634696\n",
      "STEP: 373200 - loss: 0.013251926595704825\n",
      "STEP: 373300 - loss: 0.01324954411679127\n",
      "STEP: 373400 - loss: 0.013247162355675612\n",
      "STEP: 373500 - loss: 0.013244781312139468\n",
      "STEP: 373600 - loss: 0.013242400985964412\n",
      "STEP: 373700 - loss: 0.013240021376932592\n",
      "STEP: 373800 - loss: 0.013237642484825434\n",
      "STEP: 373900 - loss: 0.013235264309425182\n",
      "STEP: 374000 - loss: 0.013232886850513848\n",
      "STEP: 374100 - loss: 0.01323051010787312\n",
      "STEP: 374200 - loss: 0.013228134081285361\n",
      "STEP: 374300 - loss: 0.013225758770532569\n",
      "STEP: 374400 - loss: 0.013223384175397448\n",
      "STEP: 374500 - loss: 0.013221010295662017\n",
      "STEP: 374600 - loss: 0.013218637131108647\n",
      "STEP: 374700 - loss: 0.013216264681519421\n",
      "STEP: 374800 - loss: 0.013213892946677732\n",
      "STEP: 374900 - loss: 0.01321152192636534\n",
      "STEP: 375000 - loss: 0.013209151620365303\n",
      "STEP: 375100 - loss: 0.01320678202846024\n",
      "STEP: 375200 - loss: 0.013204413150432998\n",
      "STEP: 375300 - loss: 0.013202044986066225\n",
      "STEP: 375400 - loss: 0.013199677535143103\n",
      "STEP: 375500 - loss: 0.013197310797446422\n",
      "STEP: 375600 - loss: 0.013194944772759246\n",
      "STEP: 375700 - loss: 0.013192579460864731\n",
      "STEP: 375800 - loss: 0.013190214861546016\n",
      "STEP: 375900 - loss: 0.013187850974586444\n",
      "STEP: 376000 - loss: 0.013185487799769127\n",
      "STEP: 376100 - loss: 0.01318312533687771\n",
      "STEP: 376200 - loss: 0.01318076358569537\n",
      "STEP: 376300 - loss: 0.013178402546005673\n",
      "STEP: 376400 - loss: 0.013176042217592176\n",
      "STEP: 376500 - loss: 0.01317368260023884\n",
      "STEP: 376600 - loss: 0.013171323693728934\n",
      "STEP: 376700 - loss: 0.01316896549784625\n",
      "STEP: 376800 - loss: 0.013166608012375031\n",
      "STEP: 376900 - loss: 0.013164251237098861\n",
      "STEP: 377000 - loss: 0.013161895171801562\n",
      "STEP: 377100 - loss: 0.013159539816267418\n",
      "STEP: 377200 - loss: 0.013157185170280633\n",
      "STEP: 377300 - loss: 0.01315483123362517\n",
      "STEP: 377400 - loss: 0.01315247800608517\n",
      "STEP: 377500 - loss: 0.013150125487445156\n",
      "STEP: 377600 - loss: 0.013147773677489333\n",
      "STEP: 377700 - loss: 0.013145422576002107\n",
      "STEP: 377800 - loss: 0.013143072182768131\n",
      "STEP: 377900 - loss: 0.013140722497571972\n",
      "STEP: 378000 - loss: 0.013138373520197994\n",
      "STEP: 378100 - loss: 0.013136025250431027\n",
      "STEP: 378200 - loss: 0.0131336776880558\n",
      "STEP: 378300 - loss: 0.013131330832857297\n",
      "STEP: 378400 - loss: 0.013128984684620154\n",
      "STEP: 378500 - loss: 0.013126639243129682\n",
      "STEP: 378600 - loss: 0.01312429450817025\n",
      "STEP: 378700 - loss: 0.013121950479527633\n",
      "STEP: 378800 - loss: 0.013119607156986523\n",
      "STEP: 378900 - loss: 0.013117264540332314\n",
      "STEP: 379000 - loss: 0.013114922629350347\n",
      "STEP: 379100 - loss: 0.013112581423825861\n",
      "STEP: 379200 - loss: 0.013110240923544477\n",
      "STEP: 379300 - loss: 0.013107901128291216\n",
      "STEP: 379400 - loss: 0.0131055620378519\n",
      "STEP: 379500 - loss: 0.013103223652012077\n",
      "STEP: 379600 - loss: 0.013100885970557743\n",
      "STEP: 379700 - loss: 0.0130985489932739\n",
      "STEP: 379800 - loss: 0.013096212719947065\n",
      "STEP: 379900 - loss: 0.013093877150362709\n",
      "STEP: 380000 - loss: 0.013091542284306678\n",
      "STEP: 380100 - loss: 0.013089208121565402\n",
      "STEP: 380200 - loss: 0.013086874661924788\n",
      "STEP: 380300 - loss: 0.013084541905170574\n",
      "STEP: 380400 - loss: 0.013082209851089507\n",
      "STEP: 380500 - loss: 0.013079878499467148\n",
      "STEP: 380600 - loss: 0.013077547850090696\n",
      "STEP: 380700 - loss: 0.01307521790274575\n",
      "STEP: 380800 - loss: 0.013072888657219492\n",
      "STEP: 380900 - loss: 0.013070560113297602\n",
      "STEP: 381000 - loss: 0.013068232270767487\n",
      "STEP: 381100 - loss: 0.013065905129415082\n",
      "STEP: 381200 - loss: 0.013063578689027508\n",
      "STEP: 381300 - loss: 0.013061252949391389\n",
      "STEP: 381400 - loss: 0.013058927910293719\n",
      "STEP: 381500 - loss: 0.013056603571521174\n",
      "STEP: 381600 - loss: 0.013054279932860948\n",
      "STEP: 381700 - loss: 0.013051956994099964\n",
      "STEP: 381800 - loss: 0.0130496347550253\n",
      "STEP: 381900 - loss: 0.013047313215424183\n",
      "STEP: 382000 - loss: 0.013044992375083811\n",
      "STEP: 382100 - loss: 0.013042672233791266\n",
      "STEP: 382200 - loss: 0.013040352791334439\n",
      "STEP: 382300 - loss: 0.013038034047500308\n",
      "STEP: 382400 - loss: 0.013035716002076264\n",
      "STEP: 382500 - loss: 0.013033398654850375\n",
      "STEP: 382600 - loss: 0.013031082005609809\n",
      "STEP: 382700 - loss: 0.013028766054142485\n",
      "STEP: 382800 - loss: 0.013026450800235873\n",
      "STEP: 382900 - loss: 0.013024136243678116\n",
      "STEP: 383000 - loss: 0.013021822384256834\n",
      "STEP: 383100 - loss: 0.013019509221760173\n",
      "STEP: 383200 - loss: 0.013017196755975777\n",
      "STEP: 383300 - loss: 0.013014884986692093\n",
      "STEP: 383400 - loss: 0.013012573913697362\n",
      "STEP: 383500 - loss: 0.013010263536779426\n",
      "STEP: 383600 - loss: 0.013007953855726567\n",
      "STEP: 383700 - loss: 0.013005644870327107\n",
      "STEP: 383800 - loss: 0.013003336580369729\n",
      "STEP: 383900 - loss: 0.013001028985642529\n",
      "STEP: 384000 - loss: 0.01299872208593444\n",
      "STEP: 384100 - loss: 0.012996415881033615\n",
      "STEP: 384200 - loss: 0.012994110370728945\n",
      "STEP: 384300 - loss: 0.012991805554808966\n",
      "STEP: 384400 - loss: 0.012989501433062664\n",
      "STEP: 384500 - loss: 0.012987198005278939\n",
      "STEP: 384600 - loss: 0.012984895271246365\n",
      "STEP: 384700 - loss: 0.012982593230753906\n",
      "STEP: 384800 - loss: 0.012980291883590959\n",
      "STEP: 384900 - loss: 0.012977991229546673\n",
      "STEP: 385000 - loss: 0.012975691268409681\n",
      "STEP: 385100 - loss: 0.012973391999969492\n",
      "STEP: 385200 - loss: 0.012971093424015442\n",
      "STEP: 385300 - loss: 0.012968795540336817\n",
      "STEP: 385400 - loss: 0.012966498348723099\n",
      "STEP: 385500 - loss: 0.012964201848963663\n",
      "STEP: 385600 - loss: 0.012961906040848325\n",
      "STEP: 385700 - loss: 0.012959610924166368\n",
      "STEP: 385800 - loss: 0.012957316498707617\n",
      "STEP: 385900 - loss: 0.012955022764261952\n",
      "STEP: 386000 - loss: 0.012952729720618611\n",
      "STEP: 386100 - loss: 0.012950437367568068\n",
      "STEP: 386200 - loss: 0.012948145704900058\n",
      "STEP: 386300 - loss: 0.012945854732404557\n",
      "STEP: 386400 - loss: 0.012943564449871655\n",
      "STEP: 386500 - loss: 0.012941274857091381\n",
      "STEP: 386600 - loss: 0.012938985953853895\n",
      "STEP: 386700 - loss: 0.012936697739949612\n",
      "STEP: 386800 - loss: 0.012934410215168977\n",
      "STEP: 386900 - loss: 0.012932123379301805\n",
      "STEP: 387000 - loss: 0.012929837232139089\n",
      "STEP: 387100 - loss: 0.012927551773471227\n",
      "STEP: 387200 - loss: 0.012925267003088456\n",
      "STEP: 387300 - loss: 0.012922982920781692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 387400 - loss: 0.012920699526341731\n",
      "STEP: 387500 - loss: 0.012918416819558996\n",
      "STEP: 387600 - loss: 0.012916134800224724\n",
      "STEP: 387700 - loss: 0.012913853468129325\n",
      "STEP: 387800 - loss: 0.012911572823064137\n",
      "STEP: 387900 - loss: 0.012909292864819979\n",
      "STEP: 388000 - loss: 0.012907013593188035\n",
      "STEP: 388100 - loss: 0.01290473500795937\n",
      "STEP: 388200 - loss: 0.012902457108925198\n",
      "STEP: 388300 - loss: 0.012900179895876666\n",
      "STEP: 388400 - loss: 0.012897903368605374\n",
      "STEP: 388500 - loss: 0.012895627526902606\n",
      "STEP: 388600 - loss: 0.012893352370559854\n",
      "STEP: 388700 - loss: 0.012891077899368395\n",
      "STEP: 388800 - loss: 0.012888804113120225\n",
      "STEP: 388900 - loss: 0.012886531011606455\n",
      "STEP: 389000 - loss: 0.012884258594619408\n",
      "STEP: 389100 - loss: 0.01288198686195045\n",
      "STEP: 389200 - loss: 0.01287971581339146\n",
      "STEP: 389300 - loss: 0.01287744544873464\n",
      "STEP: 389400 - loss: 0.012875175767771588\n",
      "STEP: 389500 - loss: 0.01287290677029435\n",
      "STEP: 389600 - loss: 0.012870638456095304\n",
      "STEP: 389700 - loss: 0.01286837082496641\n",
      "STEP: 389800 - loss: 0.012866103876699744\n",
      "STEP: 389900 - loss: 0.012863837611087925\n",
      "STEP: 390000 - loss: 0.012861572027923068\n",
      "STEP: 390100 - loss: 0.012859307126997779\n",
      "STEP: 390200 - loss: 0.012857042908104207\n",
      "STEP: 390300 - loss: 0.01285477937103533\n",
      "STEP: 390400 - loss: 0.012852516515583038\n",
      "STEP: 390500 - loss: 0.012850254341540895\n",
      "STEP: 390600 - loss: 0.012847992848700893\n",
      "STEP: 390700 - loss: 0.012845732036856248\n",
      "STEP: 390800 - loss: 0.01284347190579962\n",
      "STEP: 390900 - loss: 0.012841212455324056\n",
      "STEP: 391000 - loss: 0.012838953685222157\n",
      "STEP: 391100 - loss: 0.012836695595287568\n",
      "STEP: 391200 - loss: 0.012834438185312843\n",
      "STEP: 391300 - loss: 0.012832181455091404\n",
      "STEP: 391400 - loss: 0.012829925404416664\n",
      "STEP: 391500 - loss: 0.01282767003308142\n",
      "STEP: 391600 - loss: 0.012825415340879224\n",
      "STEP: 391700 - loss: 0.012823161327603782\n",
      "STEP: 391800 - loss: 0.01282090799304822\n",
      "STEP: 391900 - loss: 0.012818655337006426\n",
      "STEP: 392000 - loss: 0.012816403359271622\n",
      "STEP: 392100 - loss: 0.012814152059637693\n",
      "STEP: 392200 - loss: 0.012811901437898222\n",
      "STEP: 392300 - loss: 0.012809651493847279\n",
      "STEP: 392400 - loss: 0.012807402227278367\n",
      "STEP: 392500 - loss: 0.012805153637985688\n",
      "STEP: 392600 - loss: 0.012802905725763166\n",
      "STEP: 392700 - loss: 0.012800658490404595\n",
      "STEP: 392800 - loss: 0.01279841193170452\n",
      "STEP: 392900 - loss: 0.012796166049456608\n",
      "STEP: 393000 - loss: 0.012793920843455635\n",
      "STEP: 393100 - loss: 0.012791676313495454\n",
      "STEP: 393200 - loss: 0.012789432459370639\n",
      "STEP: 393300 - loss: 0.012787189280875619\n",
      "STEP: 393400 - loss: 0.012784946777804545\n",
      "STEP: 393500 - loss: 0.012782704949952439\n",
      "STEP: 393600 - loss: 0.012780463797113678\n",
      "STEP: 393700 - loss: 0.012778223319082915\n",
      "STEP: 393800 - loss: 0.012775983515654982\n",
      "STEP: 393900 - loss: 0.012773744386624491\n",
      "STEP: 394000 - loss: 0.012771505931786548\n",
      "STEP: 394100 - loss: 0.01276926815093587\n",
      "STEP: 394200 - loss: 0.01276703104386748\n",
      "STEP: 394300 - loss: 0.012764794610376454\n",
      "STEP: 394400 - loss: 0.012762558850257972\n",
      "STEP: 394500 - loss: 0.012760323763306986\n",
      "STEP: 394600 - loss: 0.01275808934931907\n",
      "STEP: 394700 - loss: 0.0127558556080891\n",
      "STEP: 394800 - loss: 0.012753622539412812\n",
      "STEP: 394900 - loss: 0.012751390143085291\n",
      "STEP: 395000 - loss: 0.012749158418902251\n",
      "STEP: 395100 - loss: 0.012746927366659114\n",
      "STEP: 395200 - loss: 0.01274469698615186\n",
      "STEP: 395300 - loss: 0.012742467277175224\n",
      "STEP: 395400 - loss: 0.012740238239526084\n",
      "STEP: 395500 - loss: 0.0127380098729995\n",
      "STEP: 395600 - loss: 0.012735782177391474\n",
      "STEP: 395700 - loss: 0.012733555152498185\n",
      "STEP: 395800 - loss: 0.01273132879811524\n",
      "STEP: 395900 - loss: 0.012729103114038628\n",
      "STEP: 396000 - loss: 0.012726878100064936\n",
      "STEP: 396100 - loss: 0.012724653755989851\n",
      "STEP: 396200 - loss: 0.012722430081609917\n",
      "STEP: 396300 - loss: 0.01272020707672127\n",
      "STEP: 396400 - loss: 0.012717984741120208\n",
      "STEP: 396500 - loss: 0.012715763074603351\n",
      "STEP: 396600 - loss: 0.012713542076966806\n",
      "STEP: 396700 - loss: 0.012711321748007442\n",
      "STEP: 396800 - loss: 0.012709102087522007\n",
      "STEP: 396900 - loss: 0.012706883095306525\n",
      "STEP: 397000 - loss: 0.01270466477115836\n",
      "STEP: 397100 - loss: 0.012702447114874092\n",
      "STEP: 397200 - loss: 0.01270023012625022\n",
      "STEP: 397300 - loss: 0.012698013805084035\n",
      "STEP: 397400 - loss: 0.012695798151172482\n",
      "STEP: 397500 - loss: 0.012693583164312483\n",
      "STEP: 397600 - loss: 0.012691368844301257\n",
      "STEP: 397700 - loss: 0.012689155190935728\n",
      "STEP: 397800 - loss: 0.01268694220401334\n",
      "STEP: 397900 - loss: 0.012684729883331289\n",
      "STEP: 398000 - loss: 0.012682518228686834\n",
      "STEP: 398100 - loss: 0.012680307239877559\n",
      "STEP: 398200 - loss: 0.012678096916700684\n",
      "STEP: 398300 - loss: 0.012675887258953864\n",
      "STEP: 398400 - loss: 0.01267367826643476\n",
      "STEP: 398500 - loss: 0.012671469938940954\n",
      "STEP: 398600 - loss: 0.012669262276269997\n",
      "STEP: 398700 - loss: 0.012667055278219878\n",
      "STEP: 398800 - loss: 0.012664848944588325\n",
      "STEP: 398900 - loss: 0.012662643275173394\n",
      "STEP: 399000 - loss: 0.012660438269772895\n",
      "STEP: 399100 - loss: 0.012658233928184695\n",
      "STEP: 399200 - loss: 0.012656030250207164\n",
      "STEP: 399300 - loss: 0.01265382723563829\n",
      "STEP: 399400 - loss: 0.01265162488427605\n",
      "STEP: 399500 - loss: 0.012649423195919437\n",
      "STEP: 399600 - loss: 0.012647222170365752\n",
      "STEP: 399700 - loss: 0.012645021807414095\n",
      "STEP: 399800 - loss: 0.012642822106862841\n",
      "STEP: 399900 - loss: 0.012640623068510432\n",
      "STEP: 400000 - loss: 0.01263842469215506\n",
      "STEP: 400100 - loss: 0.012636226977596064\n",
      "STEP: 400200 - loss: 0.012634029924631557\n",
      "STEP: 400300 - loss: 0.012631833533060402\n",
      "STEP: 400400 - loss: 0.01262963780268156\n",
      "STEP: 400500 - loss: 0.012627442733293961\n",
      "STEP: 400600 - loss: 0.012625248324696279\n",
      "STEP: 400700 - loss: 0.012623054576687628\n",
      "STEP: 400800 - loss: 0.012620861489067247\n",
      "STEP: 400900 - loss: 0.012618669061634025\n",
      "STEP: 401000 - loss: 0.012616477294187023\n",
      "STEP: 401100 - loss: 0.012614286186525905\n",
      "STEP: 401200 - loss: 0.012612095738449555\n",
      "STEP: 401300 - loss: 0.012609905949757947\n",
      "STEP: 401400 - loss: 0.012607716820249632\n",
      "STEP: 401500 - loss: 0.01260552834972482\n",
      "STEP: 401600 - loss: 0.012603340537982618\n",
      "STEP: 401700 - loss: 0.012601153384822716\n",
      "STEP: 401800 - loss: 0.012598966890045157\n",
      "STEP: 401900 - loss: 0.012596781053449047\n",
      "STEP: 402000 - loss: 0.012594595874834701\n",
      "STEP: 402100 - loss: 0.012592411354001723\n",
      "STEP: 402200 - loss: 0.01259022749075014\n",
      "STEP: 402300 - loss: 0.012588044284879868\n",
      "STEP: 402400 - loss: 0.012585861736191029\n",
      "STEP: 402500 - loss: 0.012583679844483452\n",
      "STEP: 402600 - loss: 0.012581498609557516\n",
      "STEP: 402700 - loss: 0.012579318031213437\n",
      "STEP: 402800 - loss: 0.012577138109251285\n",
      "STEP: 402900 - loss: 0.012574958843471673\n",
      "STEP: 403000 - loss: 0.012572780233674951\n",
      "STEP: 403100 - loss: 0.012570602279661478\n",
      "STEP: 403200 - loss: 0.012568424981231785\n",
      "STEP: 403300 - loss: 0.012566248338186559\n",
      "STEP: 403400 - loss: 0.012564072350326106\n",
      "STEP: 403500 - loss: 0.012561897017451506\n",
      "STEP: 403600 - loss: 0.012559722339363413\n",
      "STEP: 403700 - loss: 0.012557548315862541\n",
      "STEP: 403800 - loss: 0.012555374946750159\n",
      "STEP: 403900 - loss: 0.012553202231826618\n",
      "STEP: 404000 - loss: 0.012551030170893355\n",
      "STEP: 404100 - loss: 0.01254885876375123\n",
      "STEP: 404200 - loss: 0.01254668801020146\n",
      "STEP: 404300 - loss: 0.012544517910045261\n",
      "STEP: 404400 - loss: 0.012542348463083757\n",
      "STEP: 404500 - loss: 0.012540179669118388\n",
      "STEP: 404600 - loss: 0.012538011527950458\n",
      "STEP: 404700 - loss: 0.012535844039381317\n",
      "STEP: 404800 - loss: 0.012533677203212776\n",
      "STEP: 404900 - loss: 0.012531511019245978\n",
      "STEP: 405000 - loss: 0.012529345487282911\n",
      "STEP: 405100 - loss: 0.012527180607124776\n",
      "STEP: 405200 - loss: 0.012525016378573575\n",
      "STEP: 405300 - loss: 0.012522852801431176\n",
      "STEP: 405400 - loss: 0.012520689875499416\n",
      "STEP: 405500 - loss: 0.012518527600579981\n",
      "STEP: 405600 - loss: 0.012516365976475038\n",
      "STEP: 405700 - loss: 0.012514205002986532\n",
      "STEP: 405800 - loss: 0.012512044679916647\n",
      "STEP: 405900 - loss: 0.01250988500706752\n",
      "STEP: 406000 - loss: 0.012507725984241349\n",
      "STEP: 406100 - loss: 0.012505567611240423\n",
      "STEP: 406200 - loss: 0.012503409887866876\n",
      "STEP: 406300 - loss: 0.012501252813923433\n",
      "STEP: 406400 - loss: 0.012499096389212363\n",
      "STEP: 406500 - loss: 0.012496940613535953\n",
      "STEP: 406600 - loss: 0.012494785486697245\n",
      "STEP: 406700 - loss: 0.012492631008498538\n",
      "STEP: 406800 - loss: 0.01249047717874276\n",
      "STEP: 406900 - loss: 0.012488323997232513\n",
      "STEP: 407000 - loss: 0.012486171463770433\n",
      "STEP: 407100 - loss: 0.012484019578159932\n",
      "STEP: 407200 - loss: 0.012481868340203229\n",
      "STEP: 407300 - loss: 0.012479717749703835\n",
      "STEP: 407400 - loss: 0.012477567806464885\n",
      "STEP: 407500 - loss: 0.012475418510288942\n",
      "STEP: 407600 - loss: 0.012473269860979793\n",
      "STEP: 407700 - loss: 0.012471121858340208\n",
      "STEP: 407800 - loss: 0.012468974502173612\n",
      "STEP: 407900 - loss: 0.012466827792283577\n",
      "STEP: 408000 - loss: 0.012464681728473313\n",
      "STEP: 408100 - loss: 0.012462536310546227\n",
      "STEP: 408200 - loss: 0.01246039153830599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 408300 - loss: 0.012458247411556172\n",
      "STEP: 408400 - loss: 0.012456103930100347\n",
      "STEP: 408500 - loss: 0.012453961093742196\n",
      "STEP: 408600 - loss: 0.01245181890228567\n",
      "STEP: 408700 - loss: 0.012449677355534724\n",
      "STEP: 408800 - loss: 0.012447536453292617\n",
      "STEP: 408900 - loss: 0.01244539619536402\n",
      "STEP: 409000 - loss: 0.01244325658155244\n",
      "STEP: 409100 - loss: 0.012441117611662212\n",
      "STEP: 409200 - loss: 0.012438979285497464\n",
      "STEP: 409300 - loss: 0.012436841602862216\n",
      "STEP: 409400 - loss: 0.012434704563560675\n",
      "STEP: 409500 - loss: 0.012432568167397674\n",
      "STEP: 409600 - loss: 0.012430432414177031\n",
      "STEP: 409700 - loss: 0.012428297303703258\n",
      "STEP: 409800 - loss: 0.012426162835780725\n",
      "STEP: 409900 - loss: 0.012424029010214532\n",
      "STEP: 410000 - loss: 0.012421895826808672\n",
      "STEP: 410100 - loss: 0.012419763285368157\n",
      "STEP: 410200 - loss: 0.012417631385697622\n",
      "STEP: 410300 - loss: 0.012415500127602095\n",
      "STEP: 410400 - loss: 0.012413369510885775\n",
      "STEP: 410500 - loss: 0.012411239535354028\n",
      "STEP: 410600 - loss: 0.0124091102008118\n",
      "STEP: 410700 - loss: 0.012406981507064216\n",
      "STEP: 410800 - loss: 0.012404853453915876\n",
      "STEP: 410900 - loss: 0.012402726041172549\n",
      "STEP: 411000 - loss: 0.012400599268638856\n",
      "STEP: 411100 - loss: 0.012398473136120452\n",
      "STEP: 411200 - loss: 0.012396347643422543\n",
      "STEP: 411300 - loss: 0.012394222790350486\n",
      "STEP: 411400 - loss: 0.012392098576709757\n",
      "STEP: 411500 - loss: 0.012389975002305735\n",
      "STEP: 411600 - loss: 0.012387852066944297\n",
      "STEP: 411700 - loss: 0.012385729770430357\n",
      "STEP: 411800 - loss: 0.0123836081125704\n",
      "STEP: 411900 - loss: 0.012381487093169562\n",
      "STEP: 412000 - loss: 0.012379366712034006\n",
      "STEP: 412100 - loss: 0.012377246968969568\n",
      "STEP: 412200 - loss: 0.01237512786378188\n",
      "STEP: 412300 - loss: 0.012373009396276922\n",
      "STEP: 412400 - loss: 0.012370891566261123\n",
      "STEP: 412500 - loss: 0.012368774373540295\n",
      "STEP: 412600 - loss: 0.012366657817920403\n",
      "STEP: 412700 - loss: 0.012364541899207975\n",
      "STEP: 412800 - loss: 0.012362426617209295\n",
      "STEP: 412900 - loss: 0.01236031197173047\n",
      "STEP: 413000 - loss: 0.012358197962577644\n",
      "STEP: 413100 - loss: 0.012356084589557852\n",
      "STEP: 413200 - loss: 0.012353971852477235\n",
      "STEP: 413300 - loss: 0.012351859751142495\n",
      "STEP: 413400 - loss: 0.012349748285360075\n",
      "STEP: 413500 - loss: 0.012347637454936767\n",
      "STEP: 413600 - loss: 0.012345527259679303\n",
      "STEP: 413700 - loss: 0.012343417699394456\n",
      "STEP: 413800 - loss: 0.012341308773888952\n",
      "STEP: 413900 - loss: 0.012339200482969995\n",
      "STEP: 414000 - loss: 0.012337092826444147\n",
      "STEP: 414100 - loss: 0.012334985804119052\n",
      "STEP: 414200 - loss: 0.012332879415801112\n",
      "STEP: 414300 - loss: 0.01233077366129793\n",
      "STEP: 414400 - loss: 0.01232866854041639\n",
      "STEP: 414500 - loss: 0.01232656405296419\n",
      "STEP: 414600 - loss: 0.012324460198748345\n",
      "STEP: 414700 - loss: 0.012322356977576165\n",
      "STEP: 414800 - loss: 0.012320254389255065\n",
      "STEP: 414900 - loss: 0.012318152433592858\n",
      "STEP: 415000 - loss: 0.012316051110396808\n",
      "STEP: 415100 - loss: 0.012313950419474754\n",
      "STEP: 415200 - loss: 0.012311850360634192\n",
      "STEP: 415300 - loss: 0.012309750933682884\n",
      "STEP: 415400 - loss: 0.012307652138428668\n",
      "STEP: 415500 - loss: 0.012305553974679495\n",
      "STEP: 415600 - loss: 0.01230345644224303\n",
      "STEP: 415700 - loss: 0.012301359540927367\n",
      "STEP: 415800 - loss: 0.012299263270540532\n",
      "STEP: 415900 - loss: 0.012297167630890766\n",
      "STEP: 416000 - loss: 0.012295072621785686\n",
      "STEP: 416100 - loss: 0.012292978243033919\n",
      "STEP: 416200 - loss: 0.012290884494443759\n",
      "STEP: 416300 - loss: 0.012288791375823381\n",
      "STEP: 416400 - loss: 0.012286698886981303\n",
      "STEP: 416500 - loss: 0.012284607027725783\n",
      "STEP: 416600 - loss: 0.01228251579786548\n",
      "STEP: 416700 - loss: 0.012280425197208483\n",
      "STEP: 416800 - loss: 0.01227833522556385\n",
      "STEP: 416900 - loss: 0.012276245882740334\n",
      "STEP: 417000 - loss: 0.012274157168546237\n",
      "STEP: 417100 - loss: 0.012272069082790797\n",
      "STEP: 417200 - loss: 0.01226998162528243\n",
      "STEP: 417300 - loss: 0.012267894795830413\n",
      "STEP: 417400 - loss: 0.01226580859424333\n",
      "STEP: 417500 - loss: 0.01226372302033044\n",
      "STEP: 417600 - loss: 0.01226163807390093\n",
      "STEP: 417700 - loss: 0.012259553754763624\n",
      "STEP: 417800 - loss: 0.012257470062727838\n",
      "STEP: 417900 - loss: 0.012255386997602851\n",
      "STEP: 418000 - loss: 0.012253304559197771\n",
      "STEP: 418100 - loss: 0.012251222747322413\n",
      "STEP: 418200 - loss: 0.012249141561785736\n",
      "STEP: 418300 - loss: 0.012247061002397443\n",
      "STEP: 418400 - loss: 0.012244981068967228\n",
      "STEP: 418500 - loss: 0.012242901761304126\n",
      "STEP: 418600 - loss: 0.012240823079218293\n",
      "STEP: 418700 - loss: 0.012238745022519369\n",
      "STEP: 418800 - loss: 0.012236667591016836\n",
      "STEP: 418900 - loss: 0.012234590784520907\n",
      "STEP: 419000 - loss: 0.012232514602841332\n",
      "STEP: 419100 - loss: 0.012230439045787836\n",
      "STEP: 419200 - loss: 0.01222836411317067\n",
      "STEP: 419300 - loss: 0.012226289804799723\n",
      "STEP: 419400 - loss: 0.012224216120485375\n",
      "STEP: 419500 - loss: 0.012222143060037554\n",
      "STEP: 419600 - loss: 0.012220070623266533\n",
      "STEP: 419700 - loss: 0.012217998809982648\n",
      "STEP: 419800 - loss: 0.012215927619996176\n",
      "STEP: 419900 - loss: 0.012213857053117495\n",
      "STEP: 420000 - loss: 0.012211787109157244\n",
      "STEP: 420100 - loss: 0.012209717787925936\n",
      "STEP: 420200 - loss: 0.012207649089233875\n",
      "STEP: 420300 - loss: 0.012205581012891995\n",
      "STEP: 420400 - loss: 0.01220351355871083\n",
      "STEP: 420500 - loss: 0.012201446726501097\n",
      "STEP: 420600 - loss: 0.012199380516073584\n",
      "STEP: 420700 - loss: 0.012197314927239581\n",
      "STEP: 420800 - loss: 0.012195249959809331\n",
      "STEP: 420900 - loss: 0.012193185613594306\n",
      "STEP: 421000 - loss: 0.012191121888405178\n",
      "STEP: 421100 - loss: 0.012189058784053512\n",
      "STEP: 421200 - loss: 0.012186996300350078\n",
      "STEP: 421300 - loss: 0.012184934437106007\n",
      "STEP: 421400 - loss: 0.012182873194133196\n",
      "STEP: 421500 - loss: 0.012180812571242464\n",
      "STEP: 421600 - loss: 0.012178752568244886\n",
      "STEP: 421700 - loss: 0.012176693184952642\n",
      "STEP: 421800 - loss: 0.012174634421176627\n",
      "STEP: 421900 - loss: 0.012172576276728759\n",
      "STEP: 422000 - loss: 0.012170518751420655\n",
      "STEP: 422100 - loss: 0.0121684618450638\n",
      "STEP: 422200 - loss: 0.01216640555746995\n",
      "STEP: 422300 - loss: 0.012164349888450736\n",
      "STEP: 422400 - loss: 0.012162294837818426\n",
      "STEP: 422500 - loss: 0.012160240405384423\n",
      "STEP: 422600 - loss: 0.01215818659096105\n",
      "STEP: 422700 - loss: 0.01215613339436022\n",
      "STEP: 422800 - loss: 0.012154080815393873\n",
      "STEP: 422900 - loss: 0.012152028853874498\n",
      "STEP: 423000 - loss: 0.012149977509613772\n",
      "STEP: 423100 - loss: 0.012147926782424167\n",
      "STEP: 423200 - loss: 0.01214587667211812\n",
      "STEP: 423300 - loss: 0.012143827178507783\n",
      "STEP: 423400 - loss: 0.012141778301405901\n",
      "STEP: 423500 - loss: 0.012139730040624297\n",
      "STEP: 423600 - loss: 0.012137682395976108\n",
      "STEP: 423700 - loss: 0.012135635367273799\n",
      "STEP: 423800 - loss: 0.012133588954329715\n",
      "STEP: 423900 - loss: 0.012131543156956925\n",
      "STEP: 424000 - loss: 0.012129497974967958\n",
      "STEP: 424100 - loss: 0.01212745340817558\n",
      "STEP: 424200 - loss: 0.01212540945639277\n",
      "STEP: 424300 - loss: 0.012123366119432542\n",
      "STEP: 424400 - loss: 0.012121323397107654\n",
      "STEP: 424500 - loss: 0.012119281289231596\n",
      "STEP: 424600 - loss: 0.01211723979561668\n",
      "STEP: 424700 - loss: 0.012115198916076833\n",
      "STEP: 424800 - loss: 0.012113158650424692\n",
      "STEP: 424900 - loss: 0.012111118998474005\n",
      "STEP: 425000 - loss: 0.01210907996003782\n",
      "STEP: 425100 - loss: 0.012107041534929454\n",
      "STEP: 425200 - loss: 0.012105003722962433\n",
      "STEP: 425300 - loss: 0.012102966523950409\n",
      "STEP: 425400 - loss: 0.012100929937706655\n",
      "STEP: 425500 - loss: 0.012098893964045057\n",
      "STEP: 425600 - loss: 0.012096858602778987\n",
      "STEP: 425700 - loss: 0.012094823853722517\n",
      "STEP: 425800 - loss: 0.012092789716688984\n",
      "STEP: 425900 - loss: 0.012090756191492661\n",
      "STEP: 426000 - loss: 0.012088723277947192\n",
      "STEP: 426100 - loss: 0.012086690975866435\n",
      "STEP: 426200 - loss: 0.012084659285064787\n",
      "STEP: 426300 - loss: 0.01208262820535587\n",
      "STEP: 426400 - loss: 0.012080597736553957\n",
      "STEP: 426500 - loss: 0.01207856787847339\n",
      "STEP: 426600 - loss: 0.012076538630928228\n",
      "STEP: 426700 - loss: 0.012074509993732573\n",
      "STEP: 426800 - loss: 0.012072481966701246\n",
      "STEP: 426900 - loss: 0.012070454549648179\n",
      "STEP: 427000 - loss: 0.012068427742388145\n",
      "STEP: 427100 - loss: 0.012066401544735506\n",
      "STEP: 427200 - loss: 0.01206437595650479\n",
      "STEP: 427300 - loss: 0.012062350977510794\n",
      "STEP: 427400 - loss: 0.012060326607568108\n",
      "STEP: 427500 - loss: 0.01205830284649145\n",
      "STEP: 427600 - loss: 0.012056279694095756\n",
      "STEP: 427700 - loss: 0.01205425715019561\n",
      "STEP: 427800 - loss: 0.012052235214605887\n",
      "STEP: 427900 - loss: 0.012050213887141965\n",
      "STEP: 428000 - loss: 0.012048193167618553\n",
      "STEP: 428100 - loss: 0.012046173055850743\n",
      "STEP: 428200 - loss: 0.012044153551653848\n",
      "STEP: 428300 - loss: 0.012042134654842707\n",
      "STEP: 428400 - loss: 0.012040116365232893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 428500 - loss: 0.012038098682639737\n",
      "STEP: 428600 - loss: 0.012036081606878312\n",
      "STEP: 428700 - loss: 0.01203406513776416\n",
      "STEP: 428800 - loss: 0.012032049275112873\n",
      "STEP: 428900 - loss: 0.012030034018739717\n",
      "STEP: 429000 - loss: 0.012028019368460553\n",
      "STEP: 429100 - loss: 0.012026005324090903\n",
      "STEP: 429200 - loss: 0.01202399188544634\n",
      "STEP: 429300 - loss: 0.012021979052342773\n",
      "STEP: 429400 - loss: 0.012019966824595971\n",
      "STEP: 429500 - loss: 0.012017955202021756\n",
      "STEP: 429600 - loss: 0.012015944184436045\n",
      "STEP: 429700 - loss: 0.01201393377165504\n",
      "STEP: 429800 - loss: 0.012011923963494394\n",
      "STEP: 429900 - loss: 0.012009914759770337\n",
      "STEP: 430000 - loss: 0.01200790616029902\n",
      "STEP: 430100 - loss: 0.012005898164896703\n",
      "STEP: 430200 - loss: 0.012003890773379594\n",
      "STEP: 430300 - loss: 0.012001883985564177\n",
      "STEP: 430400 - loss: 0.011999877801266299\n",
      "STEP: 430500 - loss: 0.011997872220303204\n",
      "STEP: 430600 - loss: 0.01199586724249057\n",
      "STEP: 430700 - loss: 0.011993862867645264\n",
      "STEP: 430800 - loss: 0.011991859095584024\n",
      "STEP: 430900 - loss: 0.011989855926123158\n",
      "STEP: 431000 - loss: 0.011987853359079861\n",
      "STEP: 431100 - loss: 0.011985851394270162\n",
      "STEP: 431200 - loss: 0.011983850031511594\n",
      "STEP: 431300 - loss: 0.011981849270620534\n",
      "STEP: 431400 - loss: 0.011979849111414073\n",
      "STEP: 431500 - loss: 0.011977849553709318\n",
      "STEP: 431600 - loss: 0.011975850597323102\n",
      "STEP: 431700 - loss: 0.011973852242072728\n",
      "STEP: 431800 - loss: 0.01197185448777496\n",
      "STEP: 431900 - loss: 0.011969857334247564\n",
      "STEP: 432000 - loss: 0.011967860781307418\n",
      "STEP: 432100 - loss: 0.01196586482877186\n",
      "STEP: 432200 - loss: 0.011963869476458385\n",
      "STEP: 432300 - loss: 0.011961874724184121\n",
      "STEP: 432400 - loss: 0.011959880571766982\n",
      "STEP: 432500 - loss: 0.011957887019024319\n",
      "STEP: 432600 - loss: 0.011955894065773642\n",
      "STEP: 432700 - loss: 0.011953901711832414\n",
      "STEP: 432800 - loss: 0.011951909957018898\n",
      "STEP: 432900 - loss: 0.011949918801150464\n",
      "STEP: 433000 - loss: 0.011947928244044777\n",
      "STEP: 433100 - loss: 0.01194593828552008\n",
      "STEP: 433200 - loss: 0.011943948925394025\n",
      "STEP: 433300 - loss: 0.011941960163484849\n",
      "STEP: 433400 - loss: 0.011939971999610211\n",
      "STEP: 433500 - loss: 0.01193798443358859\n",
      "STEP: 433600 - loss: 0.011935997465238038\n",
      "STEP: 433700 - loss: 0.011934011094376517\n",
      "STEP: 433800 - loss: 0.01193202532082251\n",
      "STEP: 433900 - loss: 0.011930040144394055\n",
      "STEP: 434000 - loss: 0.01192805556490993\n",
      "STEP: 434100 - loss: 0.011926071582188408\n",
      "STEP: 434200 - loss: 0.011924088196047733\n",
      "STEP: 434300 - loss: 0.01192210540630679\n",
      "STEP: 434400 - loss: 0.011920123212783863\n",
      "STEP: 434500 - loss: 0.01191814161529776\n",
      "STEP: 434600 - loss: 0.011916160613667234\n",
      "STEP: 434700 - loss: 0.011914180207710727\n",
      "STEP: 434800 - loss: 0.011912200397247432\n",
      "STEP: 434900 - loss: 0.011910221182096026\n",
      "STEP: 435000 - loss: 0.011908242562075474\n",
      "STEP: 435100 - loss: 0.011906264537004666\n",
      "STEP: 435200 - loss: 0.011904287106702485\n",
      "STEP: 435300 - loss: 0.01190231027098855\n",
      "STEP: 435400 - loss: 0.011900334029681436\n",
      "STEP: 435500 - loss: 0.011898358382600787\n",
      "STEP: 435600 - loss: 0.011896383329565285\n",
      "STEP: 435700 - loss: 0.011894408870394854\n",
      "STEP: 435800 - loss: 0.011892435004908492\n",
      "STEP: 435900 - loss: 0.011890461732925638\n",
      "STEP: 436000 - loss: 0.011888489054265943\n",
      "STEP: 436100 - loss: 0.01188651696874878\n",
      "STEP: 436200 - loss: 0.011884545476193484\n",
      "STEP: 436300 - loss: 0.011882574576420266\n",
      "STEP: 436400 - loss: 0.011880604269248352\n",
      "STEP: 436500 - loss: 0.01187863455449763\n",
      "STEP: 436600 - loss: 0.011876665431987812\n",
      "STEP: 436700 - loss: 0.011874696901539116\n",
      "STEP: 436800 - loss: 0.011872728962970919\n",
      "STEP: 436900 - loss: 0.011870761616103601\n",
      "STEP: 437000 - loss: 0.011868794860756971\n",
      "STEP: 437100 - loss: 0.011866828696751125\n",
      "STEP: 437200 - loss: 0.01186486312390608\n",
      "STEP: 437300 - loss: 0.011862898142042331\n",
      "STEP: 437400 - loss: 0.011860933750980011\n",
      "STEP: 437500 - loss: 0.011858969950539065\n",
      "STEP: 437600 - loss: 0.011857006740540172\n",
      "STEP: 437700 - loss: 0.011855044120803638\n",
      "STEP: 437800 - loss: 0.011853082091150028\n",
      "STEP: 437900 - loss: 0.011851120651399809\n",
      "STEP: 438000 - loss: 0.011849159801373493\n",
      "STEP: 438100 - loss: 0.011847199540891376\n",
      "STEP: 438200 - loss: 0.011845239869774739\n",
      "STEP: 438300 - loss: 0.011843280787843856\n",
      "STEP: 438400 - loss: 0.011841322294919802\n",
      "STEP: 438500 - loss: 0.011839364390823372\n",
      "STEP: 438600 - loss: 0.011837407075375244\n",
      "STEP: 438700 - loss: 0.011835450348396415\n",
      "STEP: 438800 - loss: 0.011833494209708424\n",
      "STEP: 438900 - loss: 0.011831538659131275\n",
      "STEP: 439000 - loss: 0.011829583696487001\n",
      "STEP: 439100 - loss: 0.011827629321596412\n",
      "STEP: 439200 - loss: 0.011825675534280714\n",
      "STEP: 439300 - loss: 0.011823722334361067\n",
      "STEP: 439400 - loss: 0.011821769721659178\n",
      "STEP: 439500 - loss: 0.01181981769599595\n",
      "STEP: 439600 - loss: 0.01181786625719317\n",
      "STEP: 439700 - loss: 0.011815915405072146\n",
      "STEP: 439800 - loss: 0.011813965139454537\n",
      "STEP: 439900 - loss: 0.01181201546016193\n",
      "STEP: 440000 - loss: 0.011810066367015985\n",
      "STEP: 440100 - loss: 0.011808117859838289\n",
      "STEP: 440200 - loss: 0.011806169938450472\n",
      "STEP: 440300 - loss: 0.01180422260267511\n",
      "STEP: 440400 - loss: 0.011802275852332775\n",
      "STEP: 440500 - loss: 0.011800329687246699\n",
      "STEP: 440600 - loss: 0.01179838410723818\n",
      "STEP: 440700 - loss: 0.011796439112129247\n",
      "STEP: 440800 - loss: 0.011794494701742047\n",
      "STEP: 440900 - loss: 0.01179255087589905\n",
      "STEP: 441000 - loss: 0.011790607634421819\n",
      "STEP: 441100 - loss: 0.011788664977133271\n",
      "STEP: 441200 - loss: 0.011786722903855209\n",
      "STEP: 441300 - loss: 0.011784781414410379\n",
      "STEP: 441400 - loss: 0.011782840508620739\n",
      "STEP: 441500 - loss: 0.01178090018630915\n",
      "STEP: 441600 - loss: 0.011778960447297955\n",
      "STEP: 441700 - loss: 0.01177702129140966\n",
      "STEP: 441800 - loss: 0.011775082718467094\n",
      "STEP: 441900 - loss: 0.011773144728292832\n",
      "STEP: 442000 - loss: 0.011771207320709562\n",
      "STEP: 442100 - loss: 0.011769270495539965\n",
      "STEP: 442200 - loss: 0.011767334252607193\n",
      "STEP: 442300 - loss: 0.011765398591734073\n",
      "STEP: 442400 - loss: 0.011763463512743374\n",
      "STEP: 442500 - loss: 0.011761529015457928\n",
      "STEP: 442600 - loss: 0.011759595099701375\n",
      "STEP: 442700 - loss: 0.011757661765296464\n",
      "STEP: 442800 - loss: 0.011755729012066408\n",
      "STEP: 442900 - loss: 0.011753796839834298\n",
      "STEP: 443000 - loss: 0.011751865248423586\n",
      "STEP: 443100 - loss: 0.011749934237657654\n",
      "STEP: 443200 - loss: 0.011748003807359578\n",
      "STEP: 443300 - loss: 0.011746073957353177\n",
      "STEP: 443400 - loss: 0.011744144687461528\n",
      "STEP: 443500 - loss: 0.011742215997508685\n",
      "STEP: 443600 - loss: 0.011740287887317525\n",
      "STEP: 443700 - loss: 0.011738360356712472\n",
      "STEP: 443800 - loss: 0.011736433405516788\n",
      "STEP: 443900 - loss: 0.011734507033554411\n",
      "STEP: 444000 - loss: 0.01173258124064879\n",
      "STEP: 444100 - loss: 0.011730656026624212\n",
      "STEP: 444200 - loss: 0.011728731391304408\n",
      "STEP: 444300 - loss: 0.011726807334513336\n",
      "STEP: 444400 - loss: 0.011724883856075154\n",
      "STEP: 444500 - loss: 0.011722960955813847\n",
      "STEP: 444600 - loss: 0.011721038633553543\n",
      "STEP: 444700 - loss: 0.011719116889118407\n",
      "STEP: 444800 - loss: 0.011717195722332669\n",
      "STEP: 444900 - loss: 0.011715275133020736\n",
      "STEP: 445000 - loss: 0.01171335512100671\n",
      "STEP: 445100 - loss: 0.011711435686115353\n",
      "STEP: 445200 - loss: 0.011709516828170766\n",
      "STEP: 445300 - loss: 0.011707598546997636\n",
      "STEP: 445400 - loss: 0.011705680842420498\n",
      "STEP: 445500 - loss: 0.01170376371426397\n",
      "STEP: 445600 - loss: 0.011701847162352706\n",
      "STEP: 445700 - loss: 0.011699931186511369\n",
      "STEP: 445800 - loss: 0.01169801578656479\n",
      "STEP: 445900 - loss: 0.011696100962337894\n",
      "STEP: 446000 - loss: 0.011694186713655321\n",
      "STEP: 446100 - loss: 0.011692273040342125\n",
      "STEP: 446200 - loss: 0.011690359942223486\n",
      "STEP: 446300 - loss: 0.011688447419123987\n",
      "STEP: 446400 - loss: 0.011686535470869158\n",
      "STEP: 446500 - loss: 0.011684624097284015\n",
      "STEP: 446600 - loss: 0.01168271329819367\n",
      "STEP: 446700 - loss: 0.011680803073423281\n",
      "STEP: 446800 - loss: 0.01167889342279837\n",
      "STEP: 446900 - loss: 0.011676984346144333\n",
      "STEP: 447000 - loss: 0.011675075843286415\n",
      "STEP: 447100 - loss: 0.01167316791405009\n",
      "STEP: 447200 - loss: 0.01167126055826097\n",
      "STEP: 447300 - loss: 0.011669353775744466\n",
      "STEP: 447400 - loss: 0.01166744756632632\n",
      "STEP: 447500 - loss: 0.011665541929832244\n",
      "STEP: 447600 - loss: 0.011663636866087866\n",
      "STEP: 447700 - loss: 0.01166173237491909\n",
      "STEP: 447800 - loss: 0.011659828456151784\n",
      "STEP: 447900 - loss: 0.011657925109611549\n",
      "STEP: 448000 - loss: 0.01165602233512452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 448100 - loss: 0.01165412013251675\n",
      "STEP: 448200 - loss: 0.011652218501614168\n",
      "STEP: 448300 - loss: 0.011650317442242902\n",
      "STEP: 448400 - loss: 0.011648416954229092\n",
      "STEP: 448500 - loss: 0.011646517037399145\n",
      "STEP: 448600 - loss: 0.011644617691579134\n",
      "STEP: 448700 - loss: 0.011642718916595398\n",
      "STEP: 448800 - loss: 0.011640820712274094\n",
      "STEP: 448900 - loss: 0.011638923078442045\n",
      "STEP: 449000 - loss: 0.011637026014925392\n",
      "STEP: 449100 - loss: 0.011635129521550831\n",
      "STEP: 449200 - loss: 0.011633233598144839\n",
      "STEP: 449300 - loss: 0.011631338244534177\n",
      "STEP: 449400 - loss: 0.011629443460545398\n",
      "STEP: 449500 - loss: 0.011627549246005361\n",
      "STEP: 449600 - loss: 0.011625655600740666\n",
      "STEP: 449700 - loss: 0.011623762524578172\n",
      "STEP: 449800 - loss: 0.011621870017345017\n",
      "STEP: 449900 - loss: 0.01161997807886803\n",
      "STEP: 450000 - loss: 0.011618086708973971\n",
      "STEP: 450100 - loss: 0.011616195907490185\n",
      "STEP: 450200 - loss: 0.011614305674243706\n",
      "STEP: 450300 - loss: 0.011612416009061446\n",
      "STEP: 450400 - loss: 0.011610526911771234\n",
      "STEP: 450500 - loss: 0.011608638382199455\n",
      "STEP: 450600 - loss: 0.011606750420174346\n",
      "STEP: 450700 - loss: 0.011604863025522574\n",
      "STEP: 450800 - loss: 0.011602976198071662\n",
      "STEP: 450900 - loss: 0.011601089937649491\n",
      "STEP: 451000 - loss: 0.011599204244083047\n",
      "STEP: 451100 - loss: 0.011597319117200208\n",
      "STEP: 451200 - loss: 0.011595434556828724\n",
      "STEP: 451300 - loss: 0.01159355056279613\n",
      "STEP: 451400 - loss: 0.011591667134929977\n",
      "STEP: 451500 - loss: 0.011589784273058268\n",
      "STEP: 451600 - loss: 0.011587901977008755\n",
      "STEP: 451700 - loss: 0.01158602024660946\n",
      "STEP: 451800 - loss: 0.011584139081688132\n",
      "STEP: 451900 - loss: 0.011582258482072835\n",
      "STEP: 452000 - loss: 0.011580378447591685\n",
      "STEP: 452100 - loss: 0.01157849897807295\n",
      "STEP: 452200 - loss: 0.011576620073344101\n",
      "STEP: 452300 - loss: 0.011574741733234065\n",
      "STEP: 452400 - loss: 0.011572863957570918\n",
      "STEP: 452500 - loss: 0.011570986746182805\n",
      "STEP: 452600 - loss: 0.011569110098898166\n",
      "STEP: 452700 - loss: 0.011567234015545408\n",
      "STEP: 452800 - loss: 0.011565358495953004\n",
      "STEP: 452900 - loss: 0.011563483539949503\n",
      "STEP: 453000 - loss: 0.011561609147363372\n",
      "STEP: 453100 - loss: 0.011559735318023408\n",
      "STEP: 453200 - loss: 0.011557862051758166\n",
      "STEP: 453300 - loss: 0.011555989348396275\n",
      "STEP: 453400 - loss: 0.01155411720776672\n",
      "STEP: 453500 - loss: 0.011552245629698395\n",
      "STEP: 453600 - loss: 0.01155037461401982\n",
      "STEP: 453700 - loss: 0.011548504160560226\n",
      "STEP: 453800 - loss: 0.01154663426914842\n",
      "STEP: 453900 - loss: 0.011544764939613688\n",
      "STEP: 454000 - loss: 0.011542896171784799\n",
      "STEP: 454100 - loss: 0.011541027965491172\n",
      "STEP: 454200 - loss: 0.01153916032056186\n",
      "STEP: 454300 - loss: 0.011537293236826103\n",
      "STEP: 454400 - loss: 0.011535426714113343\n",
      "STEP: 454500 - loss: 0.011533560752252542\n",
      "STEP: 454600 - loss: 0.011531695351073432\n",
      "STEP: 454700 - loss: 0.011529830510405592\n",
      "STEP: 454800 - loss: 0.01152796623007834\n",
      "STEP: 454900 - loss: 0.011526102509921084\n",
      "STEP: 455000 - loss: 0.011524239349763894\n",
      "STEP: 455100 - loss: 0.01152237674943599\n",
      "STEP: 455200 - loss: 0.011520514708766921\n",
      "STEP: 455300 - loss: 0.011518653227587144\n",
      "STEP: 455400 - loss: 0.011516792305725979\n",
      "STEP: 455500 - loss: 0.011514931943013276\n",
      "STEP: 455600 - loss: 0.011513072139279228\n",
      "STEP: 455700 - loss: 0.011511212894353584\n",
      "STEP: 455800 - loss: 0.011509354208066522\n",
      "STEP: 455900 - loss: 0.011507496080248055\n",
      "STEP: 456000 - loss: 0.01150563851072817\n",
      "STEP: 456100 - loss: 0.01150378149933731\n",
      "STEP: 456200 - loss: 0.011501925045905442\n",
      "STEP: 456300 - loss: 0.011500069150263113\n",
      "STEP: 456400 - loss: 0.011498213812240417\n",
      "STEP: 456500 - loss: 0.011496359031667833\n",
      "STEP: 456600 - loss: 0.011494504808375753\n",
      "STEP: 456700 - loss: 0.011492651142194842\n",
      "STEP: 456800 - loss: 0.011490798032955418\n",
      "STEP: 456900 - loss: 0.011488945480488432\n",
      "STEP: 457000 - loss: 0.011487093484623945\n",
      "STEP: 457100 - loss: 0.011485242045193172\n",
      "STEP: 457200 - loss: 0.011483391162026416\n",
      "STEP: 457300 - loss: 0.01148154083495506\n",
      "STEP: 457400 - loss: 0.011479691063809475\n",
      "STEP: 457500 - loss: 0.011477841848420631\n",
      "STEP: 457600 - loss: 0.011475993188619603\n",
      "STEP: 457700 - loss: 0.011474145084237341\n",
      "STEP: 457800 - loss: 0.01147229753510482\n",
      "STEP: 457900 - loss: 0.011470450541053494\n",
      "STEP: 458000 - loss: 0.011468604101913926\n",
      "STEP: 458100 - loss: 0.011466758217517787\n",
      "STEP: 458200 - loss: 0.011464912887696231\n",
      "STEP: 458300 - loss: 0.011463068112280417\n",
      "STEP: 458400 - loss: 0.011461223891102074\n",
      "STEP: 458500 - loss: 0.011459380223992188\n",
      "STEP: 458600 - loss: 0.011457537110782508\n",
      "STEP: 458700 - loss: 0.011455694551304397\n",
      "STEP: 458800 - loss: 0.011453852545389503\n",
      "STEP: 458900 - loss: 0.011452011092869259\n",
      "STEP: 459000 - loss: 0.011450170193575835\n",
      "STEP: 459100 - loss: 0.011448329847340451\n",
      "STEP: 459200 - loss: 0.011446490053994996\n",
      "STEP: 459300 - loss: 0.011444650813371402\n",
      "STEP: 459400 - loss: 0.011442812125301679\n",
      "STEP: 459500 - loss: 0.011440973989617416\n",
      "STEP: 459600 - loss: 0.01143913640615066\n",
      "STEP: 459700 - loss: 0.011437299374733536\n",
      "STEP: 459800 - loss: 0.01143546289519813\n",
      "STEP: 459900 - loss: 0.011433626967376447\n",
      "STEP: 460000 - loss: 0.01143179159110103\n",
      "STEP: 460100 - loss: 0.011429956766203698\n",
      "STEP: 460200 - loss: 0.011428122492516766\n",
      "STEP: 460300 - loss: 0.011426288769873064\n",
      "STEP: 460400 - loss: 0.01142445559810425\n",
      "STEP: 460500 - loss: 0.01142262297704336\n",
      "STEP: 460600 - loss: 0.011420790906522552\n",
      "STEP: 460700 - loss: 0.011418959386374532\n",
      "STEP: 460800 - loss: 0.01141712841643162\n",
      "STEP: 460900 - loss: 0.011415297996526705\n",
      "STEP: 461000 - loss: 0.011413468126492557\n",
      "STEP: 461100 - loss: 0.011411638806161795\n",
      "STEP: 461200 - loss: 0.011409810035367115\n",
      "STEP: 461300 - loss: 0.011407981813941624\n",
      "STEP: 461400 - loss: 0.011406154141717886\n",
      "STEP: 461500 - loss: 0.011404327018529033\n",
      "STEP: 461600 - loss: 0.011402500444207984\n",
      "STEP: 461700 - loss: 0.011400674418587943\n",
      "STEP: 461800 - loss: 0.011398848941501937\n",
      "STEP: 461900 - loss: 0.011397024012783153\n",
      "STEP: 462000 - loss: 0.011395199632264544\n",
      "STEP: 462100 - loss: 0.01139337579977966\n",
      "STEP: 462200 - loss: 0.011391552515161723\n",
      "STEP: 462300 - loss: 0.011389729778243835\n",
      "STEP: 462400 - loss: 0.011387907588859816\n",
      "STEP: 462500 - loss: 0.011386085946842813\n",
      "STEP: 462600 - loss: 0.011384264852026598\n",
      "STEP: 462700 - loss: 0.011382444304244402\n",
      "STEP: 462800 - loss: 0.011380624303329962\n",
      "STEP: 462900 - loss: 0.011378804849117023\n",
      "STEP: 463000 - loss: 0.01137698594143925\n",
      "STEP: 463100 - loss: 0.011375167580130477\n",
      "STEP: 463200 - loss: 0.011373349765024278\n",
      "STEP: 463300 - loss: 0.011371532495954689\n",
      "STEP: 463400 - loss: 0.01136971577275586\n",
      "STEP: 463500 - loss: 0.011367899595261195\n",
      "STEP: 463600 - loss: 0.011366083963305032\n",
      "STEP: 463700 - loss: 0.011364268876721335\n",
      "STEP: 463800 - loss: 0.011362454335344241\n",
      "STEP: 463900 - loss: 0.011360640339008236\n",
      "STEP: 464000 - loss: 0.011358826887546908\n",
      "STEP: 464100 - loss: 0.011357013980794925\n",
      "STEP: 464200 - loss: 0.01135520161858656\n",
      "STEP: 464300 - loss: 0.011353389800756095\n",
      "STEP: 464400 - loss: 0.011351578527138008\n",
      "STEP: 464500 - loss: 0.011349767797566877\n",
      "STEP: 464600 - loss: 0.011347957611876871\n",
      "STEP: 464700 - loss: 0.011346147969902926\n",
      "STEP: 464800 - loss: 0.011344338871479478\n",
      "STEP: 464900 - loss: 0.011342530316441281\n",
      "STEP: 465000 - loss: 0.011340722304622828\n",
      "STEP: 465100 - loss: 0.011338914835859075\n",
      "STEP: 465200 - loss: 0.011337107909984804\n",
      "STEP: 465300 - loss: 0.01133530152683488\n",
      "STEP: 465400 - loss: 0.011333495686244343\n",
      "STEP: 465500 - loss: 0.01133169038804786\n",
      "STEP: 465600 - loss: 0.01132988563208083\n",
      "STEP: 465700 - loss: 0.011328081418177899\n",
      "STEP: 465800 - loss: 0.011326277746174718\n",
      "STEP: 465900 - loss: 0.011324474615905833\n",
      "STEP: 466000 - loss: 0.011322672027207006\n",
      "STEP: 466100 - loss: 0.011320869979913236\n",
      "STEP: 466200 - loss: 0.01131906847385966\n",
      "STEP: 466300 - loss: 0.011317267508882256\n",
      "STEP: 466400 - loss: 0.011315467084815775\n",
      "STEP: 466500 - loss: 0.011313667201495885\n",
      "STEP: 466600 - loss: 0.011311867858758528\n",
      "STEP: 466700 - loss: 0.011310069056438563\n",
      "STEP: 466800 - loss: 0.011308270794372145\n",
      "STEP: 466900 - loss: 0.011306473072394798\n",
      "STEP: 467000 - loss: 0.011304675890342283\n",
      "STEP: 467100 - loss: 0.011302879248050056\n",
      "STEP: 467200 - loss: 0.011301083145354291\n",
      "STEP: 467300 - loss: 0.011299287582090957\n",
      "STEP: 467400 - loss: 0.01129749255809549\n",
      "STEP: 467500 - loss: 0.011295698073204298\n",
      "STEP: 467600 - loss: 0.011293904127253337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 467700 - loss: 0.011292110720078189\n",
      "STEP: 467800 - loss: 0.01129031785151562\n",
      "STEP: 467900 - loss: 0.011288525521401457\n",
      "STEP: 468000 - loss: 0.011286733729572031\n",
      "STEP: 468100 - loss: 0.011284942475863681\n",
      "STEP: 468200 - loss: 0.011283151760112608\n",
      "STEP: 468300 - loss: 0.01128136158215506\n",
      "STEP: 468400 - loss: 0.011279571941827724\n",
      "STEP: 468500 - loss: 0.011277782838966622\n",
      "STEP: 468600 - loss: 0.011275994273408776\n",
      "STEP: 468700 - loss: 0.011274206244990717\n",
      "STEP: 468800 - loss: 0.011272418753548629\n",
      "STEP: 468900 - loss: 0.011270631798919748\n",
      "STEP: 469000 - loss: 0.01126884538094035\n",
      "STEP: 469100 - loss: 0.01126705949944718\n",
      "STEP: 469200 - loss: 0.011265274154277314\n",
      "STEP: 469300 - loss: 0.011263489345267537\n",
      "STEP: 469400 - loss: 0.011261705072254853\n",
      "STEP: 469500 - loss: 0.011259921335076\n",
      "STEP: 469600 - loss: 0.011258138133568149\n",
      "STEP: 469700 - loss: 0.011256355467568358\n",
      "STEP: 469800 - loss: 0.01125457333691357\n",
      "STEP: 469900 - loss: 0.011252791741441035\n",
      "STEP: 470000 - loss: 0.011251010680988282\n",
      "STEP: 470100 - loss: 0.01124923015539193\n",
      "STEP: 470200 - loss: 0.011247450164490143\n",
      "STEP: 470300 - loss: 0.011245670708119384\n",
      "STEP: 470400 - loss: 0.01124389178611761\n",
      "STEP: 470500 - loss: 0.011242113398322113\n",
      "STEP: 470600 - loss: 0.011240335544570476\n",
      "STEP: 470700 - loss: 0.01123855822470042\n",
      "STEP: 470800 - loss: 0.01123678143854922\n",
      "STEP: 470900 - loss: 0.01123500518595452\n",
      "STEP: 471000 - loss: 0.011233229466754296\n",
      "STEP: 471100 - loss: 0.011231454280786042\n",
      "STEP: 471200 - loss: 0.011229679627887857\n",
      "STEP: 471300 - loss: 0.011227905507897474\n",
      "STEP: 471400 - loss: 0.011226131920652518\n",
      "STEP: 471500 - loss: 0.011224358865991437\n",
      "STEP: 471600 - loss: 0.011222586343751803\n",
      "STEP: 471700 - loss: 0.011220814353771907\n",
      "STEP: 471800 - loss: 0.01121904289588977\n",
      "STEP: 471900 - loss: 0.011217271969943803\n",
      "STEP: 472000 - loss: 0.01121550157577168\n",
      "STEP: 472100 - loss: 0.011213731713211997\n",
      "STEP: 472200 - loss: 0.01121196238210306\n",
      "STEP: 472300 - loss: 0.011210193582283081\n",
      "STEP: 472400 - loss: 0.01120842531359055\n",
      "STEP: 472500 - loss: 0.011206657575863803\n",
      "STEP: 472600 - loss: 0.01120489036894166\n",
      "STEP: 472700 - loss: 0.01120312369266209\n",
      "STEP: 472800 - loss: 0.011201357546864205\n",
      "STEP: 472900 - loss: 0.011199591931386494\n",
      "STEP: 473000 - loss: 0.011197826846067484\n",
      "STEP: 473100 - loss: 0.01119606229074607\n",
      "STEP: 473200 - loss: 0.01119429826526117\n",
      "STEP: 473300 - loss: 0.011192534769451088\n",
      "STEP: 473400 - loss: 0.011190771803155454\n",
      "STEP: 473500 - loss: 0.011189009366212782\n",
      "STEP: 473600 - loss: 0.011187247458462108\n",
      "STEP: 473700 - loss: 0.011185486079742506\n",
      "STEP: 473800 - loss: 0.011183725229892921\n",
      "STEP: 473900 - loss: 0.011181964908752567\n",
      "STEP: 474000 - loss: 0.011180205116160772\n",
      "STEP: 474100 - loss: 0.011178445851956566\n",
      "STEP: 474200 - loss: 0.011176687115979378\n",
      "STEP: 474300 - loss: 0.011174928908068243\n",
      "STEP: 474400 - loss: 0.011173171228062925\n",
      "STEP: 474500 - loss: 0.011171414075802726\n",
      "STEP: 474600 - loss: 0.011169657451127088\n",
      "STEP: 474700 - loss: 0.011167901353875498\n",
      "STEP: 474800 - loss: 0.011166145783887551\n",
      "STEP: 474900 - loss: 0.011164390741002768\n",
      "STEP: 475000 - loss: 0.011162636225061149\n",
      "STEP: 475100 - loss: 0.01116088223590195\n",
      "STEP: 475200 - loss: 0.011159128773365347\n",
      "STEP: 475300 - loss: 0.011157375837290973\n",
      "STEP: 475400 - loss: 0.011155623427518642\n",
      "STEP: 475500 - loss: 0.011153871543888208\n",
      "STEP: 475600 - loss: 0.011152120186240021\n",
      "STEP: 475700 - loss: 0.011150369354413728\n",
      "STEP: 475800 - loss: 0.011148619048249445\n",
      "STEP: 475900 - loss: 0.011146869267587357\n",
      "STEP: 476000 - loss: 0.01114512001226745\n",
      "STEP: 476100 - loss: 0.011143371282130284\n",
      "STEP: 476200 - loss: 0.01114162307701572\n",
      "STEP: 476300 - loss: 0.011139875396764407\n",
      "STEP: 476400 - loss: 0.011138128241216386\n",
      "STEP: 476500 - loss: 0.011136381610212253\n",
      "STEP: 476600 - loss: 0.01113463550359228\n",
      "STEP: 476700 - loss: 0.011132889921197354\n",
      "STEP: 476800 - loss: 0.011131144862867476\n",
      "STEP: 476900 - loss: 0.011129400328443697\n",
      "STEP: 477000 - loss: 0.011127656317766466\n",
      "STEP: 477100 - loss: 0.011125912830676397\n",
      "STEP: 477200 - loss: 0.011124169867014137\n",
      "STEP: 477300 - loss: 0.011122427426621024\n",
      "STEP: 477400 - loss: 0.011120685509337393\n",
      "STEP: 477500 - loss: 0.011118944115004154\n",
      "STEP: 477600 - loss: 0.011117203243462426\n",
      "STEP: 477700 - loss: 0.01111546289455306\n",
      "STEP: 477800 - loss: 0.011113723068117306\n",
      "STEP: 477900 - loss: 0.011111983763996037\n",
      "STEP: 478000 - loss: 0.011110244982030237\n",
      "STEP: 478100 - loss: 0.011108506722061416\n",
      "STEP: 478200 - loss: 0.011106768983930575\n",
      "STEP: 478300 - loss: 0.01110503176747901\n",
      "STEP: 478400 - loss: 0.01110329507254831\n",
      "STEP: 478500 - loss: 0.01110155889897928\n",
      "STEP: 478600 - loss: 0.011099823246613983\n",
      "STEP: 478700 - loss: 0.011098088115293423\n",
      "STEP: 478800 - loss: 0.011096353504859241\n",
      "STEP: 478900 - loss: 0.011094619415153047\n",
      "STEP: 479000 - loss: 0.011092885846016424\n",
      "STEP: 479100 - loss: 0.011091152797291157\n",
      "STEP: 479200 - loss: 0.011089420268818873\n",
      "STEP: 479300 - loss: 0.011087688260441054\n",
      "STEP: 479400 - loss: 0.011085956771999809\n",
      "STEP: 479500 - loss: 0.011084225803336926\n",
      "STEP: 479600 - loss: 0.01108249535429432\n",
      "STEP: 479700 - loss: 0.011080765424713932\n",
      "STEP: 479800 - loss: 0.011079036014437438\n",
      "STEP: 479900 - loss: 0.01107730712330751\n",
      "STEP: 480000 - loss: 0.01107557875116571\n",
      "STEP: 480100 - loss: 0.01107385089785435\n",
      "STEP: 480200 - loss: 0.011072123563215835\n",
      "STEP: 480300 - loss: 0.011070396747092013\n",
      "STEP: 480400 - loss: 0.011068670449325512\n",
      "STEP: 480500 - loss: 0.011066944669758346\n",
      "STEP: 480600 - loss: 0.011065219408233146\n",
      "STEP: 480700 - loss: 0.011063494664592242\n",
      "STEP: 480800 - loss: 0.011061770438678002\n",
      "STEP: 480900 - loss: 0.011060046730333197\n",
      "STEP: 481000 - loss: 0.011058323539400217\n",
      "STEP: 481100 - loss: 0.011056600865721506\n",
      "STEP: 481200 - loss: 0.01105487870914023\n",
      "STEP: 481300 - loss: 0.011053157069498628\n",
      "STEP: 481400 - loss: 0.011051435946639673\n",
      "STEP: 481500 - loss: 0.011049715340406297\n",
      "STEP: 481600 - loss: 0.011047995250641162\n",
      "STEP: 481700 - loss: 0.011046275677187129\n",
      "STEP: 481800 - loss: 0.011044556619887359\n",
      "STEP: 481900 - loss: 0.011042838078584676\n",
      "STEP: 482000 - loss: 0.01104112005312237\n",
      "STEP: 482100 - loss: 0.011039402543343074\n",
      "STEP: 482200 - loss: 0.011037685549090457\n",
      "STEP: 482300 - loss: 0.011035969070207275\n",
      "STEP: 482400 - loss: 0.011034253106537103\n",
      "STEP: 482500 - loss: 0.011032537657923088\n",
      "STEP: 482600 - loss: 0.011030822724208422\n",
      "STEP: 482700 - loss: 0.011029108305236924\n",
      "STEP: 482800 - loss: 0.01102739440085136\n",
      "STEP: 482900 - loss: 0.011025681010895742\n",
      "STEP: 483000 - loss: 0.011023968135213688\n",
      "STEP: 483100 - loss: 0.011022255773648034\n",
      "STEP: 483200 - loss: 0.011020543926043184\n",
      "STEP: 483300 - loss: 0.01101883259224255\n",
      "STEP: 483400 - loss: 0.011017121772089866\n",
      "STEP: 483500 - loss: 0.01101541146542869\n",
      "STEP: 483600 - loss: 0.011013701672102885\n",
      "STEP: 483700 - loss: 0.011011992391956705\n",
      "STEP: 483800 - loss: 0.01101028362483342\n",
      "STEP: 483900 - loss: 0.011008575370577514\n",
      "STEP: 484000 - loss: 0.011006867629032637\n",
      "STEP: 484100 - loss: 0.011005160400043061\n",
      "STEP: 484200 - loss: 0.011003453683452983\n",
      "STEP: 484300 - loss: 0.01100174747910608\n",
      "STEP: 484400 - loss: 0.011000041786847194\n",
      "STEP: 484500 - loss: 0.010998336606519901\n",
      "STEP: 484600 - loss: 0.010996631937968928\n",
      "STEP: 484700 - loss: 0.010994927781038515\n",
      "STEP: 484800 - loss: 0.010993224135572788\n",
      "STEP: 484900 - loss: 0.010991521001416642\n",
      "STEP: 485000 - loss: 0.010989818378414217\n",
      "STEP: 485100 - loss: 0.010988116266410152\n",
      "STEP: 485200 - loss: 0.010986414665248906\n",
      "STEP: 485300 - loss: 0.01098471357477525\n",
      "STEP: 485400 - loss: 0.01098301299483366\n",
      "STEP: 485500 - loss: 0.010981312925268995\n",
      "STEP: 485600 - loss: 0.010979613365926236\n",
      "STEP: 485700 - loss: 0.010977914316649488\n",
      "STEP: 485800 - loss: 0.010976215777284352\n",
      "STEP: 485900 - loss: 0.010974517747675301\n",
      "STEP: 486000 - loss: 0.010972820227667428\n",
      "STEP: 486100 - loss: 0.010971123217105788\n",
      "STEP: 486200 - loss: 0.010969426715835175\n",
      "STEP: 486300 - loss: 0.010967730723700903\n",
      "STEP: 486400 - loss: 0.010966035240548042\n",
      "STEP: 486500 - loss: 0.010964340266221822\n",
      "STEP: 486600 - loss: 0.0109626458005675\n",
      "STEP: 486700 - loss: 0.010960951843430317\n",
      "STEP: 486800 - loss: 0.010959258394655304\n",
      "STEP: 486900 - loss: 0.01095756545408822\n",
      "STEP: 487000 - loss: 0.0109558730215745\n",
      "STEP: 487100 - loss: 0.010954181096959203\n",
      "STEP: 487200 - loss: 0.010952489680088346\n",
      "STEP: 487300 - loss: 0.010950798770807107\n",
      "STEP: 487400 - loss: 0.010949108368961203\n",
      "STEP: 487500 - loss: 0.010947418474396352\n",
      "STEP: 487600 - loss: 0.010945729086958378\n",
      "STEP: 487700 - loss: 0.010944040206492804\n",
      "STEP: 487800 - loss: 0.01094235183284524\n",
      "STEP: 487900 - loss: 0.010940663965862168\n",
      "STEP: 488000 - loss: 0.010938976605388711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 488100 - loss: 0.010937289751271489\n",
      "STEP: 488200 - loss: 0.01093560340335608\n",
      "STEP: 488300 - loss: 0.010933917561488602\n",
      "STEP: 488400 - loss: 0.010932232225515107\n",
      "STEP: 488500 - loss: 0.010930547395281784\n",
      "STEP: 488600 - loss: 0.010928863070634825\n",
      "STEP: 488700 - loss: 0.010927179251420106\n",
      "STEP: 488800 - loss: 0.0109254959374844\n",
      "STEP: 488900 - loss: 0.010923813128673913\n",
      "STEP: 489000 - loss: 0.010922130824834743\n",
      "STEP: 489100 - loss: 0.010920449025813373\n",
      "STEP: 489200 - loss: 0.010918767731456393\n",
      "STEP: 489300 - loss: 0.010917086941610171\n",
      "STEP: 489400 - loss: 0.010915406656121307\n",
      "STEP: 489500 - loss: 0.010913726874836344\n",
      "STEP: 489600 - loss: 0.010912047597602116\n",
      "STEP: 489700 - loss: 0.010910368824264707\n",
      "STEP: 489800 - loss: 0.010908690554671759\n",
      "STEP: 489900 - loss: 0.010907012788669448\n",
      "STEP: 490000 - loss: 0.010905335526104613\n",
      "STEP: 490100 - loss: 0.010903658766824151\n",
      "STEP: 490200 - loss: 0.010901982510675343\n",
      "STEP: 490300 - loss: 0.010900306757504615\n",
      "STEP: 490400 - loss: 0.0108986315071593\n",
      "STEP: 490500 - loss: 0.010896956759486437\n",
      "STEP: 490600 - loss: 0.0108952825143331\n",
      "STEP: 490700 - loss: 0.010893608771546295\n",
      "STEP: 490800 - loss: 0.010891935530973545\n",
      "STEP: 490900 - loss: 0.010890262792461795\n",
      "STEP: 491000 - loss: 0.010888590555858448\n",
      "STEP: 491100 - loss: 0.010886918821010711\n",
      "STEP: 491200 - loss: 0.010885247587766237\n",
      "STEP: 491300 - loss: 0.010883576855971992\n",
      "STEP: 491400 - loss: 0.01088190662547591\n",
      "STEP: 491500 - loss: 0.010880236896125474\n",
      "STEP: 491600 - loss: 0.01087856766776783\n",
      "STEP: 491700 - loss: 0.010876898940251203\n",
      "STEP: 491800 - loss: 0.010875230713422765\n",
      "STEP: 491900 - loss: 0.010873562987130433\n",
      "STEP: 492000 - loss: 0.010871895761221769\n",
      "STEP: 492100 - loss: 0.01087022903554501\n",
      "STEP: 492200 - loss: 0.010868562809947625\n",
      "STEP: 492300 - loss: 0.01086689708427756\n",
      "STEP: 492400 - loss: 0.010865231858382786\n",
      "STEP: 492500 - loss: 0.010863567132111174\n",
      "STEP: 492600 - loss: 0.010861902905310942\n",
      "STEP: 492700 - loss: 0.0108602391778301\n",
      "STEP: 492800 - loss: 0.010858575949516861\n",
      "STEP: 492900 - loss: 0.010856913220219146\n",
      "STEP: 493000 - loss: 0.0108552509897854\n",
      "STEP: 493100 - loss: 0.010853589258063668\n",
      "STEP: 493200 - loss: 0.010851928024902601\n",
      "STEP: 493300 - loss: 0.010850267290150265\n",
      "STEP: 493400 - loss: 0.0108486070536553\n",
      "STEP: 493500 - loss: 0.010846947315265948\n",
      "STEP: 493600 - loss: 0.010845288074830617\n",
      "STEP: 493700 - loss: 0.010843629332197997\n",
      "STEP: 493800 - loss: 0.010841971087216584\n",
      "STEP: 493900 - loss: 0.0108403133397352\n",
      "STEP: 494000 - loss: 0.01083865608960241\n",
      "STEP: 494100 - loss: 0.010836999336666719\n",
      "STEP: 494200 - loss: 0.010835343080777427\n",
      "STEP: 494300 - loss: 0.01083368732178284\n",
      "STEP: 494400 - loss: 0.010832032059531954\n",
      "STEP: 494500 - loss: 0.010830377293873733\n",
      "STEP: 494600 - loss: 0.010828723024657271\n",
      "STEP: 494700 - loss: 0.010827069251731374\n",
      "STEP: 494800 - loss: 0.010825415974945109\n",
      "STEP: 494900 - loss: 0.010823763194147367\n",
      "STEP: 495000 - loss: 0.010822110909187607\n",
      "STEP: 495100 - loss: 0.010820459119915048\n",
      "STEP: 495200 - loss: 0.010818807826178646\n",
      "STEP: 495300 - loss: 0.010817157027827718\n",
      "STEP: 495400 - loss: 0.010815506724711928\n",
      "STEP: 495500 - loss: 0.010813856916680185\n",
      "STEP: 495600 - loss: 0.010812207603582194\n",
      "STEP: 495700 - loss: 0.010810558785267342\n",
      "STEP: 495800 - loss: 0.010808910461585122\n",
      "STEP: 495900 - loss: 0.010807262632384936\n",
      "STEP: 496000 - loss: 0.010805615297516589\n",
      "STEP: 496100 - loss: 0.01080396845682962\n",
      "STEP: 496200 - loss: 0.010802322110173726\n",
      "STEP: 496300 - loss: 0.010800676257398705\n",
      "STEP: 496400 - loss: 0.010799030898354298\n",
      "STEP: 496500 - loss: 0.010797386032890197\n",
      "STEP: 496600 - loss: 0.010795741660856471\n",
      "STEP: 496700 - loss: 0.010794097782102894\n",
      "STEP: 496800 - loss: 0.010792454396479495\n",
      "STEP: 496900 - loss: 0.010790811503836211\n",
      "STEP: 497000 - loss: 0.010789169104023225\n",
      "STEP: 497100 - loss: 0.010787527196890486\n",
      "STEP: 497200 - loss: 0.010785885782288283\n",
      "STEP: 497300 - loss: 0.010784244860066648\n",
      "STEP: 497400 - loss: 0.010782604430075931\n",
      "STEP: 497500 - loss: 0.010780964492166496\n",
      "STEP: 497600 - loss: 0.010779325046188417\n",
      "STEP: 497700 - loss: 0.010777686091992221\n",
      "STEP: 497800 - loss: 0.010776047629428206\n",
      "STEP: 497900 - loss: 0.010774409658346917\n",
      "STEP: 498000 - loss: 0.010772772178599264\n",
      "STEP: 498100 - loss: 0.010771135190034946\n",
      "STEP: 498200 - loss: 0.010769498692505078\n",
      "STEP: 498300 - loss: 0.010767862685860363\n",
      "STEP: 498400 - loss: 0.010766227169951403\n",
      "STEP: 498500 - loss: 0.010764592144628842\n",
      "STEP: 498600 - loss: 0.01076295760974367\n",
      "STEP: 498700 - loss: 0.010761323565146171\n",
      "STEP: 498800 - loss: 0.010759690010687937\n",
      "STEP: 498900 - loss: 0.01075805694621954\n",
      "STEP: 499000 - loss: 0.010756424371591859\n",
      "STEP: 499100 - loss: 0.010754792286656064\n",
      "STEP: 499200 - loss: 0.010753160691263199\n",
      "STEP: 499300 - loss: 0.010751529585264085\n",
      "STEP: 499400 - loss: 0.01074989896851011\n",
      "STEP: 499500 - loss: 0.010748268840852589\n",
      "STEP: 499600 - loss: 0.010746639202142515\n",
      "STEP: 499700 - loss: 0.010745010052231036\n",
      "STEP: 499800 - loss: 0.010743381390969978\n",
      "STEP: 499900 - loss: 0.010741753218210538\n",
      "STEP: 500000 - loss: 0.010740125533803872\n",
      "STEP: 500100 - loss: 0.010738498337601403\n",
      "STEP: 500200 - loss: 0.010736871629455007\n",
      "STEP: 500300 - loss: 0.010735245409215959\n",
      "STEP: 500400 - loss: 0.010733619676736113\n",
      "STEP: 500500 - loss: 0.01073199443186663\n",
      "STEP: 500600 - loss: 0.010730369674459698\n",
      "STEP: 500700 - loss: 0.010728745404366804\n",
      "STEP: 500800 - loss: 0.010727121621439713\n",
      "STEP: 500900 - loss: 0.010725498325530323\n",
      "STEP: 501000 - loss: 0.010723875516490542\n",
      "STEP: 501100 - loss: 0.010722253194172167\n",
      "STEP: 501200 - loss: 0.010720631358427185\n",
      "STEP: 501300 - loss: 0.010719010009107462\n",
      "STEP: 501400 - loss: 0.01071738914606538\n",
      "STEP: 501500 - loss: 0.010715768769152898\n",
      "STEP: 501600 - loss: 0.010714148878222092\n",
      "STEP: 501700 - loss: 0.010712529473124982\n",
      "STEP: 501800 - loss: 0.010710910553713919\n",
      "STEP: 501900 - loss: 0.010709292119841417\n",
      "STEP: 502000 - loss: 0.010707674171359418\n",
      "STEP: 502100 - loss: 0.010706056708120519\n",
      "STEP: 502200 - loss: 0.010704439729977137\n",
      "STEP: 502300 - loss: 0.010702823236781542\n",
      "STEP: 502400 - loss: 0.010701207228386505\n",
      "STEP: 502500 - loss: 0.010699591704644247\n",
      "STEP: 502600 - loss: 0.010697976665407316\n",
      "STEP: 502700 - loss: 0.010696362110528688\n",
      "STEP: 502800 - loss: 0.01069474803986099\n",
      "STEP: 502900 - loss: 0.010693134453256679\n",
      "STEP: 503000 - loss: 0.010691521350568776\n",
      "STEP: 503100 - loss: 0.010689908731650082\n",
      "STEP: 503200 - loss: 0.01068829659635313\n",
      "STEP: 503300 - loss: 0.010686684944531127\n",
      "STEP: 503400 - loss: 0.010685073776036755\n",
      "STEP: 503500 - loss: 0.010683463090723388\n",
      "STEP: 503600 - loss: 0.01068185288844373\n",
      "STEP: 503700 - loss: 0.010680243169051067\n",
      "STEP: 503800 - loss: 0.010678633932398386\n",
      "STEP: 503900 - loss: 0.010677025178338788\n",
      "STEP: 504000 - loss: 0.010675416906725835\n",
      "STEP: 504100 - loss: 0.010673809117412215\n",
      "STEP: 504200 - loss: 0.010672201810251917\n",
      "STEP: 504300 - loss: 0.010670594985097632\n",
      "STEP: 504400 - loss: 0.010668988641803115\n",
      "STEP: 504500 - loss: 0.010667382780221807\n",
      "STEP: 504600 - loss: 0.010665777400207152\n",
      "STEP: 504700 - loss: 0.010664172501612399\n",
      "STEP: 504800 - loss: 0.01066256808429156\n",
      "STEP: 504900 - loss: 0.01066096414809816\n",
      "STEP: 505000 - loss: 0.010659360692885433\n",
      "STEP: 505100 - loss: 0.01065775771850735\n",
      "STEP: 505200 - loss: 0.010656155224817877\n",
      "STEP: 505300 - loss: 0.010654553211670605\n",
      "STEP: 505400 - loss: 0.01065295167891939\n",
      "STEP: 505500 - loss: 0.010651350626418\n",
      "STEP: 505600 - loss: 0.010649750054020444\n",
      "STEP: 505700 - loss: 0.010648149961580828\n",
      "STEP: 505800 - loss: 0.010646550348953112\n",
      "STEP: 505900 - loss: 0.010644951215991145\n",
      "STEP: 506000 - loss: 0.010643352562549125\n",
      "STEP: 506100 - loss: 0.010641754388481499\n",
      "STEP: 506200 - loss: 0.010640156693641958\n",
      "STEP: 506300 - loss: 0.010638559477885036\n",
      "STEP: 506400 - loss: 0.010636962741064983\n",
      "STEP: 506500 - loss: 0.010635366483036023\n",
      "STEP: 506600 - loss: 0.010633770703652783\n",
      "STEP: 506700 - loss: 0.010632175402769428\n",
      "STEP: 506800 - loss: 0.010630580580240419\n",
      "STEP: 506900 - loss: 0.010628986235920441\n",
      "STEP: 507000 - loss: 0.010627392369663938\n",
      "STEP: 507100 - loss: 0.010625798981325232\n",
      "STEP: 507200 - loss: 0.010624206070759368\n",
      "STEP: 507300 - loss: 0.010622613637820872\n",
      "STEP: 507400 - loss: 0.010621021682364415\n",
      "STEP: 507500 - loss: 0.010619430204244721\n",
      "STEP: 507600 - loss: 0.01061783920331688\n",
      "STEP: 507700 - loss: 0.010616248679435536\n",
      "STEP: 507800 - loss: 0.010614658632455434\n",
      "STEP: 507900 - loss: 0.010613069062231917\n",
      "STEP: 508000 - loss: 0.010611479968619632\n",
      "STEP: 508100 - loss: 0.010609891351473617\n",
      "STEP: 508200 - loss: 0.010608303210649152\n",
      "STEP: 508300 - loss: 0.01060671554600132\n",
      "STEP: 508400 - loss: 0.010605128357385155\n",
      "STEP: 508500 - loss: 0.010603541644656008\n",
      "STEP: 508600 - loss: 0.01060195540766873\n",
      "STEP: 508700 - loss: 0.010600369646279448\n",
      "STEP: 508800 - loss: 0.010598784360342737\n",
      "STEP: 508900 - loss: 0.010597199549714015\n",
      "STEP: 509000 - loss: 0.010595615214249458\n",
      "STEP: 509100 - loss: 0.01059403135380363\n",
      "STEP: 509200 - loss: 0.010592447968232502\n",
      "STEP: 509300 - loss: 0.01059086505739158\n",
      "STEP: 509400 - loss: 0.010589282621136497\n",
      "STEP: 509500 - loss: 0.010587700659322822\n",
      "STEP: 509600 - loss: 0.010586119171806223\n",
      "STEP: 509700 - loss: 0.010584538158442668\n",
      "STEP: 509800 - loss: 0.01058295761908767\n",
      "STEP: 509900 - loss: 0.010581377553596968\n",
      "STEP: 510000 - loss: 0.010579797961826783\n",
      "STEP: 510100 - loss: 0.010578218843632843\n",
      "STEP: 510200 - loss: 0.010576640198871164\n",
      "STEP: 510300 - loss: 0.010575062027397561\n",
      "STEP: 510400 - loss: 0.010573484329068242\n",
      "STEP: 510500 - loss: 0.010571907103739212\n",
      "STEP: 510600 - loss: 0.010570330351266753\n",
      "STEP: 510700 - loss: 0.010568754071506863\n",
      "STEP: 510800 - loss: 0.01056717826431585\n",
      "STEP: 510900 - loss: 0.010565602929549955\n",
      "STEP: 511000 - loss: 0.01056402806706552\n",
      "STEP: 511100 - loss: 0.010562453676718741\n",
      "STEP: 511200 - loss: 0.010560879758366285\n",
      "STEP: 511300 - loss: 0.010559306311864521\n",
      "STEP: 511400 - loss: 0.01055773333706975\n",
      "STEP: 511500 - loss: 0.010556160833838287\n",
      "STEP: 511600 - loss: 0.010554588802027342\n",
      "STEP: 511700 - loss: 0.01055301724149291\n",
      "STEP: 511800 - loss: 0.010551446152092112\n",
      "STEP: 511900 - loss: 0.010549875533681382\n",
      "STEP: 512000 - loss: 0.010548305386117604\n",
      "STEP: 512100 - loss: 0.01054673570925748\n",
      "STEP: 512200 - loss: 0.010545166502957675\n",
      "STEP: 512300 - loss: 0.010543597767075377\n",
      "STEP: 512400 - loss: 0.010542029501467359\n",
      "STEP: 512500 - loss: 0.0105404617059905\n",
      "STEP: 512600 - loss: 0.010538894380501927\n",
      "STEP: 512700 - loss: 0.010537327524858654\n",
      "STEP: 512800 - loss: 0.010535761138917693\n",
      "STEP: 512900 - loss: 0.010534195222536393\n",
      "STEP: 513000 - loss: 0.010532629775571805\n",
      "STEP: 513100 - loss: 0.01053106479788089\n",
      "STEP: 513200 - loss: 0.010529500289321224\n",
      "STEP: 513300 - loss: 0.010527936249750167\n",
      "STEP: 513400 - loss: 0.010526372679024901\n",
      "STEP: 513500 - loss: 0.01052480957700273\n",
      "STEP: 513600 - loss: 0.010523246943541325\n",
      "STEP: 513700 - loss: 0.0105216847784981\n",
      "STEP: 513800 - loss: 0.010520123081730563\n",
      "STEP: 513900 - loss: 0.010518561853096036\n",
      "STEP: 514000 - loss: 0.010517001092452582\n",
      "STEP: 514100 - loss: 0.010515440799657617\n",
      "STEP: 514200 - loss: 0.010513880974568793\n",
      "STEP: 514300 - loss: 0.010512321617043846\n",
      "STEP: 514400 - loss: 0.010510762726940884\n",
      "STEP: 514500 - loss: 0.010509204304117174\n",
      "STEP: 514600 - loss: 0.010507646348430875\n",
      "STEP: 514700 - loss: 0.010506088859740189\n",
      "STEP: 514800 - loss: 0.010504531837902582\n",
      "STEP: 514900 - loss: 0.01050297528277638\n",
      "STEP: 515000 - loss: 0.010501419194219373\n",
      "STEP: 515100 - loss: 0.010499863572090021\n",
      "STEP: 515200 - loss: 0.010498308416246034\n",
      "STEP: 515300 - loss: 0.010496753726545807\n",
      "STEP: 515400 - loss: 0.01049519950284744\n",
      "STEP: 515500 - loss: 0.010493645745009574\n",
      "STEP: 515600 - loss: 0.010492092452889961\n",
      "STEP: 515700 - loss: 0.010490539626347146\n",
      "STEP: 515800 - loss: 0.010488987265239726\n",
      "STEP: 515900 - loss: 0.0104874353694259\n",
      "STEP: 516000 - loss: 0.010485883938764333\n",
      "STEP: 516100 - loss: 0.010484332973113252\n",
      "STEP: 516200 - loss: 0.0104827824723316\n",
      "STEP: 516300 - loss: 0.010481232436277538\n",
      "STEP: 516400 - loss: 0.010479682864810214\n",
      "STEP: 516500 - loss: 0.01047813375778792\n",
      "STEP: 516600 - loss: 0.010476585115069386\n",
      "STEP: 516700 - loss: 0.010475036936513719\n",
      "STEP: 516800 - loss: 0.010473489221979628\n",
      "STEP: 516900 - loss: 0.010471941971325768\n",
      "STEP: 517000 - loss: 0.010470395184411015\n",
      "STEP: 517100 - loss: 0.010468848861094768\n",
      "STEP: 517200 - loss: 0.01046730300123547\n",
      "STEP: 517300 - loss: 0.010465757604692517\n",
      "STEP: 517400 - loss: 0.010464212671324816\n",
      "STEP: 517500 - loss: 0.010462668200991596\n",
      "STEP: 517600 - loss: 0.010461124193551994\n",
      "STEP: 517700 - loss: 0.010459580648865149\n",
      "STEP: 517800 - loss: 0.010458037566790411\n",
      "STEP: 517900 - loss: 0.010456494947186996\n",
      "STEP: 518000 - loss: 0.01045495278991412\n",
      "STEP: 518100 - loss: 0.010453411094831363\n",
      "STEP: 518200 - loss: 0.010451869861798111\n",
      "STEP: 518300 - loss: 0.010450329090673779\n",
      "STEP: 518400 - loss: 0.010448788781317851\n",
      "STEP: 518500 - loss: 0.010447248933589871\n",
      "STEP: 518600 - loss: 0.010445709547349481\n",
      "STEP: 518700 - loss: 0.010444170622456322\n",
      "STEP: 518800 - loss: 0.010442632158770037\n",
      "STEP: 518900 - loss: 0.01044109415615038\n",
      "STEP: 519000 - loss: 0.010439556614457075\n",
      "STEP: 519100 - loss: 0.01043801953354993\n",
      "STEP: 519200 - loss: 0.010436482913288497\n",
      "STEP: 519300 - loss: 0.01043494675353341\n",
      "STEP: 519400 - loss: 0.01043341105414396\n",
      "STEP: 519500 - loss: 0.010431875814980188\n",
      "STEP: 519600 - loss: 0.010430341035902232\n",
      "STEP: 519700 - loss: 0.010428806716770319\n",
      "STEP: 519800 - loss: 0.010427272857444572\n",
      "STEP: 519900 - loss: 0.0104257394577845\n",
      "STEP: 520000 - loss: 0.010424206517651049\n",
      "STEP: 520100 - loss: 0.01042267403690392\n",
      "STEP: 520200 - loss: 0.010421142015403732\n",
      "STEP: 520300 - loss: 0.010419610453010649\n",
      "STEP: 520400 - loss: 0.010418079349584965\n",
      "STEP: 520500 - loss: 0.010416548704987316\n",
      "STEP: 520600 - loss: 0.010415018519078044\n",
      "STEP: 520700 - loss: 0.01041348879171724\n",
      "STEP: 520800 - loss: 0.010411959522766117\n",
      "STEP: 520900 - loss: 0.010410430712084761\n",
      "STEP: 521000 - loss: 0.010408902359533878\n",
      "STEP: 521100 - loss: 0.010407374464974212\n",
      "STEP: 521200 - loss: 0.010405847028266461\n",
      "STEP: 521300 - loss: 0.010404320049271214\n",
      "STEP: 521400 - loss: 0.010402793527849294\n",
      "STEP: 521500 - loss: 0.01040126746386178\n",
      "STEP: 521600 - loss: 0.010399741857169405\n",
      "STEP: 521700 - loss: 0.010398216707632763\n",
      "STEP: 521800 - loss: 0.010396692015113175\n",
      "STEP: 521900 - loss: 0.010395167779471543\n",
      "STEP: 522000 - loss: 0.010393644000568726\n",
      "STEP: 522100 - loss: 0.010392120678266125\n",
      "STEP: 522200 - loss: 0.010390597812424601\n",
      "STEP: 522300 - loss: 0.010389075402905436\n",
      "STEP: 522400 - loss: 0.01038755344956974\n",
      "STEP: 522500 - loss: 0.01038603195227906\n",
      "STEP: 522600 - loss: 0.010384510910894162\n",
      "STEP: 522700 - loss: 0.010382990325276785\n",
      "STEP: 522800 - loss: 0.010381470195288196\n",
      "STEP: 522900 - loss: 0.010379950520789726\n",
      "STEP: 523000 - loss: 0.010378431301642888\n",
      "STEP: 523100 - loss: 0.010376912537709239\n",
      "STEP: 523200 - loss: 0.010375394228850213\n",
      "STEP: 523300 - loss: 0.01037387637492738\n",
      "STEP: 523400 - loss: 0.010372358975802645\n",
      "STEP: 523500 - loss: 0.0103708420313374\n",
      "STEP: 523600 - loss: 0.01036932554139335\n",
      "STEP: 523700 - loss: 0.010367809505832363\n",
      "STEP: 523800 - loss: 0.010366293924516252\n",
      "STEP: 523900 - loss: 0.010364778797306717\n",
      "STEP: 524000 - loss: 0.010363264124065805\n",
      "STEP: 524100 - loss: 0.010361749904655255\n",
      "STEP: 524200 - loss: 0.010360236138937275\n",
      "STEP: 524300 - loss: 0.010358722826773437\n",
      "STEP: 524400 - loss: 0.010357209968026184\n",
      "STEP: 524500 - loss: 0.010355697562557482\n",
      "STEP: 524600 - loss: 0.010354185610229579\n",
      "STEP: 524700 - loss: 0.010352674110904358\n",
      "STEP: 524800 - loss: 0.010351163064444316\n",
      "STEP: 524900 - loss: 0.010349652470711703\n",
      "STEP: 525000 - loss: 0.010348142329568509\n",
      "STEP: 525100 - loss: 0.010346632640877346\n",
      "STEP: 525200 - loss: 0.010345123404500566\n",
      "STEP: 525300 - loss: 0.010343614620300522\n",
      "STEP: 525400 - loss: 0.010342106288139854\n",
      "STEP: 525500 - loss: 0.01034059840788073\n",
      "STEP: 525600 - loss: 0.010339090979385946\n",
      "STEP: 525700 - loss: 0.010337584002518026\n",
      "STEP: 525800 - loss: 0.010336077477139717\n",
      "STEP: 525900 - loss: 0.01033457140311347\n",
      "STEP: 526000 - loss: 0.010333065780302369\n",
      "STEP: 526100 - loss: 0.010331560608568886\n",
      "STEP: 526200 - loss: 0.01033005588777549\n",
      "STEP: 526300 - loss: 0.010328551617785791\n",
      "STEP: 526400 - loss: 0.010327047798462236\n",
      "STEP: 526500 - loss: 0.010325544429667629\n",
      "STEP: 526600 - loss: 0.010324041511265206\n",
      "STEP: 526700 - loss: 0.010322539043117874\n",
      "STEP: 526800 - loss: 0.010321037025088766\n",
      "STEP: 526900 - loss: 0.010319535457040813\n",
      "STEP: 527000 - loss: 0.010318034338837016\n",
      "STEP: 527100 - loss: 0.010316533670341025\n",
      "STEP: 527200 - loss: 0.010315033451415831\n",
      "STEP: 527300 - loss: 0.010313533681924494\n",
      "STEP: 527400 - loss: 0.010312034361730567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 527500 - loss: 0.010310535490697294\n",
      "STEP: 527600 - loss: 0.010309037068687962\n",
      "STEP: 527700 - loss: 0.01030753909556623\n",
      "STEP: 527800 - loss: 0.010306041571195456\n",
      "STEP: 527900 - loss: 0.010304544495438854\n",
      "STEP: 528000 - loss: 0.010303047868160687\n",
      "STEP: 528100 - loss: 0.010301551689223792\n",
      "STEP: 528200 - loss: 0.010300055958492047\n",
      "STEP: 528300 - loss: 0.010298560675829408\n",
      "STEP: 528400 - loss: 0.010297065841099321\n",
      "STEP: 528500 - loss: 0.010295571454165648\n",
      "STEP: 528600 - loss: 0.010294077514891805\n",
      "STEP: 528700 - loss: 0.01029258402314226\n",
      "STEP: 528800 - loss: 0.010291090978780509\n",
      "STEP: 528900 - loss: 0.010289598381670595\n",
      "STEP: 529000 - loss: 0.010288106231676515\n",
      "STEP: 529100 - loss: 0.010286614528662145\n",
      "STEP: 529200 - loss: 0.010285123272491497\n",
      "STEP: 529300 - loss: 0.010283632463028795\n",
      "STEP: 529400 - loss: 0.010282142100138125\n",
      "STEP: 529500 - loss: 0.010280652183683823\n",
      "STEP: 529600 - loss: 0.010279162713529764\n",
      "STEP: 529700 - loss: 0.010277673689540374\n",
      "STEP: 529800 - loss: 0.010276185111580056\n",
      "STEP: 529900 - loss: 0.010274696979512898\n",
      "STEP: 530000 - loss: 0.010273209293203607\n",
      "STEP: 530100 - loss: 0.010271722052516263\n",
      "STEP: 530200 - loss: 0.010270235257315616\n",
      "STEP: 530300 - loss: 0.010268748907465836\n",
      "STEP: 530400 - loss: 0.010267263002831721\n",
      "STEP: 530500 - loss: 0.01026577754327796\n",
      "STEP: 530600 - loss: 0.010264292528668829\n",
      "STEP: 530700 - loss: 0.010262807958869144\n",
      "STEP: 530800 - loss: 0.01026132383374372\n",
      "STEP: 530900 - loss: 0.010259840153157172\n",
      "STEP: 531000 - loss: 0.010258356916974607\n",
      "STEP: 531100 - loss: 0.010256874125060222\n",
      "STEP: 531200 - loss: 0.01025539177727949\n",
      "STEP: 531300 - loss: 0.010253909873496978\n",
      "STEP: 531400 - loss: 0.010252428413577817\n",
      "STEP: 531500 - loss: 0.010250947397386937\n",
      "STEP: 531600 - loss: 0.010249466824789364\n",
      "STEP: 531700 - loss: 0.010247986695650236\n",
      "STEP: 531800 - loss: 0.010246507009834375\n",
      "STEP: 531900 - loss: 0.010245027767207522\n",
      "STEP: 532000 - loss: 0.010243548967634403\n",
      "STEP: 532100 - loss: 0.010242070610980475\n",
      "STEP: 532200 - loss: 0.010240592697110694\n",
      "STEP: 532300 - loss: 0.010239115225890901\n",
      "STEP: 532400 - loss: 0.01023763819718598\n",
      "STEP: 532500 - loss: 0.010236161610861817\n",
      "STEP: 532600 - loss: 0.0102346854667832\n",
      "STEP: 532700 - loss: 0.01023320976481626\n",
      "STEP: 532800 - loss: 0.0102317345048264\n",
      "STEP: 532900 - loss: 0.01023025968667878\n",
      "STEP: 533000 - loss: 0.010228785310239467\n",
      "STEP: 533100 - loss: 0.010227311375373955\n",
      "STEP: 533200 - loss: 0.010225837881947673\n",
      "STEP: 533300 - loss: 0.010224364829826846\n",
      "STEP: 533400 - loss: 0.01022289221887684\n",
      "STEP: 533500 - loss: 0.010221420048963646\n",
      "STEP: 533600 - loss: 0.010219948319952977\n",
      "STEP: 533700 - loss: 0.010218477031711212\n",
      "STEP: 533800 - loss: 0.010217006184103723\n",
      "STEP: 533900 - loss: 0.010215535776996663\n",
      "STEP: 534000 - loss: 0.010214065810256046\n",
      "STEP: 534100 - loss: 0.010212596283748087\n",
      "STEP: 534200 - loss: 0.010211127197338533\n",
      "STEP: 534300 - loss: 0.010209658550893903\n",
      "STEP: 534400 - loss: 0.010208190344280221\n",
      "STEP: 534500 - loss: 0.010206722577363594\n",
      "STEP: 534600 - loss: 0.010205255250010526\n",
      "STEP: 534700 - loss: 0.010203788362087145\n",
      "STEP: 534800 - loss: 0.01020232191345995\n",
      "STEP: 534900 - loss: 0.010200855903995118\n",
      "STEP: 535000 - loss: 0.010199390333559133\n",
      "STEP: 535100 - loss: 0.01019792520201873\n",
      "STEP: 535200 - loss: 0.010196460509239978\n",
      "STEP: 535300 - loss: 0.010194996255089682\n",
      "STEP: 535400 - loss: 0.010193532439434416\n",
      "STEP: 535500 - loss: 0.010192069062140745\n",
      "STEP: 535600 - loss: 0.010190606123075414\n",
      "STEP: 535700 - loss: 0.010189143622105039\n",
      "STEP: 535800 - loss: 0.010187681559096346\n",
      "STEP: 535900 - loss: 0.01018621993391629\n",
      "STEP: 536000 - loss: 0.010184758746431573\n",
      "STEP: 536100 - loss: 0.010183297996509135\n",
      "STEP: 536200 - loss: 0.010181837684015935\n",
      "STEP: 536300 - loss: 0.01018037780881876\n",
      "STEP: 536400 - loss: 0.01017891837078461\n",
      "STEP: 536500 - loss: 0.010177459369780581\n",
      "STEP: 536600 - loss: 0.010176000805673891\n",
      "STEP: 536700 - loss: 0.010174542678331575\n",
      "STEP: 536800 - loss: 0.010173084987620689\n",
      "STEP: 536900 - loss: 0.010171627733408291\n",
      "STEP: 537000 - loss: 0.010170170915562058\n",
      "STEP: 537100 - loss: 0.010168714533948844\n",
      "STEP: 537200 - loss: 0.010167258588436224\n",
      "STEP: 537300 - loss: 0.010165803078891289\n",
      "STEP: 537400 - loss: 0.010164348005181857\n",
      "STEP: 537500 - loss: 0.010162893367174897\n",
      "STEP: 537600 - loss: 0.01016143916473823\n",
      "STEP: 537700 - loss: 0.010159985397739216\n",
      "STEP: 537800 - loss: 0.010158532066045407\n",
      "STEP: 537900 - loss: 0.010157079169524562\n",
      "STEP: 538000 - loss: 0.010155626708044135\n",
      "STEP: 538100 - loss: 0.01015417468147193\n",
      "STEP: 538200 - loss: 0.010152723089675417\n",
      "STEP: 538300 - loss: 0.010151271932522772\n",
      "STEP: 538400 - loss: 0.010149821209881501\n",
      "STEP: 538500 - loss: 0.010148370921619503\n",
      "STEP: 538600 - loss: 0.010146921067604616\n",
      "STEP: 538700 - loss: 0.010145471647705039\n",
      "STEP: 538800 - loss: 0.010144022661788207\n",
      "STEP: 538900 - loss: 0.01014257410972257\n",
      "STEP: 539000 - loss: 0.010141125991375843\n",
      "STEP: 539100 - loss: 0.010139678306616436\n",
      "STEP: 539200 - loss: 0.010138231055312268\n",
      "STEP: 539300 - loss: 0.010136784237331645\n",
      "STEP: 539400 - loss: 0.010135337852542562\n",
      "STEP: 539500 - loss: 0.010133891900813522\n",
      "STEP: 539600 - loss: 0.010132446382012408\n",
      "STEP: 539700 - loss: 0.010131001296007941\n",
      "STEP: 539800 - loss: 0.010129556642668297\n",
      "STEP: 539900 - loss: 0.010128112421862017\n",
      "STEP: 540000 - loss: 0.010126668633457304\n",
      "STEP: 540100 - loss: 0.010125225277322892\n",
      "STEP: 540200 - loss: 0.010123782353327072\n",
      "STEP: 540300 - loss: 0.010122339861338698\n",
      "STEP: 540400 - loss: 0.010120897801226051\n",
      "STEP: 540500 - loss: 0.010119456172857876\n",
      "STEP: 540600 - loss: 0.010118014976102992\n",
      "STEP: 540700 - loss: 0.010116574210829987\n",
      "STEP: 540800 - loss: 0.010115133876907614\n",
      "STEP: 540900 - loss: 0.010113693974204748\n",
      "STEP: 541000 - loss: 0.010112254502590281\n",
      "STEP: 541100 - loss: 0.01011081546193289\n",
      "STEP: 541200 - loss: 0.010109376852101637\n",
      "STEP: 541300 - loss: 0.01010793867296552\n",
      "STEP: 541400 - loss: 0.010106500924393445\n",
      "STEP: 541500 - loss: 0.010105063606254556\n",
      "STEP: 541600 - loss: 0.01010362671841777\n",
      "STEP: 541700 - loss: 0.010102190260752524\n",
      "STEP: 541800 - loss: 0.010100754233127625\n",
      "STEP: 541900 - loss: 0.01009931863541232\n",
      "STEP: 542000 - loss: 0.010097883467475992\n",
      "STEP: 542100 - loss: 0.010096448729187985\n",
      "STEP: 542200 - loss: 0.010095014420417454\n",
      "STEP: 542300 - loss: 0.010093580541033894\n",
      "STEP: 542400 - loss: 0.010092147090906391\n",
      "STEP: 542500 - loss: 0.010090714069904845\n",
      "STEP: 542600 - loss: 0.010089281477898292\n",
      "STEP: 542700 - loss: 0.010087849314756677\n",
      "STEP: 542800 - loss: 0.010086417580349224\n",
      "STEP: 542900 - loss: 0.010084986274545689\n",
      "STEP: 543000 - loss: 0.010083555397215579\n",
      "STEP: 543100 - loss: 0.010082124948228863\n",
      "STEP: 543200 - loss: 0.010080694927454876\n",
      "STEP: 543300 - loss: 0.010079265334763624\n",
      "STEP: 543400 - loss: 0.010077836170024796\n",
      "STEP: 543500 - loss: 0.010076407433108395\n",
      "STEP: 543600 - loss: 0.010074979123883917\n",
      "STEP: 543700 - loss: 0.010073551242221836\n",
      "STEP: 543800 - loss: 0.010072123787991553\n",
      "STEP: 543900 - loss: 0.010070696761063492\n",
      "STEP: 544000 - loss: 0.010069270161307141\n",
      "STEP: 544100 - loss: 0.010067843988593204\n",
      "STEP: 544200 - loss: 0.010066418242791619\n",
      "STEP: 544300 - loss: 0.010064992923772342\n",
      "STEP: 544400 - loss: 0.010063568031405583\n",
      "STEP: 544500 - loss: 0.010062143565561862\n",
      "STEP: 544600 - loss: 0.010060719526111062\n",
      "STEP: 544700 - loss: 0.010059295912923759\n",
      "STEP: 544800 - loss: 0.010057872725870125\n",
      "STEP: 544900 - loss: 0.010056449964820541\n",
      "STEP: 545000 - loss: 0.01005502762964579\n",
      "STEP: 545100 - loss: 0.010053605720215948\n",
      "STEP: 545200 - loss: 0.010052184236401723\n",
      "STEP: 545300 - loss: 0.010050763178073545\n",
      "STEP: 545400 - loss: 0.010049342545102046\n",
      "STEP: 545500 - loss: 0.0100479223373579\n",
      "STEP: 545600 - loss: 0.01004650255471164\n",
      "STEP: 545700 - loss: 0.010045083197034173\n",
      "STEP: 545800 - loss: 0.010043664264196048\n",
      "STEP: 545900 - loss: 0.010042245756068097\n",
      "STEP: 546000 - loss: 0.010040827672521374\n",
      "STEP: 546100 - loss: 0.010039410013426366\n",
      "STEP: 546200 - loss: 0.010037992778654073\n",
      "STEP: 546300 - loss: 0.010036575968075691\n",
      "STEP: 546400 - loss: 0.010035159581562073\n",
      "STEP: 546500 - loss: 0.010033743618983878\n",
      "STEP: 546600 - loss: 0.01003232808021264\n",
      "STEP: 546700 - loss: 0.010030912965119263\n",
      "STEP: 546800 - loss: 0.010029498273574935\n",
      "STEP: 546900 - loss: 0.010028084005450837\n",
      "STEP: 547000 - loss: 0.01002667016061815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 547100 - loss: 0.010025256738948014\n",
      "STEP: 547200 - loss: 0.010023843740311882\n",
      "STEP: 547300 - loss: 0.01002243116458098\n",
      "STEP: 547400 - loss: 0.0100210190116268\n",
      "STEP: 547500 - loss: 0.010019607281320701\n",
      "STEP: 547600 - loss: 0.010018195973534122\n",
      "STEP: 547700 - loss: 0.010016785088138525\n",
      "STEP: 547800 - loss: 0.010015374625005378\n",
      "STEP: 547900 - loss: 0.010013964584006477\n",
      "STEP: 548000 - loss: 0.010012554965013219\n",
      "STEP: 548100 - loss: 0.010011145767897231\n",
      "STEP: 548200 - loss: 0.010009736992530483\n",
      "STEP: 548300 - loss: 0.01000832863878437\n",
      "STEP: 548400 - loss: 0.010006920706530632\n",
      "STEP: 548500 - loss: 0.010005513195641432\n",
      "STEP: 548600 - loss: 0.010004106105988408\n",
      "STEP: 548700 - loss: 0.01000269943744329\n",
      "STEP: 548800 - loss: 0.010001293189877985\n",
      "STEP: 548900 - loss: 0.00999988736316477\n",
      "STEP: 549000 - loss: 0.00999848195717552\n",
      "STEP: 549100 - loss: 0.009997076971782096\n",
      "STEP: 549200 - loss: 0.009995672406856906\n",
      "STEP: 549300 - loss: 0.009994268262271695\n",
      "STEP: 549400 - loss: 0.009992864537898608\n",
      "STEP: 549500 - loss: 0.009991461233610244\n",
      "STEP: 549600 - loss: 0.009990058349278482\n",
      "STEP: 549700 - loss: 0.009988655884775733\n",
      "STEP: 549800 - loss: 0.009987253839974135\n",
      "STEP: 549900 - loss: 0.009985852214746203\n",
      "STEP: 550000 - loss: 0.009984451008964336\n",
      "STEP: 550100 - loss: 0.009983050222500792\n",
      "STEP: 550200 - loss: 0.009981649855228187\n",
      "STEP: 550300 - loss: 0.009980249907019297\n",
      "STEP: 550400 - loss: 0.009978850377745976\n",
      "STEP: 550500 - loss: 0.009977451267281277\n",
      "STEP: 550600 - loss: 0.009976052575497546\n",
      "STEP: 550700 - loss: 0.009974654302267776\n",
      "STEP: 550800 - loss: 0.009973256447464485\n",
      "STEP: 550900 - loss: 0.009971859010960333\n",
      "STEP: 551000 - loss: 0.009970461992628257\n",
      "STEP: 551100 - loss: 0.00996906539234101\n",
      "STEP: 551200 - loss: 0.00996766920997125\n",
      "STEP: 551300 - loss: 0.009966273445392047\n",
      "STEP: 551400 - loss: 0.009964878098476588\n",
      "STEP: 551500 - loss: 0.009963483169097299\n",
      "STEP: 551600 - loss: 0.009962088657127572\n",
      "STEP: 551700 - loss: 0.0099606945624401\n",
      "STEP: 551800 - loss: 0.009959300884908321\n",
      "STEP: 551900 - loss: 0.009957907624405331\n",
      "STEP: 552000 - loss: 0.009956514780804077\n",
      "STEP: 552100 - loss: 0.009955122353978002\n",
      "STEP: 552200 - loss: 0.009953730343800087\n",
      "STEP: 552300 - loss: 0.009952338750143638\n",
      "STEP: 552400 - loss: 0.009950947572882125\n",
      "STEP: 552500 - loss: 0.009949556811888707\n",
      "STEP: 552600 - loss: 0.00994816646703701\n",
      "STEP: 552700 - loss: 0.009946776538200646\n",
      "STEP: 552800 - loss: 0.0099453870252524\n",
      "STEP: 552900 - loss: 0.009943997928066273\n",
      "STEP: 553000 - loss: 0.009942609246515579\n",
      "STEP: 553100 - loss: 0.009941220980474268\n",
      "STEP: 553200 - loss: 0.009939833129815457\n",
      "STEP: 553300 - loss: 0.00993844569441325\n",
      "STEP: 553400 - loss: 0.009937058674141016\n",
      "STEP: 553500 - loss: 0.009935672068872723\n",
      "STEP: 553600 - loss: 0.009934285878482026\n",
      "STEP: 553700 - loss: 0.009932900102842814\n",
      "STEP: 553800 - loss: 0.009931514741828905\n",
      "STEP: 553900 - loss: 0.009930129795313878\n",
      "STEP: 554000 - loss: 0.00992874526317241\n",
      "STEP: 554100 - loss: 0.009927361145277595\n",
      "STEP: 554200 - loss: 0.009925977441504192\n",
      "STEP: 554300 - loss: 0.009924594151725757\n",
      "STEP: 554400 - loss: 0.009923211275816687\n",
      "STEP: 554500 - loss: 0.009921828813650873\n",
      "STEP: 554600 - loss: 0.009920446765102467\n",
      "STEP: 554700 - loss: 0.009919065130045788\n",
      "STEP: 554800 - loss: 0.00991768390835497\n",
      "STEP: 554900 - loss: 0.009916303099904468\n",
      "STEP: 555000 - loss: 0.009914922704568521\n",
      "STEP: 555100 - loss: 0.009913542722221154\n",
      "STEP: 555200 - loss: 0.00991216315273724\n",
      "STEP: 555300 - loss: 0.00991078399599092\n",
      "STEP: 555400 - loss: 0.009909405251856787\n",
      "STEP: 555500 - loss: 0.009908026920209295\n",
      "STEP: 555600 - loss: 0.00990664900092295\n",
      "STEP: 555700 - loss: 0.009905271493872289\n",
      "STEP: 555800 - loss: 0.009903894398932075\n",
      "STEP: 555900 - loss: 0.009902517715976862\n",
      "STEP: 556000 - loss: 0.009901141444881426\n",
      "STEP: 556100 - loss: 0.009899765585520491\n",
      "STEP: 556200 - loss: 0.009898390137768736\n",
      "STEP: 556300 - loss: 0.009897015101500931\n",
      "STEP: 556400 - loss: 0.009895640476591989\n",
      "STEP: 556500 - loss: 0.009894266262916887\n",
      "STEP: 556600 - loss: 0.009892892460350352\n",
      "STEP: 556700 - loss: 0.009891519068767573\n",
      "STEP: 556800 - loss: 0.009890146088043408\n",
      "STEP: 556900 - loss: 0.009888773518052974\n",
      "STEP: 557000 - loss: 0.009887401358671324\n",
      "STEP: 557100 - loss: 0.009886029609773462\n",
      "STEP: 557200 - loss: 0.00988465827123444\n",
      "STEP: 557300 - loss: 0.009883287342929955\n",
      "STEP: 557400 - loss: 0.009881916824734771\n",
      "STEP: 557500 - loss: 0.009880546716524118\n",
      "STEP: 557600 - loss: 0.00987917701817371\n",
      "STEP: 557700 - loss: 0.00987780772955854\n",
      "STEP: 557800 - loss: 0.00987643885055392\n",
      "STEP: 557900 - loss: 0.009875070381035673\n",
      "STEP: 558000 - loss: 0.009873702320878804\n",
      "STEP: 558100 - loss: 0.009872334669959244\n",
      "STEP: 558200 - loss: 0.00987096742815204\n",
      "STEP: 558300 - loss: 0.009869600595333327\n",
      "STEP: 558400 - loss: 0.009868234171377952\n",
      "STEP: 558500 - loss: 0.009866868156162377\n",
      "STEP: 558600 - loss: 0.009865502549561636\n",
      "STEP: 558700 - loss: 0.009864137351451951\n",
      "STEP: 558800 - loss: 0.009862772561708677\n",
      "STEP: 558900 - loss: 0.009861408180207865\n",
      "STEP: 559000 - loss: 0.009860044206825286\n",
      "STEP: 559100 - loss: 0.009858680641436485\n",
      "STEP: 559200 - loss: 0.009857317483917927\n",
      "STEP: 559300 - loss: 0.009855954734145389\n",
      "STEP: 559400 - loss: 0.009854592391994686\n",
      "STEP: 559500 - loss: 0.009853230457341745\n",
      "STEP: 559600 - loss: 0.009851868930062766\n",
      "STEP: 559700 - loss: 0.009850507810034163\n",
      "STEP: 559800 - loss: 0.009849147097131677\n",
      "STEP: 559900 - loss: 0.00984778679123156\n",
      "STEP: 560000 - loss: 0.009846426892209955\n",
      "STEP: 560100 - loss: 0.009845067399943114\n",
      "STEP: 560200 - loss: 0.009843708314307888\n",
      "STEP: 560300 - loss: 0.009842349635179787\n",
      "STEP: 560400 - loss: 0.009840991362435327\n",
      "STEP: 560500 - loss: 0.009839633495951258\n",
      "STEP: 560600 - loss: 0.009838276035603999\n",
      "STEP: 560700 - loss: 0.009836918981269647\n",
      "STEP: 560800 - loss: 0.009835562332824909\n",
      "STEP: 560900 - loss: 0.009834206090146513\n",
      "STEP: 561000 - loss: 0.009832850253110656\n",
      "STEP: 561100 - loss: 0.00983149482159434\n",
      "STEP: 561200 - loss: 0.009830139795474086\n",
      "STEP: 561300 - loss: 0.009828785174626514\n",
      "STEP: 561400 - loss: 0.00982743095892849\n",
      "STEP: 561500 - loss: 0.009826077148256727\n",
      "STEP: 561600 - loss: 0.009824723742488023\n",
      "STEP: 561700 - loss: 0.009823370741499077\n",
      "STEP: 561800 - loss: 0.009822018145167064\n",
      "STEP: 561900 - loss: 0.009820665953368828\n",
      "STEP: 562000 - loss: 0.009819314165981174\n",
      "STEP: 562100 - loss: 0.00981796278288145\n",
      "STEP: 562200 - loss: 0.009816611803946208\n",
      "STEP: 562300 - loss: 0.009815261229052703\n",
      "STEP: 562400 - loss: 0.009813911058078214\n",
      "STEP: 562500 - loss: 0.009812561290899765\n",
      "STEP: 562600 - loss: 0.009811211927394514\n",
      "STEP: 562700 - loss: 0.009809862967439712\n",
      "STEP: 562800 - loss: 0.009808514410912595\n",
      "STEP: 562900 - loss: 0.009807166257690495\n",
      "STEP: 563000 - loss: 0.009805818507650964\n",
      "STEP: 563100 - loss: 0.009804471160670885\n",
      "STEP: 563200 - loss: 0.009803124216627997\n",
      "STEP: 563300 - loss: 0.009801777675399696\n",
      "STEP: 563400 - loss: 0.009800431536863512\n",
      "STEP: 563500 - loss: 0.009799085800896851\n",
      "STEP: 563600 - loss: 0.009797740467377357\n",
      "STEP: 563700 - loss: 0.009796395536182664\n",
      "STEP: 563800 - loss: 0.009795051007190076\n",
      "STEP: 563900 - loss: 0.00979370688027772\n",
      "STEP: 564000 - loss: 0.009792363155322992\n",
      "STEP: 564100 - loss: 0.009791019832203873\n",
      "STEP: 564200 - loss: 0.00978967691079804\n",
      "STEP: 564300 - loss: 0.009788334390983268\n",
      "STEP: 564400 - loss: 0.00978699227263714\n",
      "STEP: 564500 - loss: 0.00978565055563818\n",
      "STEP: 564600 - loss: 0.009784309239864047\n",
      "STEP: 564700 - loss: 0.009782968325192362\n",
      "STEP: 564800 - loss: 0.009781627811501565\n",
      "STEP: 564900 - loss: 0.009780287698669574\n",
      "STEP: 565000 - loss: 0.009778947986574379\n",
      "STEP: 565100 - loss: 0.00977760867509422\n",
      "STEP: 565200 - loss: 0.00977626976410704\n",
      "STEP: 565300 - loss: 0.00977493125349136\n",
      "STEP: 565400 - loss: 0.009773593143125156\n",
      "STEP: 565500 - loss: 0.00977225543288681\n",
      "STEP: 565600 - loss: 0.009770918122654534\n",
      "STEP: 565700 - loss: 0.009769581212306661\n",
      "STEP: 565800 - loss: 0.009768244701721496\n",
      "STEP: 565900 - loss: 0.009766908590777753\n",
      "STEP: 566000 - loss: 0.009765572879353462\n",
      "STEP: 566100 - loss: 0.009764237567327479\n",
      "STEP: 566200 - loss: 0.009762902654578298\n",
      "STEP: 566300 - loss: 0.009761568140984275\n",
      "STEP: 566400 - loss: 0.009760234026424019\n",
      "STEP: 566500 - loss: 0.009758900310776348\n",
      "STEP: 566600 - loss: 0.009757566993919744\n",
      "STEP: 566700 - loss: 0.009756234075732917\n",
      "STEP: 566800 - loss: 0.009754901556094802\n",
      "STEP: 566900 - loss: 0.009753569434884146\n",
      "STEP: 567000 - loss: 0.00975223771197967\n",
      "STEP: 567100 - loss: 0.009750906387260129\n",
      "STEP: 567200 - loss: 0.009749575460604676\n",
      "STEP: 567300 - loss: 0.009748244931892159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 567400 - loss: 0.00974691480100121\n",
      "STEP: 567500 - loss: 0.009745585067811324\n",
      "STEP: 567600 - loss: 0.009744255732201302\n",
      "STEP: 567700 - loss: 0.009742926794050245\n",
      "STEP: 567800 - loss: 0.009741598253237182\n",
      "STEP: 567900 - loss: 0.009740270109641335\n",
      "STEP: 568000 - loss: 0.009738942363141916\n",
      "STEP: 568100 - loss: 0.009737615013618157\n",
      "STEP: 568200 - loss: 0.009736288060949238\n",
      "STEP: 568300 - loss: 0.009734961505014713\n",
      "STEP: 568400 - loss: 0.009733635345693575\n",
      "STEP: 568500 - loss: 0.009732309582865289\n",
      "STEP: 568600 - loss: 0.009730984216409439\n",
      "STEP: 568700 - loss: 0.009729659246205228\n",
      "STEP: 568800 - loss: 0.009728334672132079\n",
      "STEP: 568900 - loss: 0.009727010494070019\n",
      "STEP: 569000 - loss: 0.009725686711897918\n",
      "STEP: 569100 - loss: 0.009724363325495991\n",
      "STEP: 569200 - loss: 0.009723040334743334\n",
      "STEP: 569300 - loss: 0.009721717739520035\n",
      "STEP: 569400 - loss: 0.009720395539705487\n",
      "STEP: 569500 - loss: 0.009719073735179488\n",
      "STEP: 569600 - loss: 0.009717752325822202\n",
      "STEP: 569700 - loss: 0.009716431311512978\n",
      "STEP: 569800 - loss: 0.009715110692131773\n",
      "STEP: 569900 - loss: 0.009713790467558296\n",
      "STEP: 570000 - loss: 0.009712470637672967\n",
      "STEP: 570100 - loss: 0.00971115120235525\n",
      "STEP: 570200 - loss: 0.0097098321614854\n",
      "STEP: 570300 - loss: 0.009708513514943246\n",
      "STEP: 570400 - loss: 0.009707195262609132\n",
      "STEP: 570500 - loss: 0.009705877404363074\n",
      "STEP: 570600 - loss: 0.00970455994008486\n",
      "STEP: 570700 - loss: 0.009703242869655121\n",
      "STEP: 570800 - loss: 0.009701926192953835\n",
      "STEP: 570900 - loss: 0.009700609909861482\n",
      "STEP: 571000 - loss: 0.009699294020258044\n",
      "STEP: 571100 - loss: 0.009697978524023905\n",
      "STEP: 571200 - loss: 0.009696663421039725\n",
      "STEP: 571300 - loss: 0.009695348711185428\n",
      "STEP: 571400 - loss: 0.009694034394341819\n",
      "STEP: 571500 - loss: 0.009692720470389157\n",
      "STEP: 571600 - loss: 0.009691406939208048\n",
      "STEP: 571700 - loss: 0.009690093800678954\n",
      "STEP: 571800 - loss: 0.009688781054682575\n",
      "STEP: 571900 - loss: 0.009687468701099444\n",
      "STEP: 572000 - loss: 0.009686156739810044\n",
      "STEP: 572100 - loss: 0.009684845170695464\n",
      "STEP: 572200 - loss: 0.009683533993635961\n",
      "STEP: 572300 - loss: 0.00968222320851256\n",
      "STEP: 572400 - loss: 0.009680912815205977\n",
      "STEP: 572500 - loss: 0.009679602813597206\n",
      "STEP: 572600 - loss: 0.009678293203566925\n",
      "STEP: 572700 - loss: 0.009676983984996017\n",
      "STEP: 572800 - loss: 0.0096756751577656\n",
      "STEP: 572900 - loss: 0.009674366721756533\n",
      "STEP: 573000 - loss: 0.009673058676849748\n",
      "STEP: 573100 - loss: 0.009671751022926348\n",
      "STEP: 573200 - loss: 0.009670443759867562\n",
      "STEP: 573300 - loss: 0.009669136887554435\n",
      "STEP: 573400 - loss: 0.009667830405867942\n",
      "STEP: 573500 - loss: 0.00966652431468948\n",
      "STEP: 573600 - loss: 0.009665218613900312\n",
      "STEP: 573700 - loss: 0.009663913303381396\n",
      "STEP: 573800 - loss: 0.009662608383014264\n",
      "STEP: 573900 - loss: 0.009661303852680495\n",
      "STEP: 574000 - loss: 0.009659999712260925\n",
      "STEP: 574100 - loss: 0.00965869596163726\n",
      "STEP: 574200 - loss: 0.009657392600690892\n",
      "STEP: 574300 - loss: 0.009656089629303399\n",
      "STEP: 574400 - loss: 0.00965478704735621\n",
      "STEP: 574500 - loss: 0.009653484854730842\n",
      "STEP: 574600 - loss: 0.009652183051308768\n",
      "STEP: 574700 - loss: 0.009650881636972145\n",
      "STEP: 574800 - loss: 0.009649580611602057\n",
      "STEP: 574900 - loss: 0.0096482799750805\n",
      "STEP: 575000 - loss: 0.009646979727289163\n",
      "STEP: 575100 - loss: 0.009645679868109481\n",
      "STEP: 575200 - loss: 0.00964438039742373\n",
      "STEP: 575300 - loss: 0.009643081315113478\n",
      "STEP: 575400 - loss: 0.009641782621060899\n",
      "STEP: 575500 - loss: 0.009640484315147493\n",
      "STEP: 575600 - loss: 0.009639186397255441\n",
      "STEP: 575700 - loss: 0.009637888867266759\n",
      "STEP: 575800 - loss: 0.009636591725063329\n",
      "STEP: 575900 - loss: 0.009635294970527338\n",
      "STEP: 576000 - loss: 0.009633998603540812\n",
      "STEP: 576100 - loss: 0.0096327026239857\n",
      "STEP: 576200 - loss: 0.00963140703174466\n",
      "STEP: 576300 - loss: 0.009630111826699542\n",
      "STEP: 576400 - loss: 0.009628817008732408\n",
      "STEP: 576500 - loss: 0.009627522577725717\n",
      "STEP: 576600 - loss: 0.009626228533562115\n",
      "STEP: 576700 - loss: 0.009624934876123443\n",
      "STEP: 576800 - loss: 0.009623641605292173\n",
      "STEP: 576900 - loss: 0.009622348720951005\n",
      "STEP: 577000 - loss: 0.009621056222982148\n",
      "STEP: 577100 - loss: 0.009619764111268195\n",
      "STEP: 577200 - loss: 0.009618472385691433\n",
      "STEP: 577300 - loss: 0.009617181046134756\n",
      "STEP: 577400 - loss: 0.00961589009248063\n",
      "STEP: 577500 - loss: 0.009614599524611403\n",
      "STEP: 577600 - loss: 0.009613309342410326\n",
      "STEP: 577700 - loss: 0.009612019545759588\n",
      "STEP: 577800 - loss: 0.009610730134542304\n",
      "STEP: 577900 - loss: 0.00960944110864079\n",
      "STEP: 578000 - loss: 0.009608152467938173\n",
      "STEP: 578100 - loss: 0.009606864212317195\n",
      "STEP: 578200 - loss: 0.009605576341660857\n",
      "STEP: 578300 - loss: 0.009604288855851958\n",
      "STEP: 578400 - loss: 0.009603001754773344\n",
      "STEP: 578500 - loss: 0.009601715038308391\n",
      "STEP: 578600 - loss: 0.009600428706339514\n",
      "STEP: 578700 - loss: 0.00959914275875047\n",
      "STEP: 578800 - loss: 0.009597857195423824\n",
      "STEP: 578900 - loss: 0.009596572016242868\n",
      "STEP: 579000 - loss: 0.009595287221091011\n",
      "STEP: 579100 - loss: 0.009594002809850937\n",
      "STEP: 579200 - loss: 0.009592718782406274\n",
      "STEP: 579300 - loss: 0.009591435138640074\n",
      "STEP: 579400 - loss: 0.009590151878435791\n",
      "STEP: 579500 - loss: 0.009588869001676732\n",
      "STEP: 579600 - loss: 0.00958758650824645\n",
      "STEP: 579700 - loss: 0.009586304398027884\n",
      "STEP: 579800 - loss: 0.009585022670904988\n",
      "STEP: 579900 - loss: 0.00958374132676099\n",
      "STEP: 580000 - loss: 0.009582460365479339\n",
      "STEP: 580100 - loss: 0.009581179786943778\n",
      "STEP: 580200 - loss: 0.00957989959103756\n",
      "STEP: 580300 - loss: 0.009578619777644832\n",
      "STEP: 580400 - loss: 0.009577340346648987\n",
      "STEP: 580500 - loss: 0.0095760612979337\n",
      "STEP: 580600 - loss: 0.00957478263138261\n",
      "STEP: 580700 - loss: 0.009573504346879756\n",
      "STEP: 580800 - loss: 0.009572226444308501\n",
      "STEP: 580900 - loss: 0.00957094892355328\n",
      "STEP: 581000 - loss: 0.009569671784497493\n",
      "STEP: 581100 - loss: 0.009568395027025048\n",
      "STEP: 581200 - loss: 0.009567118651020208\n",
      "STEP: 581300 - loss: 0.009565842656366737\n",
      "STEP: 581400 - loss: 0.009564567042948715\n",
      "STEP: 581500 - loss: 0.00956329181065017\n",
      "STEP: 581600 - loss: 0.009562016959355263\n",
      "STEP: 581700 - loss: 0.009560742488947872\n",
      "STEP: 581800 - loss: 0.009559468399312345\n",
      "STEP: 581900 - loss: 0.009558194690332793\n",
      "STEP: 582000 - loss: 0.009556921361893681\n",
      "STEP: 582100 - loss: 0.009555648413878879\n",
      "STEP: 582200 - loss: 0.009554375846172924\n",
      "STEP: 582300 - loss: 0.009553103658660078\n",
      "STEP: 582400 - loss: 0.009551831851225006\n",
      "STEP: 582500 - loss: 0.009550560423751428\n",
      "STEP: 582600 - loss: 0.009549289376124334\n",
      "STEP: 582700 - loss: 0.00954801870822829\n",
      "STEP: 582800 - loss: 0.009546748419947381\n",
      "STEP: 582900 - loss: 0.009545478511166211\n",
      "STEP: 583000 - loss: 0.009544208981769454\n",
      "STEP: 583100 - loss: 0.009542939831641993\n",
      "STEP: 583200 - loss: 0.00954167106066802\n",
      "STEP: 583300 - loss: 0.00954040266873249\n",
      "STEP: 583400 - loss: 0.009539134655720014\n",
      "STEP: 583500 - loss: 0.009537867021515456\n",
      "STEP: 583600 - loss: 0.009536599766003514\n",
      "STEP: 583700 - loss: 0.009535332889068871\n",
      "STEP: 583800 - loss: 0.009534066390596856\n",
      "STEP: 583900 - loss: 0.009532800270471795\n",
      "STEP: 584000 - loss: 0.009531534528578925\n",
      "STEP: 584100 - loss: 0.009530269164803071\n",
      "STEP: 584200 - loss: 0.009529004179029313\n",
      "STEP: 584300 - loss: 0.009527739571142916\n",
      "STEP: 584400 - loss: 0.009526475341028611\n",
      "STEP: 584500 - loss: 0.009525211488571446\n",
      "STEP: 584600 - loss: 0.009523948013656771\n",
      "STEP: 584700 - loss: 0.00952268491616982\n",
      "STEP: 584800 - loss: 0.009521422195995555\n",
      "STEP: 584900 - loss: 0.009520159853019277\n",
      "STEP: 585000 - loss: 0.009518897887126596\n",
      "STEP: 585100 - loss: 0.009517636298202169\n",
      "STEP: 585200 - loss: 0.009516375086132107\n",
      "STEP: 585300 - loss: 0.009515114250801237\n",
      "STEP: 585400 - loss: 0.009513853792095104\n",
      "STEP: 585500 - loss: 0.009512593709899259\n",
      "STEP: 585600 - loss: 0.009511334004099365\n",
      "STEP: 585700 - loss: 0.009510074674580392\n",
      "STEP: 585800 - loss: 0.009508815721228307\n",
      "STEP: 585900 - loss: 0.009507557143928686\n",
      "STEP: 586000 - loss: 0.009506298942567073\n",
      "STEP: 586100 - loss: 0.009505041117029042\n",
      "STEP: 586200 - loss: 0.009503783667200514\n",
      "STEP: 586300 - loss: 0.00950252659296688\n",
      "STEP: 586400 - loss: 0.00950126989421442\n",
      "STEP: 586500 - loss: 0.009500013570828407\n",
      "STEP: 586600 - loss: 0.00949875762269509\n",
      "STEP: 586700 - loss: 0.009497502049699931\n",
      "STEP: 586800 - loss: 0.00949624685172926\n",
      "STEP: 586900 - loss: 0.00949499202866861\n",
      "STEP: 587000 - loss: 0.009493737580404147\n",
      "STEP: 587100 - loss: 0.00949248350682201\n",
      "STEP: 587200 - loss: 0.009491229807808043\n",
      "STEP: 587300 - loss: 0.009489976483248421\n",
      "STEP: 587400 - loss: 0.009488723533029252\n",
      "STEP: 587500 - loss: 0.009487470957036677\n",
      "STEP: 587600 - loss: 0.009486218755156843\n",
      "STEP: 587700 - loss: 0.009484966927275844\n",
      "STEP: 587800 - loss: 0.009483715473280165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 587900 - loss: 0.009482464393055941\n",
      "STEP: 588000 - loss: 0.009481213686489805\n",
      "STEP: 588100 - loss: 0.009479963353467645\n",
      "STEP: 588200 - loss: 0.009478713393875984\n",
      "STEP: 588300 - loss: 0.009477463807601403\n",
      "STEP: 588400 - loss: 0.009476214594530064\n",
      "STEP: 588500 - loss: 0.009474965754548977\n",
      "STEP: 588600 - loss: 0.009473717287544363\n",
      "STEP: 588700 - loss: 0.009472469193402567\n",
      "STEP: 588800 - loss: 0.009471221472010587\n",
      "STEP: 588900 - loss: 0.009469974123254739\n",
      "STEP: 589000 - loss: 0.00946872714702183\n",
      "STEP: 589100 - loss: 0.009467480543198454\n",
      "STEP: 589200 - loss: 0.009466234311671566\n",
      "STEP: 589300 - loss: 0.009464988452327848\n",
      "STEP: 589400 - loss: 0.00946374296505404\n",
      "STEP: 589500 - loss: 0.009462497849736903\n",
      "STEP: 589600 - loss: 0.00946125310626329\n",
      "STEP: 589700 - loss: 0.009460008734520508\n",
      "STEP: 589800 - loss: 0.009458764734395064\n",
      "STEP: 589900 - loss: 0.009457521105774242\n",
      "STEP: 590000 - loss: 0.009456277848544612\n",
      "STEP: 590100 - loss: 0.009455034962593627\n",
      "STEP: 590200 - loss: 0.009453792447808276\n",
      "STEP: 590300 - loss: 0.009452550304075713\n",
      "STEP: 590400 - loss: 0.00945130853128277\n",
      "STEP: 590500 - loss: 0.009450067129316883\n",
      "STEP: 590600 - loss: 0.00944882609806531\n",
      "STEP: 590700 - loss: 0.00944758543741515\n",
      "STEP: 590800 - loss: 0.009446345147253996\n",
      "STEP: 590900 - loss: 0.009445105227468786\n",
      "STEP: 591000 - loss: 0.009443865677947116\n",
      "STEP: 591100 - loss: 0.009442626498576183\n",
      "STEP: 591200 - loss: 0.009441387689243482\n",
      "STEP: 591300 - loss: 0.009440149249836787\n",
      "STEP: 591400 - loss: 0.009438911180242981\n",
      "STEP: 591500 - loss: 0.009437673480349934\n",
      "STEP: 591600 - loss: 0.00943643615004516\n",
      "STEP: 591700 - loss: 0.009435199189216321\n",
      "STEP: 591800 - loss: 0.009433962597751118\n",
      "STEP: 591900 - loss: 0.009432726375536916\n",
      "STEP: 592000 - loss: 0.009431490522461675\n",
      "STEP: 592100 - loss: 0.009430255038412978\n",
      "STEP: 592200 - loss: 0.009429019923278702\n",
      "STEP: 592300 - loss: 0.009427785176946464\n",
      "STEP: 592400 - loss: 0.009426550799304089\n",
      "STEP: 592500 - loss: 0.0094253167902399\n",
      "STEP: 592600 - loss: 0.009424083149641169\n",
      "STEP: 592700 - loss: 0.00942284987739628\n",
      "STEP: 592800 - loss: 0.009421616973392836\n",
      "STEP: 592900 - loss: 0.00942038443751929\n",
      "STEP: 593000 - loss: 0.009419152269663337\n",
      "STEP: 593100 - loss: 0.009417920469713315\n",
      "STEP: 593200 - loss: 0.009416689037556896\n",
      "STEP: 593300 - loss: 0.009415457973082473\n",
      "STEP: 593400 - loss: 0.00941422727617841\n",
      "STEP: 593500 - loss: 0.009412996946732661\n",
      "STEP: 593600 - loss: 0.009411766984633543\n",
      "STEP: 593700 - loss: 0.009410537389769295\n",
      "STEP: 593800 - loss: 0.009409308162028198\n",
      "STEP: 593900 - loss: 0.009408079301298729\n",
      "STEP: 594000 - loss: 0.009406850807469096\n",
      "STEP: 594100 - loss: 0.009405622680427956\n",
      "STEP: 594200 - loss: 0.009404394920063317\n",
      "STEP: 594300 - loss: 0.009403167526264179\n",
      "STEP: 594400 - loss: 0.009401940498918683\n",
      "STEP: 594500 - loss: 0.009400713837915556\n",
      "STEP: 594600 - loss: 0.009399487543143372\n",
      "STEP: 594700 - loss: 0.009398261614490434\n",
      "STEP: 594800 - loss: 0.00939703605184584\n",
      "STEP: 594900 - loss: 0.009395810855098082\n",
      "STEP: 595000 - loss: 0.009394586024136047\n",
      "STEP: 595100 - loss: 0.009393361558847997\n",
      "STEP: 595200 - loss: 0.00939213745912321\n",
      "STEP: 595300 - loss: 0.009390913724850267\n",
      "STEP: 595400 - loss: 0.009389690355918004\n",
      "STEP: 595500 - loss: 0.009388467352215503\n",
      "STEP: 595600 - loss: 0.009387244713631471\n",
      "STEP: 595700 - loss: 0.009386022440055075\n",
      "STEP: 595800 - loss: 0.009384800531375005\n",
      "STEP: 595900 - loss: 0.009383578987480587\n",
      "STEP: 596000 - loss: 0.009382357808260468\n",
      "STEP: 596100 - loss: 0.00938113699360434\n",
      "STEP: 596200 - loss: 0.009379916543400977\n",
      "STEP: 596300 - loss: 0.009378696457539245\n",
      "STEP: 596400 - loss: 0.009377476735908762\n",
      "STEP: 596500 - loss: 0.009376257378398692\n",
      "STEP: 596600 - loss: 0.009375038384898175\n",
      "STEP: 596700 - loss: 0.009373819755296628\n",
      "STEP: 596800 - loss: 0.009372601489483021\n",
      "STEP: 596900 - loss: 0.009371383587347317\n",
      "STEP: 597000 - loss: 0.009370166048778272\n",
      "STEP: 597100 - loss: 0.009368948873665859\n",
      "STEP: 597200 - loss: 0.009367732061899113\n",
      "STEP: 597300 - loss: 0.00936651561336776\n",
      "STEP: 597400 - loss: 0.00936529952796115\n",
      "STEP: 597500 - loss: 0.00936408380556909\n",
      "STEP: 597600 - loss: 0.00936286844608101\n",
      "STEP: 597700 - loss: 0.009361653449386694\n",
      "STEP: 597800 - loss: 0.00936043881537541\n",
      "STEP: 597900 - loss: 0.009359224543937207\n",
      "STEP: 598000 - loss: 0.009358010634961725\n",
      "STEP: 598100 - loss: 0.009356797088338708\n",
      "STEP: 598200 - loss: 0.009355583903958055\n",
      "STEP: 598300 - loss: 0.009354371081709329\n",
      "STEP: 598400 - loss: 0.009353158621482572\n",
      "STEP: 598500 - loss: 0.009351946523167896\n",
      "STEP: 598600 - loss: 0.009350734786654753\n",
      "STEP: 598700 - loss: 0.009349523411833451\n",
      "STEP: 598800 - loss: 0.009348312398593779\n",
      "STEP: 598900 - loss: 0.00934710174682592\n",
      "STEP: 599000 - loss: 0.00934589145641991\n",
      "STEP: 599100 - loss: 0.00934468152726587\n",
      "STEP: 599200 - loss: 0.009343471959253887\n",
      "STEP: 599300 - loss: 0.009342262752273862\n",
      "STEP: 599400 - loss: 0.009341053906216578\n",
      "STEP: 599500 - loss: 0.009339845420971637\n",
      "STEP: 599600 - loss: 0.009338637296429716\n",
      "STEP: 599700 - loss: 0.009337429532481054\n",
      "STEP: 599800 - loss: 0.00933622212901572\n",
      "STEP: 599900 - loss: 0.009335015085924481\n",
      "STEP: 600000 - loss: 0.009333808403097393\n",
      "STEP: 600100 - loss: 0.009332602080425011\n",
      "STEP: 600200 - loss: 0.009331396117797774\n",
      "STEP: 600300 - loss: 0.009330190515106265\n",
      "STEP: 600400 - loss: 0.009328985272240848\n",
      "STEP: 600500 - loss: 0.009327780389092292\n",
      "STEP: 600600 - loss: 0.009326575865550985\n",
      "STEP: 600700 - loss: 0.009325371701507998\n",
      "STEP: 600800 - loss: 0.00932416789685329\n",
      "STEP: 600900 - loss: 0.009322964451478032\n",
      "STEP: 601000 - loss: 0.009321761365272708\n",
      "STEP: 601100 - loss: 0.009320558638128449\n",
      "STEP: 601200 - loss: 0.009319356269935753\n",
      "STEP: 601300 - loss: 0.009318154260585337\n",
      "STEP: 601400 - loss: 0.009316952609968272\n",
      "STEP: 601500 - loss: 0.009315751317975475\n",
      "STEP: 601600 - loss: 0.009314550384497971\n",
      "STEP: 601700 - loss: 0.009313349809426346\n",
      "STEP: 601800 - loss: 0.009312149592652124\n",
      "STEP: 601900 - loss: 0.009310949734065637\n",
      "STEP: 602000 - loss: 0.009309750233558354\n",
      "STEP: 602100 - loss: 0.009308551091021697\n",
      "STEP: 602200 - loss: 0.009307352306346047\n",
      "STEP: 602300 - loss: 0.009306153879423153\n",
      "STEP: 602400 - loss: 0.009304955810144035\n",
      "STEP: 602500 - loss: 0.009303758098399909\n",
      "STEP: 602600 - loss: 0.00930256074408187\n",
      "STEP: 602700 - loss: 0.009301363747081475\n",
      "STEP: 602800 - loss: 0.009300167107289894\n",
      "STEP: 602900 - loss: 0.00929897082459852\n",
      "STEP: 603000 - loss: 0.009297774898898863\n",
      "STEP: 603100 - loss: 0.009296579330082176\n",
      "STEP: 603200 - loss: 0.009295384118039967\n",
      "STEP: 603300 - loss: 0.009294189262663701\n",
      "STEP: 603400 - loss: 0.009292994763844972\n",
      "STEP: 603500 - loss: 0.009291800621475424\n",
      "STEP: 603600 - loss: 0.009290606835446528\n",
      "STEP: 603700 - loss: 0.009289413405649687\n",
      "STEP: 603800 - loss: 0.009288220331977017\n",
      "STEP: 603900 - loss: 0.009287027614319652\n",
      "STEP: 604000 - loss: 0.009285835252569973\n",
      "STEP: 604100 - loss: 0.009284643246619224\n",
      "STEP: 604200 - loss: 0.009283451596359415\n",
      "STEP: 604300 - loss: 0.009282260301682294\n",
      "STEP: 604400 - loss: 0.009281069362479778\n",
      "STEP: 604500 - loss: 0.009279878778643435\n",
      "STEP: 604600 - loss: 0.00927868855006563\n",
      "STEP: 604700 - loss: 0.00927749867663829\n",
      "STEP: 604800 - loss: 0.009276309158253025\n",
      "STEP: 604900 - loss: 0.00927511999480229\n",
      "STEP: 605000 - loss: 0.009273931186177805\n",
      "STEP: 605100 - loss: 0.009272742732271789\n",
      "STEP: 605200 - loss: 0.009271554632976338\n",
      "STEP: 605300 - loss: 0.009270366888183532\n",
      "STEP: 605400 - loss: 0.009269179497785811\n",
      "STEP: 605500 - loss: 0.009267992461675128\n",
      "STEP: 605600 - loss: 0.009266805779743845\n",
      "STEP: 605700 - loss: 0.009265619451883993\n",
      "STEP: 605800 - loss: 0.009264433477988294\n",
      "STEP: 605900 - loss: 0.00926324785794883\n",
      "STEP: 606000 - loss: 0.009262062591658187\n",
      "STEP: 606100 - loss: 0.009260877679008427\n",
      "STEP: 606200 - loss: 0.009259693119892344\n",
      "STEP: 606300 - loss: 0.009258508914202133\n",
      "STEP: 606400 - loss: 0.009257325061830527\n",
      "STEP: 606500 - loss: 0.009256141562669944\n",
      "STEP: 606600 - loss: 0.009254958416613154\n",
      "STEP: 606700 - loss: 0.009253775623552497\n",
      "STEP: 606800 - loss: 0.009252593183380732\n",
      "STEP: 606900 - loss: 0.009251411095990392\n",
      "STEP: 607000 - loss: 0.009250229361274485\n",
      "STEP: 607100 - loss: 0.009249047979125471\n",
      "STEP: 607200 - loss: 0.009247866949436507\n",
      "STEP: 607300 - loss: 0.009246686272099878\n",
      "STEP: 607400 - loss: 0.009245505947008561\n",
      "STEP: 607500 - loss: 0.009244325974055907\n",
      "STEP: 607600 - loss: 0.009243146353134075\n",
      "STEP: 607700 - loss: 0.009241967084136498\n",
      "STEP: 607800 - loss: 0.009240788166956043\n",
      "STEP: 607900 - loss: 0.009239609601485671\n",
      "STEP: 608000 - loss: 0.009238431387618536\n",
      "STEP: 608100 - loss: 0.009237253525247239\n",
      "STEP: 608200 - loss: 0.009236076014265541\n",
      "STEP: 608300 - loss: 0.009234898854566066\n",
      "STEP: 608400 - loss: 0.0092337220460423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 608500 - loss: 0.009232545588587169\n",
      "STEP: 608600 - loss: 0.009231369482093915\n",
      "STEP: 608700 - loss: 0.009230193726456046\n",
      "STEP: 608800 - loss: 0.009229018321566865\n",
      "STEP: 608900 - loss: 0.009227843267319319\n",
      "STEP: 609000 - loss: 0.009226668563607212\n",
      "STEP: 609100 - loss: 0.009225494210323544\n",
      "STEP: 609200 - loss: 0.00922432020736203\n",
      "STEP: 609300 - loss: 0.009223146554615971\n",
      "STEP: 609400 - loss: 0.009221973251978797\n",
      "STEP: 609500 - loss: 0.009220800299344106\n",
      "STEP: 609600 - loss: 0.00921962769660559\n",
      "STEP: 609700 - loss: 0.00921845544365665\n",
      "STEP: 609800 - loss: 0.00921728354039086\n",
      "STEP: 609900 - loss: 0.009216111986702063\n",
      "STEP: 610000 - loss: 0.00921494078248385\n",
      "STEP: 610100 - loss: 0.009213769927629803\n",
      "STEP: 610200 - loss: 0.009212599422033925\n",
      "STEP: 610300 - loss: 0.009211429265589587\n",
      "STEP: 610400 - loss: 0.009210259458191088\n",
      "STEP: 610500 - loss: 0.009209089999732117\n",
      "STEP: 610600 - loss: 0.009207920890106382\n",
      "STEP: 610700 - loss: 0.009206752129207827\n",
      "STEP: 610800 - loss: 0.009205583716930548\n",
      "STEP: 610900 - loss: 0.009204415653168421\n",
      "STEP: 611000 - loss: 0.009203247937815334\n",
      "STEP: 611100 - loss: 0.009202080570765388\n",
      "STEP: 611200 - loss: 0.009200913551912916\n",
      "STEP: 611300 - loss: 0.009199746881151652\n",
      "STEP: 611400 - loss: 0.009198580558375906\n",
      "STEP: 611500 - loss: 0.009197414583479935\n",
      "STEP: 611600 - loss: 0.009196248956357712\n",
      "STEP: 611700 - loss: 0.009195083676903457\n",
      "STEP: 611800 - loss: 0.009193918745011806\n",
      "STEP: 611900 - loss: 0.009192754160576631\n",
      "STEP: 612000 - loss: 0.00919158992349246\n",
      "STEP: 612100 - loss: 0.009190426033653524\n",
      "STEP: 612200 - loss: 0.009189262490954385\n",
      "STEP: 612300 - loss: 0.00918809929528933\n",
      "STEP: 612400 - loss: 0.009186936446553044\n",
      "STEP: 612500 - loss: 0.00918577394463946\n",
      "STEP: 612600 - loss: 0.009184611789443768\n",
      "STEP: 612700 - loss: 0.009183449980860119\n",
      "STEP: 612800 - loss: 0.009182288518783206\n",
      "STEP: 612900 - loss: 0.009181127403107576\n",
      "STEP: 613000 - loss: 0.009179966633728002\n",
      "STEP: 613100 - loss: 0.009178806210539138\n",
      "STEP: 613200 - loss: 0.009177646133435488\n",
      "STEP: 613300 - loss: 0.009176486402312115\n",
      "STEP: 613400 - loss: 0.009175327017063549\n",
      "STEP: 613500 - loss: 0.00917416797758466\n",
      "STEP: 613600 - loss: 0.009173009283770534\n",
      "STEP: 613700 - loss: 0.009171850935515579\n",
      "STEP: 613800 - loss: 0.009170692932714884\n",
      "STEP: 613900 - loss: 0.009169535275263676\n",
      "STEP: 614000 - loss: 0.009168377963056553\n",
      "STEP: 614100 - loss: 0.009167220995988653\n",
      "STEP: 614200 - loss: 0.009166064373955157\n",
      "STEP: 614300 - loss: 0.009164908096850556\n",
      "STEP: 614400 - loss: 0.009163752164570578\n",
      "STEP: 614500 - loss: 0.009162596577010238\n",
      "STEP: 614600 - loss: 0.00916144133406455\n",
      "STEP: 614700 - loss: 0.009160286435628672\n",
      "STEP: 614800 - loss: 0.009159131881598092\n",
      "STEP: 614900 - loss: 0.009157977671867485\n",
      "STEP: 615000 - loss: 0.009156823806332844\n",
      "STEP: 615100 - loss: 0.009155670284889093\n",
      "STEP: 615200 - loss: 0.009154517107431679\n",
      "STEP: 615300 - loss: 0.009153364273855623\n",
      "STEP: 615400 - loss: 0.009152211784057004\n",
      "STEP: 615500 - loss: 0.009151059637930828\n",
      "STEP: 615600 - loss: 0.009149907835372745\n",
      "STEP: 615700 - loss: 0.009148756376278249\n",
      "STEP: 615800 - loss: 0.00914760526054274\n",
      "STEP: 615900 - loss: 0.00914645448806173\n",
      "STEP: 616000 - loss: 0.009145304058731107\n",
      "STEP: 616100 - loss: 0.009144153972446447\n",
      "STEP: 616200 - loss: 0.00914300422910307\n",
      "STEP: 616300 - loss: 0.009141854828597267\n",
      "STEP: 616400 - loss: 0.00914070577082407\n",
      "STEP: 616500 - loss: 0.009139557055680028\n",
      "STEP: 616600 - loss: 0.009138408683060385\n",
      "STEP: 616700 - loss: 0.009137260652860902\n",
      "STEP: 616800 - loss: 0.009136112964977896\n",
      "STEP: 616900 - loss: 0.009134965619306986\n",
      "STEP: 617000 - loss: 0.009133818615744025\n",
      "STEP: 617100 - loss: 0.009132671954185021\n",
      "STEP: 617200 - loss: 0.009131525634526181\n",
      "STEP: 617300 - loss: 0.009130379656663107\n",
      "STEP: 617400 - loss: 0.009129234020492311\n",
      "STEP: 617500 - loss: 0.0091280887259094\n",
      "STEP: 617600 - loss: 0.009126943772810724\n",
      "STEP: 617700 - loss: 0.009125799161092471\n",
      "STEP: 617800 - loss: 0.009124654890650853\n",
      "STEP: 617900 - loss: 0.009123510961382\n",
      "STEP: 618000 - loss: 0.009122367373182054\n",
      "STEP: 618100 - loss: 0.009121224125947208\n",
      "STEP: 618200 - loss: 0.009120081219574291\n",
      "STEP: 618300 - loss: 0.00911893865395898\n",
      "STEP: 618400 - loss: 0.009117796428997963\n",
      "STEP: 618500 - loss: 0.00911665454458766\n",
      "STEP: 618600 - loss: 0.009115513000624388\n",
      "STEP: 618700 - loss: 0.00911437179700468\n",
      "STEP: 618800 - loss: 0.009113230933624941\n",
      "STEP: 618900 - loss: 0.009112090410381874\n",
      "STEP: 619000 - loss: 0.009110950227171776\n",
      "STEP: 619100 - loss: 0.009109810383891503\n",
      "STEP: 619200 - loss: 0.00910867088043747\n",
      "STEP: 619300 - loss: 0.009107531716706295\n",
      "STEP: 619400 - loss: 0.00910639289259479\n",
      "STEP: 619500 - loss: 0.009105254407999511\n",
      "STEP: 619600 - loss: 0.009104116262817517\n",
      "STEP: 619700 - loss: 0.00910297845694529\n",
      "STEP: 619800 - loss: 0.00910184099027949\n",
      "STEP: 619900 - loss: 0.009100703862717423\n",
      "STEP: 620000 - loss: 0.00909956707415566\n",
      "STEP: 620100 - loss: 0.00909843062449132\n",
      "STEP: 620200 - loss: 0.009097294513620947\n",
      "STEP: 620300 - loss: 0.009096158741441849\n",
      "STEP: 620400 - loss: 0.009095023307850843\n",
      "STEP: 620500 - loss: 0.009093888212744993\n",
      "STEP: 620600 - loss: 0.00909275345602127\n",
      "STEP: 620700 - loss: 0.009091619037576888\n",
      "STEP: 620800 - loss: 0.009090484957309018\n",
      "STEP: 620900 - loss: 0.009089351215114682\n",
      "STEP: 621000 - loss: 0.009088217810891079\n",
      "STEP: 621100 - loss: 0.00908708474453532\n",
      "STEP: 621200 - loss: 0.00908595201594484\n",
      "STEP: 621300 - loss: 0.009084819625017138\n",
      "STEP: 621400 - loss: 0.009083687571648718\n",
      "STEP: 621500 - loss: 0.009082555855737806\n",
      "STEP: 621600 - loss: 0.009081424477181296\n",
      "STEP: 621700 - loss: 0.009080293435876564\n",
      "STEP: 621800 - loss: 0.009079162731721137\n",
      "STEP: 621900 - loss: 0.009078032364612635\n",
      "STEP: 622000 - loss: 0.009076902334448213\n",
      "STEP: 622100 - loss: 0.009075772641125862\n",
      "STEP: 622200 - loss: 0.009074643284542539\n",
      "STEP: 622300 - loss: 0.009073514264596298\n",
      "STEP: 622400 - loss: 0.00907238558118478\n",
      "STEP: 622500 - loss: 0.009071257234205302\n",
      "STEP: 622600 - loss: 0.009070129223555684\n",
      "STEP: 622700 - loss: 0.009069001549133782\n",
      "STEP: 622800 - loss: 0.009067874210837042\n",
      "STEP: 622900 - loss: 0.009066747208563531\n",
      "STEP: 623000 - loss: 0.009065620542211121\n",
      "STEP: 623100 - loss: 0.009064494211677019\n",
      "STEP: 623200 - loss: 0.009063368216859827\n",
      "STEP: 623300 - loss: 0.009062242557657134\n",
      "STEP: 623400 - loss: 0.009061117233966805\n",
      "STEP: 623500 - loss: 0.009059992245687097\n",
      "STEP: 623600 - loss: 0.009058867592715491\n",
      "STEP: 623700 - loss: 0.009057743274950385\n",
      "STEP: 623800 - loss: 0.009056619292289726\n",
      "STEP: 623900 - loss: 0.009055495644631798\n",
      "STEP: 624000 - loss: 0.009054372331874307\n",
      "STEP: 624100 - loss: 0.009053249353915733\n",
      "STEP: 624200 - loss: 0.009052126710654081\n",
      "STEP: 624300 - loss: 0.009051004401987816\n",
      "STEP: 624400 - loss: 0.00904988242781484\n",
      "STEP: 624500 - loss: 0.009048760788033404\n",
      "STEP: 624600 - loss: 0.009047639482542327\n",
      "STEP: 624700 - loss: 0.009046518511239464\n",
      "STEP: 624800 - loss: 0.009045397874023134\n",
      "STEP: 624900 - loss: 0.009044277570791908\n",
      "STEP: 625000 - loss: 0.009043157601444373\n",
      "STEP: 625100 - loss: 0.009042037965878607\n",
      "STEP: 625200 - loss: 0.009040918663993747\n",
      "STEP: 625300 - loss: 0.009039799695687285\n",
      "STEP: 625400 - loss: 0.00903868106085865\n",
      "STEP: 625500 - loss: 0.00903756275940611\n",
      "STEP: 625600 - loss: 0.009036444791228273\n",
      "STEP: 625700 - loss: 0.009035327156223707\n",
      "STEP: 625800 - loss: 0.009034209854291127\n",
      "STEP: 625900 - loss: 0.009033092885329407\n",
      "STEP: 626000 - loss: 0.009031976249237147\n",
      "STEP: 626100 - loss: 0.009030859945913006\n",
      "STEP: 626200 - loss: 0.00902974397525595\n",
      "STEP: 626300 - loss: 0.009028628337164606\n",
      "STEP: 626400 - loss: 0.009027513031538054\n",
      "STEP: 626500 - loss: 0.009026398058275265\n",
      "STEP: 626600 - loss: 0.009025283417274573\n",
      "STEP: 626700 - loss: 0.009024169108435544\n",
      "STEP: 626800 - loss: 0.009023055131657017\n",
      "STEP: 626900 - loss: 0.009021941486837829\n",
      "STEP: 627000 - loss: 0.009020828173877228\n",
      "STEP: 627100 - loss: 0.009019715192674046\n",
      "STEP: 627200 - loss: 0.009018602543127682\n",
      "STEP: 627300 - loss: 0.009017490225136962\n",
      "STEP: 627400 - loss: 0.009016378238601222\n",
      "STEP: 627500 - loss: 0.009015266583419657\n",
      "STEP: 627600 - loss: 0.009014155259491435\n",
      "STEP: 627700 - loss: 0.009013044266715762\n",
      "STEP: 627800 - loss: 0.009011933604992067\n",
      "STEP: 627900 - loss: 0.0090108232742198\n",
      "STEP: 628000 - loss: 0.009009713274297724\n",
      "STEP: 628100 - loss: 0.009008603605125821\n",
      "STEP: 628200 - loss: 0.009007494266603345\n",
      "STEP: 628300 - loss: 0.009006385258629514\n",
      "STEP: 628400 - loss: 0.009005276581103961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 628500 - loss: 0.00900416823392622\n",
      "STEP: 628600 - loss: 0.00900306021699573\n",
      "STEP: 628700 - loss: 0.009001952530212173\n",
      "STEP: 628800 - loss: 0.009000845173475018\n",
      "STEP: 628900 - loss: 0.008999738146684033\n",
      "STEP: 629000 - loss: 0.008998631449738573\n",
      "STEP: 629100 - loss: 0.008997525082538716\n",
      "STEP: 629200 - loss: 0.00899641904498393\n",
      "STEP: 629300 - loss: 0.008995313336973874\n",
      "STEP: 629400 - loss: 0.008994207958408757\n",
      "STEP: 629500 - loss: 0.008993102909187912\n",
      "STEP: 629600 - loss: 0.008991998189211294\n",
      "STEP: 629700 - loss: 0.008990893798378896\n",
      "STEP: 629800 - loss: 0.008989789736590635\n",
      "STEP: 629900 - loss: 0.008988686003746296\n",
      "STEP: 630000 - loss: 0.008987582599745894\n",
      "STEP: 630100 - loss: 0.008986479524489274\n",
      "STEP: 630200 - loss: 0.008985376777876902\n",
      "STEP: 630300 - loss: 0.008984274359808286\n",
      "STEP: 630400 - loss: 0.00898317227018379\n",
      "STEP: 630500 - loss: 0.0089820705089035\n",
      "STEP: 630600 - loss: 0.008980969075867547\n",
      "STEP: 630700 - loss: 0.00897986797097605\n",
      "STEP: 630800 - loss: 0.008978767194129218\n",
      "STEP: 630900 - loss: 0.008977666745227438\n",
      "STEP: 631000 - loss: 0.008976566624170786\n",
      "STEP: 631100 - loss: 0.00897546683085956\n",
      "STEP: 631200 - loss: 0.008974367365194155\n",
      "STEP: 631300 - loss: 0.008973268227075119\n",
      "STEP: 631400 - loss: 0.008972169416402422\n",
      "STEP: 631500 - loss: 0.008971070933076563\n",
      "STEP: 631600 - loss: 0.008969972776998332\n",
      "STEP: 631700 - loss: 0.008968874948067739\n",
      "STEP: 631800 - loss: 0.008967777446185704\n",
      "STEP: 631900 - loss: 0.008966680271252482\n",
      "STEP: 632000 - loss: 0.00896558342316892\n",
      "STEP: 632100 - loss: 0.00896448690183513\n",
      "STEP: 632200 - loss: 0.008963390707152198\n",
      "STEP: 632300 - loss: 0.008962294839020672\n",
      "STEP: 632400 - loss: 0.008961199297341264\n",
      "STEP: 632500 - loss: 0.008960104082014368\n",
      "STEP: 632600 - loss: 0.008959009192941198\n",
      "STEP: 632700 - loss: 0.008957914630022198\n",
      "STEP: 632800 - loss: 0.008956820393158229\n",
      "STEP: 632900 - loss: 0.008955726482250253\n",
      "STEP: 633000 - loss: 0.00895463289719914\n",
      "STEP: 633100 - loss: 0.008953539637905471\n",
      "STEP: 633200 - loss: 0.008952446704270648\n",
      "STEP: 633300 - loss: 0.00895135409619537\n",
      "STEP: 633400 - loss: 0.008950261813580442\n",
      "STEP: 633500 - loss: 0.008949169856327401\n",
      "STEP: 633600 - loss: 0.00894807822433689\n",
      "STEP: 633700 - loss: 0.008946986917509872\n",
      "STEP: 633800 - loss: 0.0089458959357478\n",
      "STEP: 633900 - loss: 0.008944805278951851\n",
      "STEP: 634000 - loss: 0.008943714947022802\n",
      "STEP: 634100 - loss: 0.008942624939862042\n",
      "STEP: 634200 - loss: 0.008941535257370976\n",
      "STEP: 634300 - loss: 0.008940445899450554\n",
      "STEP: 634400 - loss: 0.008939356866002455\n",
      "STEP: 634500 - loss: 0.008938268156927545\n",
      "STEP: 634600 - loss: 0.008937179772127416\n",
      "STEP: 634700 - loss: 0.00893609171150352\n",
      "STEP: 634800 - loss: 0.008935003974957032\n",
      "STEP: 634900 - loss: 0.008933916562389774\n",
      "STEP: 635000 - loss: 0.008932829473702792\n",
      "STEP: 635100 - loss: 0.008931742708797845\n",
      "STEP: 635200 - loss: 0.008930656267576107\n",
      "STEP: 635300 - loss: 0.0089295701499397\n",
      "STEP: 635400 - loss: 0.008928484355789846\n",
      "STEP: 635500 - loss: 0.008927398885028158\n",
      "STEP: 635600 - loss: 0.008926313737556509\n",
      "STEP: 635700 - loss: 0.008925228913276313\n",
      "STEP: 635800 - loss: 0.008924144412089365\n",
      "STEP: 635900 - loss: 0.00892306023389745\n",
      "STEP: 636000 - loss: 0.008921976378602403\n",
      "STEP: 636100 - loss: 0.008920892846106004\n",
      "STEP: 636200 - loss: 0.008919809636309882\n",
      "STEP: 636300 - loss: 0.008918726749116077\n",
      "STEP: 636400 - loss: 0.00891764418442636\n",
      "STEP: 636500 - loss: 0.008916561942142808\n",
      "STEP: 636600 - loss: 0.008915480022167124\n",
      "STEP: 636700 - loss: 0.008914398424401577\n",
      "STEP: 636800 - loss: 0.008913317148747915\n",
      "STEP: 636900 - loss: 0.008912236195108307\n",
      "STEP: 637000 - loss: 0.008911155563385042\n",
      "STEP: 637100 - loss: 0.008910075253479617\n",
      "STEP: 637200 - loss: 0.008908995265294803\n",
      "STEP: 637300 - loss: 0.00890791559873226\n",
      "STEP: 637400 - loss: 0.008906836253694525\n",
      "STEP: 637500 - loss: 0.008905757230083886\n",
      "STEP: 637600 - loss: 0.008904678527802055\n",
      "STEP: 637700 - loss: 0.008903600146751631\n",
      "STEP: 637800 - loss: 0.008902522086835027\n",
      "STEP: 637900 - loss: 0.008901444347954429\n",
      "STEP: 638000 - loss: 0.008900366930012267\n",
      "STEP: 638100 - loss: 0.00889928983291088\n",
      "STEP: 638200 - loss: 0.008898213056552678\n",
      "STEP: 638300 - loss: 0.008897136600840279\n",
      "STEP: 638400 - loss: 0.00889606046567581\n",
      "STEP: 638500 - loss: 0.00889498465096226\n",
      "STEP: 638600 - loss: 0.008893909156601824\n",
      "STEP: 638700 - loss: 0.008892833982497074\n",
      "STEP: 638800 - loss: 0.008891759128550914\n",
      "STEP: 638900 - loss: 0.008890684594665656\n",
      "STEP: 639000 - loss: 0.00888961038074407\n",
      "STEP: 639100 - loss: 0.008888536486688857\n",
      "STEP: 639200 - loss: 0.008887462912402593\n",
      "STEP: 639300 - loss: 0.008886389657788436\n",
      "STEP: 639400 - loss: 0.008885316722748827\n",
      "STEP: 639500 - loss: 0.008884244107186572\n",
      "STEP: 639600 - loss: 0.008883171811004488\n",
      "STEP: 639700 - loss: 0.008882099834105638\n",
      "STEP: 639800 - loss: 0.008881028176392618\n",
      "STEP: 639900 - loss: 0.008879956837768998\n",
      "STEP: 640000 - loss: 0.008878885818136912\n",
      "STEP: 640100 - loss: 0.008877815117399905\n",
      "STEP: 640200 - loss: 0.00887674473546064\n",
      "STEP: 640300 - loss: 0.008875674672222507\n",
      "STEP: 640400 - loss: 0.008874604927588346\n",
      "STEP: 640500 - loss: 0.008873535501461341\n",
      "STEP: 640600 - loss: 0.00887246639374464\n",
      "STEP: 640700 - loss: 0.008871397604341216\n",
      "STEP: 640800 - loss: 0.00887032913315453\n",
      "STEP: 640900 - loss: 0.008869260980087676\n",
      "STEP: 641000 - loss: 0.008868193145043914\n",
      "STEP: 641100 - loss: 0.008867125627926681\n",
      "STEP: 641200 - loss: 0.008866058428638721\n",
      "STEP: 641300 - loss: 0.008864991547084058\n",
      "STEP: 641400 - loss: 0.008863924983165843\n",
      "STEP: 641500 - loss: 0.008862858736787327\n",
      "STEP: 641600 - loss: 0.008861792807852101\n",
      "STEP: 641700 - loss: 0.008860727196263529\n",
      "STEP: 641800 - loss: 0.008859661901925104\n",
      "STEP: 641900 - loss: 0.008858596924740279\n",
      "STEP: 642000 - loss: 0.008857532264612718\n",
      "STEP: 642100 - loss: 0.008856467921445985\n",
      "STEP: 642200 - loss: 0.008855403895143648\n",
      "STEP: 642300 - loss: 0.008854340185609348\n",
      "STEP: 642400 - loss: 0.008853276792746787\n",
      "STEP: 642500 - loss: 0.008852213716459455\n",
      "STEP: 642600 - loss: 0.008851150956651534\n",
      "STEP: 642700 - loss: 0.008850088513226353\n",
      "STEP: 642800 - loss: 0.008849026386087606\n",
      "STEP: 642900 - loss: 0.008847964575139537\n",
      "STEP: 643000 - loss: 0.008846903080285778\n",
      "STEP: 643100 - loss: 0.00884584190143008\n",
      "STEP: 643200 - loss: 0.008844781038476345\n",
      "STEP: 643300 - loss: 0.008843720491328807\n",
      "STEP: 643400 - loss: 0.00884266025989099\n",
      "STEP: 643500 - loss: 0.008841600344067297\n",
      "STEP: 643600 - loss: 0.00884054074376121\n",
      "STEP: 643700 - loss: 0.008839481458877394\n",
      "STEP: 643800 - loss: 0.00883842248931936\n",
      "STEP: 643900 - loss: 0.008837363834991643\n",
      "STEP: 644000 - loss: 0.008836305495798415\n",
      "STEP: 644100 - loss: 0.008835247471643257\n",
      "STEP: 644200 - loss: 0.00883418976243087\n",
      "STEP: 644300 - loss: 0.008833132368065294\n",
      "STEP: 644400 - loss: 0.008832075288450815\n",
      "STEP: 644500 - loss: 0.008831018523491779\n",
      "STEP: 644600 - loss: 0.008829962073092388\n",
      "STEP: 644700 - loss: 0.008828905937156958\n",
      "STEP: 644800 - loss: 0.008827850115589867\n",
      "STEP: 644900 - loss: 0.008826794608295573\n",
      "STEP: 645000 - loss: 0.008825739415178505\n",
      "STEP: 645100 - loss: 0.00882468453614307\n",
      "STEP: 645200 - loss: 0.0088236299710937\n",
      "STEP: 645300 - loss: 0.008822575719934939\n",
      "STEP: 645400 - loss: 0.008821521782571427\n",
      "STEP: 645500 - loss: 0.008820468158907439\n",
      "STEP: 645600 - loss: 0.00881941484884795\n",
      "STEP: 645700 - loss: 0.008818361852297375\n",
      "STEP: 645800 - loss: 0.008817309169160462\n",
      "STEP: 645900 - loss: 0.008816256799341728\n",
      "STEP: 646000 - loss: 0.008815204742746097\n",
      "STEP: 646100 - loss: 0.00881415299927799\n",
      "STEP: 646200 - loss: 0.008813101568842626\n",
      "STEP: 646300 - loss: 0.008812050451344507\n",
      "STEP: 646400 - loss: 0.00881099964668857\n",
      "STEP: 646500 - loss: 0.008809949154779484\n",
      "STEP: 646600 - loss: 0.008808898975522405\n",
      "STEP: 646700 - loss: 0.008807849108821899\n",
      "STEP: 646800 - loss: 0.008806799554583288\n",
      "STEP: 646900 - loss: 0.00880575031271136\n",
      "STEP: 647000 - loss: 0.008804701383111084\n",
      "STEP: 647100 - loss: 0.008803652765687866\n",
      "STEP: 647200 - loss: 0.008802604460345944\n",
      "STEP: 647300 - loss: 0.008801556466991276\n",
      "STEP: 647400 - loss: 0.008800508785528537\n",
      "STEP: 647500 - loss: 0.008799461415862898\n",
      "STEP: 647600 - loss: 0.008798414357899532\n",
      "STEP: 647700 - loss: 0.008797367611543677\n",
      "STEP: 647800 - loss: 0.008796321176700674\n",
      "STEP: 647900 - loss: 0.008795275053275608\n",
      "STEP: 648000 - loss: 0.008794229241174064\n",
      "STEP: 648100 - loss: 0.008793183740301\n",
      "STEP: 648200 - loss: 0.0087921385505618\n",
      "STEP: 648300 - loss: 0.008791093671862139\n",
      "STEP: 648400 - loss: 0.008790049104107096\n",
      "STEP: 648500 - loss: 0.008789004847202313\n",
      "STEP: 648600 - loss: 0.008787960901053323\n",
      "STEP: 648700 - loss: 0.0087869172655653\n",
      "STEP: 648800 - loss: 0.008785873940644034\n",
      "STEP: 648900 - loss: 0.008784830926195069\n",
      "STEP: 649000 - loss: 0.008783788222123699\n",
      "STEP: 649100 - loss: 0.008782745828335695\n",
      "STEP: 649200 - loss: 0.008781703744737\n",
      "STEP: 649300 - loss: 0.008780661971232813\n",
      "STEP: 649400 - loss: 0.008779620507728979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 649500 - loss: 0.008778579354131379\n",
      "STEP: 649600 - loss: 0.008777538510345687\n",
      "STEP: 649700 - loss: 0.008776497976277432\n",
      "STEP: 649800 - loss: 0.00877545775183286\n",
      "STEP: 649900 - loss: 0.00877441783691733\n",
      "STEP: 650000 - loss: 0.00877337823143717\n",
      "STEP: 650100 - loss: 0.008772338935297884\n",
      "STEP: 650200 - loss: 0.00877129994840592\n",
      "STEP: 650300 - loss: 0.008770261270666367\n",
      "STEP: 650400 - loss: 0.008769222901985927\n",
      "STEP: 650500 - loss: 0.00876818484227057\n",
      "STEP: 650600 - loss: 0.008767147091426073\n",
      "STEP: 650700 - loss: 0.008766109649358431\n",
      "STEP: 650800 - loss: 0.008765072515973943\n",
      "STEP: 650900 - loss: 0.008764035691178887\n",
      "STEP: 651000 - loss: 0.008762999174878838\n",
      "STEP: 651100 - loss: 0.008761962966980622\n",
      "STEP: 651200 - loss: 0.008760927067390154\n",
      "STEP: 651300 - loss: 0.008759891476013535\n",
      "STEP: 651400 - loss: 0.00875885619275733\n",
      "STEP: 651500 - loss: 0.008757821217527568\n",
      "STEP: 651600 - loss: 0.008756786550230723\n",
      "STEP: 651700 - loss: 0.008755752190773118\n",
      "STEP: 651800 - loss: 0.008754718139061039\n",
      "STEP: 651900 - loss: 0.00875368439500104\n",
      "STEP: 652000 - loss: 0.008752650958499576\n",
      "STEP: 652100 - loss: 0.00875161782946292\n",
      "STEP: 652200 - loss: 0.008750585007797657\n",
      "STEP: 652300 - loss: 0.008749552493410338\n",
      "STEP: 652400 - loss: 0.00874852028620757\n",
      "STEP: 652500 - loss: 0.008747488386095705\n",
      "STEP: 652600 - loss: 0.00874645679298163\n",
      "STEP: 652700 - loss: 0.008745425506771775\n",
      "STEP: 652800 - loss: 0.008744394527373012\n",
      "STEP: 652900 - loss: 0.008743363854691785\n",
      "STEP: 653000 - loss: 0.008742333488635158\n",
      "STEP: 653100 - loss: 0.008741303429109382\n",
      "STEP: 653200 - loss: 0.008740273676021667\n",
      "STEP: 653300 - loss: 0.008739244229278562\n",
      "STEP: 653400 - loss: 0.00873821508878699\n",
      "STEP: 653500 - loss: 0.008737186254453747\n",
      "STEP: 653600 - loss: 0.00873615772618585\n",
      "STEP: 653700 - loss: 0.008735129503890065\n",
      "STEP: 653800 - loss: 0.008734101587473729\n",
      "STEP: 653900 - loss: 0.00873307397684337\n",
      "STEP: 654000 - loss: 0.008732046671906041\n",
      "STEP: 654100 - loss: 0.008731019672568948\n",
      "STEP: 654200 - loss: 0.008729992978739248\n",
      "STEP: 654300 - loss: 0.008728966590323487\n",
      "STEP: 654400 - loss: 0.00872794050722946\n",
      "STEP: 654500 - loss: 0.00872691472936404\n",
      "STEP: 654600 - loss: 0.008725889256634032\n",
      "STEP: 654700 - loss: 0.008724864088947328\n",
      "STEP: 654800 - loss: 0.008723839226210797\n",
      "STEP: 654900 - loss: 0.008722814668331483\n",
      "STEP: 655000 - loss: 0.008721790415216929\n",
      "STEP: 655100 - loss: 0.008720766466774535\n",
      "STEP: 655200 - loss: 0.008719742822911455\n",
      "STEP: 655300 - loss: 0.008718719483534972\n",
      "STEP: 655400 - loss: 0.008717696448552961\n",
      "STEP: 655500 - loss: 0.008716673717872197\n",
      "STEP: 655600 - loss: 0.008715651291400858\n",
      "STEP: 655700 - loss: 0.008714629169045772\n",
      "STEP: 655800 - loss: 0.008713607350714782\n",
      "STEP: 655900 - loss: 0.008712585836315224\n",
      "STEP: 656000 - loss: 0.008711564625754922\n",
      "STEP: 656100 - loss: 0.008710543718941335\n",
      "STEP: 656200 - loss: 0.008709523115781954\n",
      "STEP: 656300 - loss: 0.008708502816184826\n",
      "STEP: 656400 - loss: 0.00870748282005735\n",
      "STEP: 656500 - loss: 0.00870646312730714\n",
      "STEP: 656600 - loss: 0.008705443737842215\n",
      "STEP: 656700 - loss: 0.00870442465157008\n",
      "STEP: 656800 - loss: 0.008703405868398522\n",
      "STEP: 656900 - loss: 0.008702387388235627\n",
      "STEP: 657000 - loss: 0.0087013692109891\n",
      "STEP: 657100 - loss: 0.008700351336566827\n",
      "STEP: 657200 - loss: 0.008699333764876641\n",
      "STEP: 657300 - loss: 0.008698316495826412\n",
      "STEP: 657400 - loss: 0.008697299529324383\n",
      "STEP: 657500 - loss: 0.008696282865278241\n",
      "STEP: 657600 - loss: 0.00869526650359616\n",
      "STEP: 657700 - loss: 0.008694250444186414\n",
      "STEP: 657800 - loss: 0.008693234686956609\n",
      "STEP: 657900 - loss: 0.00869221923181501\n",
      "STEP: 658000 - loss: 0.008691204078669818\n",
      "STEP: 658100 - loss: 0.008690189227429182\n",
      "STEP: 658200 - loss: 0.008689174678001385\n",
      "STEP: 658300 - loss: 0.008688160430294353\n",
      "STEP: 658400 - loss: 0.008687146484216628\n",
      "STEP: 658500 - loss: 0.008686132839676286\n",
      "STEP: 658600 - loss: 0.008685119496581779\n",
      "STEP: 658700 - loss: 0.008684106454841136\n",
      "STEP: 658800 - loss: 0.008683093714363177\n",
      "STEP: 658900 - loss: 0.008682081275055686\n",
      "STEP: 659000 - loss: 0.008681069136827703\n",
      "STEP: 659100 - loss: 0.008680057299587199\n",
      "STEP: 659200 - loss: 0.00867904576324298\n",
      "STEP: 659300 - loss: 0.008678034527703143\n",
      "STEP: 659400 - loss: 0.008677023592876436\n",
      "STEP: 659500 - loss: 0.008676012958671559\n",
      "STEP: 659600 - loss: 0.008675002624996679\n",
      "STEP: 659700 - loss: 0.00867399259176085\n",
      "STEP: 659800 - loss: 0.008672982858872375\n",
      "STEP: 659900 - loss: 0.008671973426240171\n",
      "STEP: 660000 - loss: 0.008670964293772692\n",
      "STEP: 660100 - loss: 0.008669955461378763\n",
      "STEP: 660200 - loss: 0.008668946928967053\n",
      "STEP: 660300 - loss: 0.008667938696446416\n",
      "STEP: 660400 - loss: 0.008666930763725664\n",
      "STEP: 660500 - loss: 0.008665923130713488\n",
      "STEP: 660600 - loss: 0.00866491579731887\n",
      "STEP: 660700 - loss: 0.008663908763450584\n",
      "STEP: 660800 - loss: 0.008662902029017558\n",
      "STEP: 660900 - loss: 0.008661895593928924\n",
      "STEP: 661000 - loss: 0.008660889458093382\n",
      "STEP: 661100 - loss: 0.008659883621420141\n",
      "STEP: 661200 - loss: 0.008658878083818015\n",
      "STEP: 661300 - loss: 0.008657872845196136\n",
      "STEP: 661400 - loss: 0.00865686790546354\n",
      "STEP: 661500 - loss: 0.008655863264529492\n",
      "STEP: 661600 - loss: 0.008654858922303154\n",
      "STEP: 661700 - loss: 0.00865385487869327\n",
      "STEP: 661800 - loss: 0.008652851133609318\n",
      "STEP: 661900 - loss: 0.008651847686960689\n",
      "STEP: 662000 - loss: 0.00865084453865612\n",
      "STEP: 662100 - loss: 0.008649841688605237\n",
      "STEP: 662200 - loss: 0.008648839136717408\n",
      "STEP: 662300 - loss: 0.00864783688290189\n",
      "STEP: 662400 - loss: 0.008646834927067842\n",
      "STEP: 662500 - loss: 0.00864583326912491\n",
      "STEP: 662600 - loss: 0.00864483190898205\n",
      "STEP: 662700 - loss: 0.008643830846549305\n",
      "STEP: 662800 - loss: 0.008642830081735596\n",
      "STEP: 662900 - loss: 0.008641829614450697\n",
      "STEP: 663000 - loss: 0.00864082944460415\n",
      "STEP: 663100 - loss: 0.00863982957210542\n",
      "STEP: 663200 - loss: 0.008638829996864057\n",
      "STEP: 663300 - loss: 0.008637830718789657\n",
      "STEP: 663400 - loss: 0.008636831737791794\n",
      "STEP: 663500 - loss: 0.008635833053780363\n",
      "STEP: 663600 - loss: 0.008634834666664783\n",
      "STEP: 663700 - loss: 0.008633836576354748\n",
      "STEP: 663800 - loss: 0.008632838782760542\n",
      "STEP: 663900 - loss: 0.008631841285790993\n",
      "STEP: 664000 - loss: 0.008630844085356616\n",
      "STEP: 664100 - loss: 0.00862984718136696\n",
      "STEP: 664200 - loss: 0.008628850573731955\n",
      "STEP: 664300 - loss: 0.008627854262361356\n",
      "STEP: 664400 - loss: 0.008626858247165182\n",
      "STEP: 664500 - loss: 0.008625862528053311\n",
      "STEP: 664600 - loss: 0.00862486710493556\n",
      "STEP: 664700 - loss: 0.008623871977722426\n",
      "STEP: 664800 - loss: 0.008622877146323305\n",
      "STEP: 664900 - loss: 0.008621882610648463\n",
      "STEP: 665000 - loss: 0.008620888370608096\n",
      "STEP: 665100 - loss: 0.008619894426112062\n",
      "STEP: 665200 - loss: 0.008618900777070592\n",
      "STEP: 665300 - loss: 0.008617907423393936\n",
      "STEP: 665400 - loss: 0.008616914364992173\n",
      "STEP: 665500 - loss: 0.00861592160177556\n",
      "STEP: 665600 - loss: 0.008614929133654057\n",
      "STEP: 665700 - loss: 0.008613936960538136\n",
      "STEP: 665800 - loss: 0.008612945082338074\n",
      "STEP: 665900 - loss: 0.008611953498964462\n",
      "STEP: 666000 - loss: 0.008610962210327118\n",
      "STEP: 666100 - loss: 0.008609971216336798\n",
      "STEP: 666200 - loss: 0.0086089805169038\n",
      "STEP: 666300 - loss: 0.008607990111938229\n",
      "STEP: 666400 - loss: 0.008607000001350968\n",
      "STEP: 666500 - loss: 0.008606010185052314\n",
      "STEP: 666600 - loss: 0.008605020662952618\n",
      "STEP: 666700 - loss: 0.00860403143496281\n",
      "STEP: 666800 - loss: 0.008603042500993096\n",
      "STEP: 666900 - loss: 0.008602053860954025\n",
      "STEP: 667000 - loss: 0.008601065514756527\n",
      "STEP: 667100 - loss: 0.008600077462310863\n",
      "STEP: 667200 - loss: 0.008599089703528021\n",
      "STEP: 667300 - loss: 0.008598102238318495\n",
      "STEP: 667400 - loss: 0.008597115066593147\n",
      "STEP: 667500 - loss: 0.008596128188262619\n",
      "STEP: 667600 - loss: 0.008595141603237882\n",
      "STEP: 667700 - loss: 0.00859415531142933\n",
      "STEP: 667800 - loss: 0.008593169312748136\n",
      "STEP: 667900 - loss: 0.00859218360710509\n",
      "STEP: 668000 - loss: 0.008591198194410888\n",
      "STEP: 668100 - loss: 0.008590213074576688\n",
      "STEP: 668200 - loss: 0.008589228247513212\n",
      "STEP: 668300 - loss: 0.008588243713131797\n",
      "STEP: 668400 - loss: 0.008587259471342906\n",
      "STEP: 668500 - loss: 0.008586275522057828\n",
      "STEP: 668600 - loss: 0.008585291865187734\n",
      "STEP: 668700 - loss: 0.008584308500643472\n",
      "STEP: 668800 - loss: 0.008583325428336291\n",
      "STEP: 668900 - loss: 0.008582342648177251\n",
      "STEP: 669000 - loss: 0.008581360160077435\n",
      "STEP: 669100 - loss: 0.008580377963948276\n",
      "STEP: 669200 - loss: 0.00857939605970058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 669300 - loss: 0.008578414447245917\n",
      "STEP: 669400 - loss: 0.008577433126495452\n",
      "STEP: 669500 - loss: 0.008576452097360368\n",
      "STEP: 669600 - loss: 0.008575471359752023\n",
      "STEP: 669700 - loss: 0.008574490913582051\n",
      "STEP: 669800 - loss: 0.008573510758761286\n",
      "STEP: 669900 - loss: 0.008572530895201578\n",
      "STEP: 670000 - loss: 0.00857155132281413\n",
      "STEP: 670100 - loss: 0.008570572041510424\n",
      "STEP: 670200 - loss: 0.00856959305120197\n",
      "STEP: 670300 - loss: 0.00856861435180031\n",
      "STEP: 670400 - loss: 0.008567635943216887\n",
      "STEP: 670500 - loss: 0.008566657825363335\n",
      "STEP: 670600 - loss: 0.008565679998151114\n",
      "STEP: 670700 - loss: 0.008564702461492024\n",
      "STEP: 670800 - loss: 0.008563725215297474\n",
      "STEP: 670900 - loss: 0.008562748259479263\n",
      "STEP: 671000 - loss: 0.008561771593949156\n",
      "STEP: 671100 - loss: 0.008560795218618768\n",
      "STEP: 671200 - loss: 0.008559819133399798\n",
      "STEP: 671300 - loss: 0.008558843338204137\n",
      "STEP: 671400 - loss: 0.008557867832943418\n",
      "STEP: 671500 - loss: 0.00855689261752982\n",
      "STEP: 671600 - loss: 0.008555917691874668\n",
      "STEP: 671700 - loss: 0.008554943055890196\n",
      "STEP: 671800 - loss: 0.008553968709488472\n",
      "STEP: 671900 - loss: 0.008552994652580996\n",
      "STEP: 672000 - loss: 0.00855202088508011\n",
      "STEP: 672100 - loss: 0.00855104740689748\n",
      "STEP: 672200 - loss: 0.008550074217945316\n",
      "STEP: 672300 - loss: 0.008549101318135412\n",
      "STEP: 672400 - loss: 0.008548128707380264\n",
      "STEP: 672500 - loss: 0.008547156385591722\n",
      "STEP: 672600 - loss: 0.008546184352681884\n",
      "STEP: 672700 - loss: 0.008545212608562862\n",
      "STEP: 672800 - loss: 0.00854424115314695\n",
      "STEP: 672900 - loss: 0.008543269986346223\n",
      "STEP: 673000 - loss: 0.008542299108073168\n",
      "STEP: 673100 - loss: 0.008541328518239742\n",
      "STEP: 673200 - loss: 0.008540358216758383\n",
      "STEP: 673300 - loss: 0.008539388203541299\n",
      "STEP: 673400 - loss: 0.008538418478500854\n",
      "STEP: 673500 - loss: 0.008537449041549365\n",
      "STEP: 673600 - loss: 0.008536479892599334\n",
      "STEP: 673700 - loss: 0.008535511031563373\n",
      "STEP: 673800 - loss: 0.008534542458353403\n",
      "STEP: 673900 - loss: 0.0085335741728822\n",
      "STEP: 674000 - loss: 0.008532606175062407\n",
      "STEP: 674100 - loss: 0.008531638464806191\n",
      "STEP: 674200 - loss: 0.008530671042026407\n",
      "STEP: 674300 - loss: 0.008529703906635388\n",
      "STEP: 674400 - loss: 0.008528737058545713\n",
      "STEP: 674500 - loss: 0.00852777049767046\n",
      "STEP: 674600 - loss: 0.008526804223921575\n",
      "STEP: 674700 - loss: 0.008525838237212573\n",
      "STEP: 674800 - loss: 0.008524872537455574\n",
      "STEP: 674900 - loss: 0.008523907124563322\n",
      "STEP: 675000 - loss: 0.008522941998448785\n",
      "STEP: 675100 - loss: 0.008521977159024497\n",
      "STEP: 675200 - loss: 0.008521012606203528\n",
      "STEP: 675300 - loss: 0.008520048339898775\n",
      "STEP: 675400 - loss: 0.008519084360022947\n",
      "STEP: 675500 - loss: 0.008518120666488885\n",
      "STEP: 675600 - loss: 0.008517157259209585\n",
      "STEP: 675700 - loss: 0.008516194138098072\n",
      "STEP: 675800 - loss: 0.008515231303066998\n",
      "STEP: 675900 - loss: 0.008514268754029828\n",
      "STEP: 676000 - loss: 0.008513306490899143\n",
      "STEP: 676100 - loss: 0.008512344513588326\n",
      "STEP: 676200 - loss: 0.008511382822010406\n",
      "STEP: 676300 - loss: 0.008510421416078263\n",
      "STEP: 676400 - loss: 0.008509460295705212\n",
      "STEP: 676500 - loss: 0.008508499460804534\n",
      "STEP: 676600 - loss: 0.008507538911289091\n",
      "STEP: 676700 - loss: 0.00850657864707231\n",
      "STEP: 676800 - loss: 0.008505618668067464\n",
      "STEP: 676900 - loss: 0.008504658974187604\n",
      "STEP: 677000 - loss: 0.008503699565346228\n",
      "STEP: 677100 - loss: 0.008502740441456633\n",
      "STEP: 677200 - loss: 0.008501781602431995\n",
      "STEP: 677300 - loss: 0.008500823048185746\n",
      "STEP: 677400 - loss: 0.008499864778631386\n",
      "STEP: 677500 - loss: 0.008498906793682228\n",
      "STEP: 677600 - loss: 0.00849794909325184\n",
      "STEP: 677700 - loss: 0.008496991677253573\n",
      "STEP: 677800 - loss: 0.008496034545600977\n",
      "STEP: 677900 - loss: 0.008495077698207471\n",
      "STEP: 678000 - loss: 0.008494121134986784\n",
      "STEP: 678100 - loss: 0.008493164855852503\n",
      "STEP: 678200 - loss: 0.00849220886071811\n",
      "STEP: 678300 - loss: 0.008491253149497356\n",
      "STEP: 678400 - loss: 0.008490297722103822\n",
      "STEP: 678500 - loss: 0.008489342578450954\n",
      "STEP: 678600 - loss: 0.008488387718452959\n",
      "STEP: 678700 - loss: 0.008487433142023281\n",
      "STEP: 678800 - loss: 0.0084864788490753\n",
      "STEP: 678900 - loss: 0.008485524839523662\n",
      "STEP: 679000 - loss: 0.008484571113281816\n",
      "STEP: 679100 - loss: 0.008483617670263342\n",
      "STEP: 679200 - loss: 0.008482664510382305\n",
      "STEP: 679300 - loss: 0.008481711633552587\n",
      "STEP: 679400 - loss: 0.008480759039688162\n",
      "STEP: 679500 - loss: 0.00847980672870294\n",
      "STEP: 679600 - loss: 0.008478854700510765\n",
      "STEP: 679700 - loss: 0.008477902955025965\n",
      "STEP: 679800 - loss: 0.008476951492162092\n",
      "STEP: 679900 - loss: 0.00847600031183365\n",
      "STEP: 680000 - loss: 0.008475049413954574\n",
      "STEP: 680100 - loss: 0.008474098798438622\n",
      "STEP: 680200 - loss: 0.008473148465200427\n",
      "STEP: 680300 - loss: 0.008472198414153868\n",
      "STEP: 680400 - loss: 0.008471248645213312\n",
      "STEP: 680500 - loss: 0.0084702991582928\n",
      "STEP: 680600 - loss: 0.008469349953306674\n",
      "STEP: 680700 - loss: 0.008468401030169093\n",
      "STEP: 680800 - loss: 0.008467452388794294\n",
      "STEP: 680900 - loss: 0.008466504029096888\n",
      "STEP: 681000 - loss: 0.008465555950990848\n",
      "STEP: 681100 - loss: 0.008464608154390655\n",
      "STEP: 681200 - loss: 0.008463660639210857\n",
      "STEP: 681300 - loss: 0.008462713405365562\n",
      "STEP: 681400 - loss: 0.008461766452769715\n",
      "STEP: 681500 - loss: 0.008460819781337465\n",
      "STEP: 681600 - loss: 0.008459873390983067\n",
      "STEP: 681700 - loss: 0.008458927281621438\n",
      "STEP: 681800 - loss: 0.008457981453166865\n",
      "STEP: 681900 - loss: 0.008457035905534283\n",
      "STEP: 682000 - loss: 0.00845609063863781\n",
      "STEP: 682100 - loss: 0.008455145652392235\n",
      "STEP: 682200 - loss: 0.008454200946712533\n",
      "STEP: 682300 - loss: 0.008453256521512912\n",
      "STEP: 682400 - loss: 0.008452312376708631\n",
      "STEP: 682500 - loss: 0.008451368512213682\n",
      "STEP: 682600 - loss: 0.008450424927943515\n",
      "STEP: 682700 - loss: 0.008449481623812368\n",
      "STEP: 682800 - loss: 0.008448538599735374\n",
      "STEP: 682900 - loss: 0.008447595855627349\n",
      "STEP: 683000 - loss: 0.008446653391402955\n",
      "STEP: 683100 - loss: 0.00844571120697742\n",
      "STEP: 683200 - loss: 0.008444769302265304\n",
      "STEP: 683300 - loss: 0.00844382767718175\n",
      "STEP: 683400 - loss: 0.008442886331641631\n",
      "STEP: 683500 - loss: 0.008441945265559888\n",
      "STEP: 683600 - loss: 0.008441004478851332\n",
      "STEP: 683700 - loss: 0.008440063971431728\n",
      "STEP: 683800 - loss: 0.008439123743215466\n",
      "STEP: 683900 - loss: 0.008438183794117673\n",
      "STEP: 684000 - loss: 0.008437244124053982\n",
      "STEP: 684100 - loss: 0.008436304732939076\n",
      "STEP: 684200 - loss: 0.008435365620688107\n",
      "STEP: 684300 - loss: 0.008434426787216698\n",
      "STEP: 684400 - loss: 0.00843348823243938\n",
      "STEP: 684500 - loss: 0.008432549956272049\n",
      "STEP: 684600 - loss: 0.0084316119586297\n",
      "STEP: 684700 - loss: 0.008430674239427737\n",
      "STEP: 684800 - loss: 0.008429736798581283\n",
      "STEP: 684900 - loss: 0.008428799636005804\n",
      "STEP: 685000 - loss: 0.008427862751616864\n",
      "STEP: 685100 - loss: 0.008426926145329522\n",
      "STEP: 685200 - loss: 0.008425989817059235\n",
      "STEP: 685300 - loss: 0.008425053766721811\n",
      "STEP: 685400 - loss: 0.008424117994232361\n",
      "STEP: 685500 - loss: 0.00842318249950644\n",
      "STEP: 685600 - loss: 0.008422247282459936\n",
      "STEP: 685700 - loss: 0.008421312343007916\n",
      "STEP: 685800 - loss: 0.008420377681066186\n",
      "STEP: 685900 - loss: 0.008419443296550337\n",
      "STEP: 686000 - loss: 0.008418509189376065\n",
      "STEP: 686100 - loss: 0.008417575359459023\n",
      "STEP: 686200 - loss: 0.008416641806714748\n",
      "STEP: 686300 - loss: 0.008415708531059095\n",
      "STEP: 686400 - loss: 0.008414775532407711\n",
      "STEP: 686500 - loss: 0.008413842810676343\n",
      "STEP: 686600 - loss: 0.008412910365780658\n",
      "STEP: 686700 - loss: 0.008411978197636736\n",
      "STEP: 686800 - loss: 0.008411046306160488\n",
      "STEP: 686900 - loss: 0.008410114691267446\n",
      "STEP: 687000 - loss: 0.008409183352873748\n",
      "STEP: 687100 - loss: 0.00840825229089504\n",
      "STEP: 687200 - loss: 0.008407321505247478\n",
      "STEP: 687300 - loss: 0.008406390995847076\n",
      "STEP: 687400 - loss: 0.008405460762609668\n",
      "STEP: 687500 - loss: 0.008404530805451463\n",
      "STEP: 687600 - loss: 0.008403601124288089\n",
      "STEP: 687700 - loss: 0.008402671719036086\n",
      "STEP: 687800 - loss: 0.008401742589611372\n",
      "STEP: 687900 - loss: 0.008400813735929793\n",
      "STEP: 688000 - loss: 0.008399885157908035\n",
      "STEP: 688100 - loss: 0.008398956855461982\n",
      "STEP: 688200 - loss: 0.00839802882850769\n",
      "STEP: 688300 - loss: 0.008397101076961627\n",
      "STEP: 688400 - loss: 0.008396173600739791\n",
      "STEP: 688500 - loss: 0.008395246399758612\n",
      "STEP: 688600 - loss: 0.008394319473934328\n",
      "STEP: 688700 - loss: 0.008393392823183224\n",
      "STEP: 688800 - loss: 0.008392466447421918\n",
      "STEP: 688900 - loss: 0.008391540346566457\n",
      "STEP: 689000 - loss: 0.00839061452053348\n",
      "STEP: 689100 - loss: 0.008389688969239273\n",
      "STEP: 689200 - loss: 0.008388763692600268\n",
      "STEP: 689300 - loss: 0.008387838690532907\n",
      "STEP: 689400 - loss: 0.008386913962953938\n",
      "STEP: 689500 - loss: 0.008385989509779656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 689600 - loss: 0.008385065330926571\n",
      "STEP: 689700 - loss: 0.008384141426311376\n",
      "STEP: 689800 - loss: 0.00838321779585076\n",
      "STEP: 689900 - loss: 0.008382294439461209\n",
      "STEP: 690000 - loss: 0.008381371357059523\n",
      "STEP: 690100 - loss: 0.008380448548561906\n",
      "STEP: 690200 - loss: 0.008379526013885813\n",
      "STEP: 690300 - loss: 0.008378603752947326\n",
      "STEP: 690400 - loss: 0.00837768176566358\n",
      "STEP: 690500 - loss: 0.008376760051951222\n",
      "STEP: 690600 - loss: 0.008375838611726883\n",
      "STEP: 690700 - loss: 0.008374917444907797\n",
      "STEP: 690800 - loss: 0.00837399655141063\n",
      "STEP: 690900 - loss: 0.008373075931152205\n",
      "STEP: 691000 - loss: 0.008372155584049237\n",
      "STEP: 691100 - loss: 0.008371235510019028\n",
      "STEP: 691200 - loss: 0.008370315708978369\n",
      "STEP: 691300 - loss: 0.008369396180844315\n",
      "STEP: 691400 - loss: 0.008368476925533695\n",
      "STEP: 691500 - loss: 0.00836755794296384\n",
      "STEP: 691600 - loss: 0.008366639233051452\n",
      "STEP: 691700 - loss: 0.008365720795713707\n",
      "STEP: 691800 - loss: 0.008364802630867845\n",
      "STEP: 691900 - loss: 0.008363884738430945\n",
      "STEP: 692000 - loss: 0.008362967118320502\n",
      "STEP: 692100 - loss: 0.008362049770452943\n",
      "STEP: 692200 - loss: 0.00836113269474605\n",
      "STEP: 692300 - loss: 0.008360215891117026\n",
      "STEP: 692400 - loss: 0.008359299359482587\n",
      "STEP: 692500 - loss: 0.00835838309976095\n",
      "STEP: 692600 - loss: 0.00835746711186853\n",
      "STEP: 692700 - loss: 0.00835655139572319\n",
      "STEP: 692800 - loss: 0.008355635951242288\n",
      "STEP: 692900 - loss: 0.008354720778342742\n",
      "STEP: 693000 - loss: 0.00835380587694243\n",
      "STEP: 693100 - loss: 0.008352891246958769\n",
      "STEP: 693200 - loss: 0.008351976888308925\n",
      "STEP: 693300 - loss: 0.008351062800910546\n",
      "STEP: 693400 - loss: 0.008350148984681343\n",
      "STEP: 693500 - loss: 0.00834923543953838\n",
      "STEP: 693600 - loss: 0.008348322165399626\n",
      "STEP: 693700 - loss: 0.00834740916218254\n",
      "STEP: 693800 - loss: 0.00834649642980468\n",
      "STEP: 693900 - loss: 0.008345583968183832\n",
      "STEP: 694000 - loss: 0.00834467177723752\n",
      "STEP: 694100 - loss: 0.00834375985688332\n",
      "STEP: 694200 - loss: 0.008342848207039018\n",
      "STEP: 694300 - loss: 0.00834193682762267\n",
      "STEP: 694400 - loss: 0.008341025718551534\n",
      "STEP: 694500 - loss: 0.008340114879743976\n",
      "STEP: 694600 - loss: 0.008339204311117317\n",
      "STEP: 694700 - loss: 0.008338294012589509\n",
      "STEP: 694800 - loss: 0.008337383984078307\n",
      "STEP: 694900 - loss: 0.008336474225502013\n",
      "STEP: 695000 - loss: 0.008335564736778201\n",
      "STEP: 695100 - loss: 0.008334655517824741\n",
      "STEP: 695200 - loss: 0.00833374656855986\n",
      "STEP: 695300 - loss: 0.008332837888901296\n",
      "STEP: 695400 - loss: 0.008331929478767321\n",
      "STEP: 695500 - loss: 0.008331021338075681\n",
      "STEP: 695600 - loss: 0.0083301134667448\n",
      "STEP: 695700 - loss: 0.008329205864692352\n",
      "STEP: 695800 - loss: 0.0083282985318368\n",
      "STEP: 695900 - loss: 0.008327391468095817\n",
      "STEP: 696000 - loss: 0.008326484673388194\n",
      "STEP: 696100 - loss: 0.008325578147631471\n",
      "STEP: 696200 - loss: 0.008324671890744512\n",
      "STEP: 696300 - loss: 0.008323765902645035\n",
      "STEP: 696400 - loss: 0.008322860183251568\n",
      "STEP: 696500 - loss: 0.00832195473248209\n",
      "STEP: 696600 - loss: 0.008321049550255335\n",
      "STEP: 696700 - loss: 0.008320144636489547\n",
      "STEP: 696800 - loss: 0.008319239991102834\n",
      "STEP: 696900 - loss: 0.008318335614013591\n",
      "STEP: 697000 - loss: 0.008317431505140398\n",
      "STEP: 697100 - loss: 0.008316527664401694\n",
      "STEP: 697200 - loss: 0.008315624091716116\n",
      "STEP: 697300 - loss: 0.008314720787001604\n",
      "STEP: 697400 - loss: 0.008313817750177012\n",
      "STEP: 697500 - loss: 0.008312914981160987\n",
      "STEP: 697600 - loss: 0.008312012479871848\n",
      "STEP: 697700 - loss: 0.008311110246228383\n",
      "STEP: 697800 - loss: 0.008310208280148959\n",
      "STEP: 697900 - loss: 0.008309306581552404\n",
      "STEP: 698000 - loss: 0.00830840515035729\n",
      "STEP: 698100 - loss: 0.008307503986482221\n",
      "STEP: 698200 - loss: 0.008306603089845994\n",
      "STEP: 698300 - loss: 0.008305702460367652\n",
      "STEP: 698400 - loss: 0.008304802097965292\n",
      "STEP: 698500 - loss: 0.008303902002558282\n",
      "STEP: 698600 - loss: 0.008303002174065127\n",
      "STEP: 698700 - loss: 0.00830210261240468\n",
      "STEP: 698800 - loss: 0.008301203317495866\n",
      "STEP: 698900 - loss: 0.008300304289257597\n",
      "STEP: 699000 - loss: 0.008299405527608616\n",
      "STEP: 699100 - loss: 0.008298507032467985\n",
      "STEP: 699200 - loss: 0.008297608803754544\n",
      "STEP: 699300 - loss: 0.008296710841387325\n",
      "STEP: 699400 - loss: 0.008295813145285515\n",
      "STEP: 699500 - loss: 0.008294915715368015\n",
      "STEP: 699600 - loss: 0.00829401855155376\n",
      "STEP: 699700 - loss: 0.008293121653761984\n",
      "STEP: 699800 - loss: 0.008292225021911683\n",
      "STEP: 699900 - loss: 0.008291328655921932\n",
      "STEP: 700000 - loss: 0.00829043255571206\n",
      "STEP: 700100 - loss: 0.008289536721201137\n",
      "STEP: 700200 - loss: 0.008288641152308202\n",
      "STEP: 700300 - loss: 0.008287745848953\n",
      "STEP: 700400 - loss: 0.008286850811054083\n",
      "STEP: 700500 - loss: 0.008285956038531259\n",
      "STEP: 700600 - loss: 0.008285061531303808\n",
      "STEP: 700700 - loss: 0.00828416728929056\n",
      "STEP: 700800 - loss: 0.008283273312411295\n",
      "STEP: 700900 - loss: 0.008282379600585253\n",
      "STEP: 701000 - loss: 0.00828148615373193\n",
      "STEP: 701100 - loss: 0.008280592971770695\n",
      "STEP: 701200 - loss: 0.008279700054620788\n",
      "STEP: 701300 - loss: 0.008278807402201946\n",
      "STEP: 701400 - loss: 0.008277915014433711\n",
      "STEP: 701500 - loss: 0.008277022891235306\n",
      "STEP: 701600 - loss: 0.008276131032526415\n",
      "STEP: 701700 - loss: 0.008275239438226875\n",
      "STEP: 701800 - loss: 0.008274348108255686\n",
      "STEP: 701900 - loss: 0.00827345704253304\n",
      "STEP: 702000 - loss: 0.008272566240978284\n",
      "STEP: 702100 - loss: 0.008271675703511241\n",
      "STEP: 702200 - loss: 0.00827078543005123\n",
      "STEP: 702300 - loss: 0.008269895420518605\n",
      "STEP: 702400 - loss: 0.008269005674832425\n",
      "STEP: 702500 - loss: 0.00826811619291291\n",
      "STEP: 702600 - loss: 0.00826722697467979\n",
      "STEP: 702700 - loss: 0.008266338020052597\n",
      "STEP: 702800 - loss: 0.008265449328951397\n",
      "STEP: 702900 - loss: 0.00826456090129615\n",
      "STEP: 703000 - loss: 0.008263672737006584\n",
      "STEP: 703100 - loss: 0.008262784836002459\n",
      "STEP: 703200 - loss: 0.008261897198204244\n",
      "STEP: 703300 - loss: 0.008261009823531033\n",
      "STEP: 703400 - loss: 0.008260122711903652\n",
      "STEP: 703500 - loss: 0.008259235863241749\n",
      "STEP: 703600 - loss: 0.008258349277465198\n",
      "STEP: 703700 - loss: 0.008257462954494404\n",
      "STEP: 703800 - loss: 0.00825657689424918\n",
      "STEP: 703900 - loss: 0.008255691096649798\n",
      "STEP: 704000 - loss: 0.008254805561616209\n",
      "STEP: 704100 - loss: 0.008253920289068826\n",
      "STEP: 704200 - loss: 0.008253035278927554\n",
      "STEP: 704300 - loss: 0.008252150531112728\n",
      "STEP: 704400 - loss: 0.008251266045544641\n",
      "STEP: 704500 - loss: 0.008250381822143337\n",
      "STEP: 704600 - loss: 0.008249497860829205\n",
      "STEP: 704700 - loss: 0.0082486141615225\n",
      "STEP: 704800 - loss: 0.00824773072414367\n",
      "STEP: 704900 - loss: 0.008246847548613014\n",
      "STEP: 705000 - loss: 0.008245964634850897\n",
      "STEP: 705100 - loss: 0.00824508198277753\n",
      "STEP: 705200 - loss: 0.008244199592313789\n",
      "STEP: 705300 - loss: 0.0082433174633796\n",
      "STEP: 705400 - loss: 0.0082424355958957\n",
      "STEP: 705500 - loss: 0.008241553989782545\n",
      "STEP: 705600 - loss: 0.008240672644960577\n",
      "STEP: 705700 - loss: 0.008239791561350403\n",
      "STEP: 705800 - loss: 0.00823891073887279\n",
      "STEP: 705900 - loss: 0.008238030177448011\n",
      "STEP: 706000 - loss: 0.008237149876996716\n",
      "STEP: 706100 - loss: 0.008236269837439613\n",
      "STEP: 706200 - loss: 0.008235390058697455\n",
      "STEP: 706300 - loss: 0.008234510540690911\n",
      "STEP: 706400 - loss: 0.008233631283340346\n",
      "STEP: 706500 - loss: 0.008232752286567061\n",
      "STEP: 706600 - loss: 0.008231873550291292\n",
      "STEP: 706700 - loss: 0.008230995074434222\n",
      "STEP: 706800 - loss: 0.008230116858916524\n",
      "STEP: 706900 - loss: 0.00822923890365872\n",
      "STEP: 707000 - loss: 0.008228361208582172\n",
      "STEP: 707100 - loss: 0.008227483773607554\n",
      "STEP: 707200 - loss: 0.008226606598655513\n",
      "STEP: 707300 - loss: 0.008225729683647317\n",
      "STEP: 707400 - loss: 0.00822485302850393\n",
      "STEP: 707500 - loss: 0.008223976633145932\n",
      "STEP: 707600 - loss: 0.008223100497494881\n",
      "STEP: 707700 - loss: 0.008222224621471227\n",
      "STEP: 707800 - loss: 0.008221349004996433\n",
      "STEP: 707900 - loss: 0.008220473647991338\n",
      "STEP: 708000 - loss: 0.008219598550377233\n",
      "STEP: 708100 - loss: 0.008218723712075127\n",
      "STEP: 708200 - loss: 0.008217849133006223\n",
      "STEP: 708300 - loss: 0.008216974813091643\n",
      "STEP: 708400 - loss: 0.008216100752252353\n",
      "STEP: 708500 - loss: 0.008215226950409955\n",
      "STEP: 708600 - loss: 0.008214353407485307\n",
      "STEP: 708700 - loss: 0.00821348012340015\n",
      "STEP: 708800 - loss: 0.008212607098075376\n",
      "STEP: 708900 - loss: 0.008211734331432363\n",
      "STEP: 709000 - loss: 0.008210861823392548\n",
      "STEP: 709100 - loss: 0.00820998957387749\n",
      "STEP: 709200 - loss: 0.008209117582807875\n",
      "STEP: 709300 - loss: 0.008208245850105865\n",
      "STEP: 709400 - loss: 0.008207374375692559\n",
      "STEP: 709500 - loss: 0.00820650315948935\n",
      "STEP: 709600 - loss: 0.008205632201417724\n",
      "STEP: 709700 - loss: 0.008204761501399536\n",
      "STEP: 709800 - loss: 0.008203891059355945\n",
      "STEP: 709900 - loss: 0.008203020875208527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 710000 - loss: 0.008202150948878837\n",
      "STEP: 710100 - loss: 0.008201281280288592\n",
      "STEP: 710200 - loss: 0.008200411869359594\n",
      "STEP: 710300 - loss: 0.008199542716013093\n",
      "STEP: 710400 - loss: 0.008198673820171008\n",
      "STEP: 710500 - loss: 0.00819780518175479\n",
      "STEP: 710600 - loss: 0.008196936800686647\n",
      "STEP: 710700 - loss: 0.008196068676887727\n",
      "STEP: 710800 - loss: 0.008195200810280212\n",
      "STEP: 710900 - loss: 0.008194333200785471\n",
      "STEP: 711000 - loss: 0.008193465848326033\n",
      "STEP: 711100 - loss: 0.008192598752822938\n",
      "STEP: 711200 - loss: 0.008191731914198681\n",
      "STEP: 711300 - loss: 0.008190865332374446\n",
      "STEP: 711400 - loss: 0.00818999900727287\n",
      "STEP: 711500 - loss: 0.008189132938815678\n",
      "STEP: 711600 - loss: 0.00818826712692443\n",
      "STEP: 711700 - loss: 0.008187401571521553\n",
      "STEP: 711800 - loss: 0.008186536272528844\n",
      "STEP: 711900 - loss: 0.008185671229868635\n",
      "STEP: 712000 - loss: 0.008184806443462426\n",
      "STEP: 712100 - loss: 0.008183941913232327\n",
      "STEP: 712200 - loss: 0.008183077639101116\n",
      "STEP: 712300 - loss: 0.00818221362099023\n",
      "STEP: 712400 - loss: 0.008181349858822294\n",
      "STEP: 712500 - loss: 0.008180486352519018\n",
      "STEP: 712600 - loss: 0.008179623102003018\n",
      "STEP: 712700 - loss: 0.00817876010719622\n",
      "STEP: 712800 - loss: 0.008177897368021011\n",
      "STEP: 712900 - loss: 0.008177034884399456\n",
      "STEP: 713000 - loss: 0.00817617265625417\n",
      "STEP: 713100 - loss: 0.008175310683507252\n",
      "STEP: 713200 - loss: 0.008174448966080881\n",
      "STEP: 713300 - loss: 0.008173587503897614\n",
      "STEP: 713400 - loss: 0.008172726296880041\n",
      "STEP: 713500 - loss: 0.008171865344950149\n",
      "STEP: 713600 - loss: 0.008171004648030611\n",
      "STEP: 713700 - loss: 0.008170144206044\n",
      "STEP: 713800 - loss: 0.00816928401891237\n",
      "STEP: 713900 - loss: 0.00816842408655842\n",
      "STEP: 714000 - loss: 0.008167564408904933\n",
      "STEP: 714100 - loss: 0.008166704985874011\n",
      "STEP: 714200 - loss: 0.008165845817388448\n",
      "STEP: 714300 - loss: 0.008164986903371155\n",
      "STEP: 714400 - loss: 0.008164128243744122\n",
      "STEP: 714500 - loss: 0.008163269838430371\n",
      "STEP: 714600 - loss: 0.008162411687352598\n",
      "STEP: 714700 - loss: 0.008161553790433351\n",
      "STEP: 714800 - loss: 0.008160696147595384\n",
      "STEP: 714900 - loss: 0.008159838758761338\n",
      "STEP: 715000 - loss: 0.008158981623854016\n",
      "STEP: 715100 - loss: 0.0081581247427963\n",
      "STEP: 715200 - loss: 0.008157268115510784\n",
      "STEP: 715300 - loss: 0.008156411741920711\n",
      "STEP: 715400 - loss: 0.00815555562194879\n",
      "STEP: 715500 - loss: 0.008154699755517487\n",
      "STEP: 715600 - loss: 0.008153844142550059\n",
      "STEP: 715700 - loss: 0.008152988782969208\n",
      "STEP: 715800 - loss: 0.008152133676698326\n",
      "STEP: 715900 - loss: 0.008151278823659848\n",
      "STEP: 716000 - loss: 0.008150424223776981\n",
      "STEP: 716100 - loss: 0.008149569876972976\n",
      "STEP: 716200 - loss: 0.00814871578317056\n",
      "STEP: 716300 - loss: 0.008147861942292839\n",
      "STEP: 716400 - loss: 0.00814700835426306\n",
      "STEP: 716500 - loss: 0.008146155019004186\n",
      "STEP: 716600 - loss: 0.008145301936439354\n",
      "STEP: 716700 - loss: 0.008144449106491796\n",
      "STEP: 716800 - loss: 0.008143596529084533\n",
      "STEP: 716900 - loss: 0.008142744204141022\n",
      "STEP: 717000 - loss: 0.0081418921315843\n",
      "STEP: 717100 - loss: 0.008141040311337677\n",
      "STEP: 717200 - loss: 0.00814018874332446\n",
      "STEP: 717300 - loss: 0.008139337427467913\n",
      "STEP: 717400 - loss: 0.008138486363691287\n",
      "STEP: 717500 - loss: 0.008137635551918036\n",
      "STEP: 717600 - loss: 0.008136784992071246\n",
      "STEP: 717700 - loss: 0.008135934684074872\n",
      "STEP: 717800 - loss: 0.00813508462785192\n",
      "STEP: 717900 - loss: 0.008134234823325873\n",
      "STEP: 718000 - loss: 0.008133385270420186\n",
      "STEP: 718100 - loss: 0.008132535969058339\n",
      "STEP: 718200 - loss: 0.008131686919163914\n",
      "STEP: 718300 - loss: 0.008130838120660426\n",
      "STEP: 718400 - loss: 0.008129989573471389\n",
      "STEP: 718500 - loss: 0.008129141277520327\n",
      "STEP: 718600 - loss: 0.008128293232731006\n",
      "STEP: 718700 - loss: 0.008127445439026802\n",
      "STEP: 718800 - loss: 0.008126597896331744\n",
      "STEP: 718900 - loss: 0.008125750604568969\n",
      "STEP: 719000 - loss: 0.008124903563662666\n",
      "STEP: 719100 - loss: 0.008124056773536177\n",
      "STEP: 719200 - loss: 0.008123210234113555\n",
      "STEP: 719300 - loss: 0.008122363945318477\n",
      "STEP: 719400 - loss: 0.00812151790707442\n",
      "STEP: 719500 - loss: 0.008120672119305564\n",
      "STEP: 719600 - loss: 0.008119826581935734\n",
      "STEP: 719700 - loss: 0.008118981294888592\n",
      "STEP: 719800 - loss: 0.008118136258087992\n",
      "STEP: 719900 - loss: 0.008117291471458041\n",
      "STEP: 720000 - loss: 0.008116446934922465\n",
      "STEP: 720100 - loss: 0.008115602648405336\n",
      "STEP: 720200 - loss: 0.008114758611830664\n",
      "STEP: 720300 - loss: 0.008113914825122403\n",
      "STEP: 720400 - loss: 0.008113071288204357\n",
      "STEP: 720500 - loss: 0.008112228001000798\n",
      "STEP: 720600 - loss: 0.008111384963435758\n",
      "STEP: 720700 - loss: 0.008110542175433338\n",
      "STEP: 720800 - loss: 0.008109699636917626\n",
      "STEP: 720900 - loss: 0.008108857347812626\n",
      "STEP: 721000 - loss: 0.008108015308042749\n",
      "STEP: 721100 - loss: 0.008107173517531919\n",
      "STEP: 721200 - loss: 0.008106331976204428\n",
      "STEP: 721300 - loss: 0.008105490683984732\n",
      "STEP: 721400 - loss: 0.00810464964079661\n",
      "STEP: 721500 - loss: 0.00810380884656451\n",
      "STEP: 721600 - loss: 0.008102968301213024\n",
      "STEP: 721700 - loss: 0.008102128004666127\n",
      "STEP: 721800 - loss: 0.00810128795684835\n",
      "STEP: 721900 - loss: 0.008100448157683906\n",
      "STEP: 722000 - loss: 0.00809960860709745\n",
      "STEP: 722100 - loss: 0.008098769305012894\n",
      "STEP: 722200 - loss: 0.008097930251355117\n",
      "STEP: 722300 - loss: 0.008097091446048603\n",
      "STEP: 722400 - loss: 0.008096252889017528\n",
      "STEP: 722500 - loss: 0.008095414580186578\n",
      "STEP: 722600 - loss: 0.008094576519480133\n",
      "STEP: 722700 - loss: 0.008093738706822763\n",
      "STEP: 722800 - loss: 0.008092901142139283\n",
      "STEP: 722900 - loss: 0.008092063825354073\n",
      "STEP: 723000 - loss: 0.008091226756391593\n",
      "STEP: 723100 - loss: 0.008090389935176766\n",
      "STEP: 723200 - loss: 0.008089553361634338\n",
      "STEP: 723300 - loss: 0.008088717035688574\n",
      "STEP: 723400 - loss: 0.008087880957264478\n",
      "STEP: 723500 - loss: 0.008087045126286795\n",
      "STEP: 723600 - loss: 0.008086209542680084\n",
      "STEP: 723700 - loss: 0.008085374206369247\n",
      "STEP: 723800 - loss: 0.008084539117279176\n",
      "STEP: 723900 - loss: 0.008083704275334525\n",
      "STEP: 724000 - loss: 0.008082869680460413\n",
      "STEP: 724100 - loss: 0.008082035332581235\n",
      "STEP: 724200 - loss: 0.008081201231622264\n",
      "STEP: 724300 - loss: 0.008080367377508148\n",
      "STEP: 724400 - loss: 0.008079533770164122\n",
      "STEP: 724500 - loss: 0.008078700409514989\n",
      "STEP: 724600 - loss: 0.00807786729548564\n",
      "STEP: 724700 - loss: 0.00807703442800114\n",
      "STEP: 724800 - loss: 0.008076201806986805\n",
      "STEP: 724900 - loss: 0.008075369432367046\n",
      "STEP: 725000 - loss: 0.008074537304067597\n",
      "STEP: 725100 - loss: 0.008073705422013234\n",
      "STEP: 725200 - loss: 0.00807287378612907\n",
      "STEP: 725300 - loss: 0.008072042396340275\n",
      "STEP: 725400 - loss: 0.008071211252572093\n",
      "STEP: 725500 - loss: 0.008070380354749513\n",
      "STEP: 725600 - loss: 0.008069549702798042\n",
      "STEP: 725700 - loss: 0.00806871929664265\n",
      "STEP: 725800 - loss: 0.008067889136208772\n",
      "STEP: 725900 - loss: 0.008067059221421414\n",
      "STEP: 726000 - loss: 0.008066229552206242\n",
      "STEP: 726100 - loss: 0.008065400128488323\n",
      "STEP: 726200 - loss: 0.00806457095019317\n",
      "STEP: 726300 - loss: 0.008063742017245948\n",
      "STEP: 726400 - loss: 0.008062913329572274\n",
      "STEP: 726500 - loss: 0.008062084887097452\n",
      "STEP: 726600 - loss: 0.008061256689746811\n",
      "STEP: 726700 - loss: 0.008060428737446126\n",
      "STEP: 726800 - loss: 0.008059601030120558\n",
      "STEP: 726900 - loss: 0.008058773567695677\n",
      "STEP: 727000 - loss: 0.008057946350096977\n",
      "STEP: 727100 - loss: 0.008057119377250272\n",
      "STEP: 727200 - loss: 0.008056292649080721\n",
      "STEP: 727300 - loss: 0.008055466165514464\n",
      "STEP: 727400 - loss: 0.008054639926476533\n",
      "STEP: 727500 - loss: 0.008053813931893016\n",
      "STEP: 727600 - loss: 0.008052988181689251\n",
      "STEP: 727700 - loss: 0.008052162675791282\n",
      "STEP: 727800 - loss: 0.008051337414124528\n",
      "STEP: 727900 - loss: 0.008050512396614673\n",
      "STEP: 728000 - loss: 0.0080496876231878\n",
      "STEP: 728100 - loss: 0.008048863093769244\n",
      "STEP: 728200 - loss: 0.008048038808285165\n",
      "STEP: 728300 - loss: 0.008047214766661353\n",
      "STEP: 728400 - loss: 0.008046390968823428\n",
      "STEP: 728500 - loss: 0.008045567414697405\n",
      "STEP: 728600 - loss: 0.008044744104209415\n",
      "STEP: 728700 - loss: 0.00804392103728471\n",
      "STEP: 728800 - loss: 0.008043098213849896\n",
      "STEP: 728900 - loss: 0.008042275633830443\n",
      "STEP: 729000 - loss: 0.008041453297152763\n",
      "STEP: 729100 - loss: 0.008040631203742559\n",
      "STEP: 729200 - loss: 0.008039809353526026\n",
      "STEP: 729300 - loss: 0.008038987746428871\n",
      "STEP: 729400 - loss: 0.008038166382377664\n",
      "STEP: 729500 - loss: 0.008037345261298224\n",
      "STEP: 729600 - loss: 0.008036524383116635\n",
      "STEP: 729700 - loss: 0.00803570374775899\n",
      "STEP: 729800 - loss: 0.008034883355151689\n",
      "STEP: 729900 - loss: 0.008034063205220841\n",
      "STEP: 730000 - loss: 0.008033243297892432\n",
      "STEP: 730100 - loss: 0.008032423633092804\n",
      "STEP: 730200 - loss: 0.00803160421074821\n",
      "STEP: 730300 - loss: 0.008030785030785097\n",
      "STEP: 730400 - loss: 0.00802996609312958\n",
      "STEP: 730500 - loss: 0.008029147397708014\n",
      "STEP: 730600 - loss: 0.008028328944446732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 730700 - loss: 0.008027510733272045\n",
      "STEP: 730800 - loss: 0.00802669276411046\n",
      "STEP: 730900 - loss: 0.008025875036888187\n",
      "STEP: 731000 - loss: 0.008025057551531805\n",
      "STEP: 731100 - loss: 0.008024240307967802\n",
      "STEP: 731200 - loss: 0.008023423306122397\n",
      "STEP: 731300 - loss: 0.008022606545922571\n",
      "STEP: 731400 - loss: 0.0080217900272944\n",
      "STEP: 731500 - loss: 0.008020973750164582\n",
      "STEP: 731600 - loss: 0.008020157714459577\n",
      "STEP: 731700 - loss: 0.008019341920106119\n",
      "STEP: 731800 - loss: 0.008018526367030787\n",
      "STEP: 731900 - loss: 0.008017711055160054\n",
      "STEP: 732000 - loss: 0.008016895984420975\n",
      "STEP: 732100 - loss: 0.008016081154739695\n",
      "STEP: 732200 - loss: 0.008015266566043107\n",
      "STEP: 732300 - loss: 0.008014452218258096\n",
      "STEP: 732400 - loss: 0.008013638111311233\n",
      "STEP: 732500 - loss: 0.00801282424512961\n",
      "STEP: 732600 - loss: 0.008012010619639461\n",
      "STEP: 732700 - loss: 0.008011197234768062\n",
      "STEP: 732800 - loss: 0.008010384090441837\n",
      "STEP: 732900 - loss: 0.008009571186587927\n",
      "STEP: 733000 - loss: 0.008008758523133241\n",
      "STEP: 733100 - loss: 0.00800794610000439\n",
      "STEP: 733200 - loss: 0.00800713391712869\n",
      "STEP: 733300 - loss: 0.00800632197443262\n",
      "STEP: 733400 - loss: 0.008005510271843468\n",
      "STEP: 733500 - loss: 0.008004698809288228\n",
      "STEP: 733600 - loss: 0.008003887586693782\n",
      "STEP: 733700 - loss: 0.00800307660398731\n",
      "STEP: 733800 - loss: 0.008002265861095651\n",
      "STEP: 733900 - loss: 0.008001455357945848\n",
      "STEP: 734000 - loss: 0.008000645094465141\n",
      "STEP: 734100 - loss: 0.007999835070580934\n",
      "STEP: 734200 - loss: 0.00799902528621984\n",
      "STEP: 734300 - loss: 0.007998215741309257\n",
      "STEP: 734400 - loss: 0.007997406435776535\n",
      "STEP: 734500 - loss: 0.007996597369548528\n",
      "STEP: 734600 - loss: 0.007995788542552697\n",
      "STEP: 734700 - loss: 0.007994979954716314\n",
      "STEP: 734800 - loss: 0.007994171605966542\n",
      "STEP: 734900 - loss: 0.007993363496230659\n",
      "STEP: 735000 - loss: 0.007992555625436416\n",
      "STEP: 735100 - loss: 0.007991747993510377\n",
      "STEP: 735200 - loss: 0.007990940600380405\n",
      "STEP: 735300 - loss: 0.0079901334459738\n",
      "STEP: 735400 - loss: 0.007989326530217961\n",
      "STEP: 735500 - loss: 0.007988519853040367\n",
      "STEP: 735600 - loss: 0.007987713414368455\n",
      "STEP: 735700 - loss: 0.00798690721412955\n",
      "STEP: 735800 - loss: 0.007986101252251413\n",
      "STEP: 735900 - loss: 0.007985295528661234\n",
      "STEP: 736000 - loss: 0.007984490043286894\n",
      "STEP: 736100 - loss: 0.007983684796055705\n",
      "STEP: 736200 - loss: 0.007982879786895226\n",
      "STEP: 736300 - loss: 0.007982075015733238\n",
      "STEP: 736400 - loss: 0.007981270482497287\n",
      "STEP: 736500 - loss: 0.007980466187115105\n",
      "STEP: 736600 - loss: 0.007979662129514387\n",
      "STEP: 736700 - loss: 0.007978858309622723\n",
      "STEP: 736800 - loss: 0.007978054727367654\n",
      "STEP: 736900 - loss: 0.007977251382677141\n",
      "STEP: 737000 - loss: 0.007976448275479013\n",
      "STEP: 737100 - loss: 0.007975645405700782\n",
      "STEP: 737200 - loss: 0.007974842773270503\n",
      "STEP: 737300 - loss: 0.00797404037811596\n",
      "STEP: 737400 - loss: 0.007973238220164835\n",
      "STEP: 737500 - loss: 0.007972436299345045\n",
      "STEP: 737600 - loss: 0.00797163461558478\n",
      "STEP: 737700 - loss: 0.007970833168811618\n",
      "STEP: 737800 - loss: 0.00797003195895356\n",
      "STEP: 737900 - loss: 0.007969230985938514\n",
      "STEP: 738000 - loss: 0.007968430249694888\n",
      "STEP: 738100 - loss: 0.007967629750150024\n",
      "STEP: 738200 - loss: 0.007966829487232232\n",
      "STEP: 738300 - loss: 0.007966029460869651\n",
      "STEP: 738400 - loss: 0.00796522967099024\n",
      "STEP: 738500 - loss: 0.007964430117522181\n",
      "STEP: 738600 - loss: 0.007963630800393782\n",
      "STEP: 738700 - loss: 0.00796283171953258\n",
      "STEP: 738800 - loss: 0.007962032874867239\n",
      "STEP: 738900 - loss: 0.007961234266325664\n",
      "STEP: 739000 - loss: 0.007960435893836205\n",
      "STEP: 739100 - loss: 0.007959637757326912\n",
      "STEP: 739200 - loss: 0.007958839856726477\n",
      "STEP: 739300 - loss: 0.00795804219196245\n",
      "STEP: 739400 - loss: 0.007957244762963726\n",
      "STEP: 739500 - loss: 0.007956447569658267\n",
      "STEP: 739600 - loss: 0.007955650611974587\n",
      "STEP: 739700 - loss: 0.00795485388984091\n",
      "STEP: 739800 - loss: 0.007954057403185676\n",
      "STEP: 739900 - loss: 0.007953261151937464\n",
      "STEP: 740000 - loss: 0.00795246513602421\n",
      "STEP: 740100 - loss: 0.007951669355374908\n",
      "STEP: 740200 - loss: 0.007950873809917505\n",
      "STEP: 740300 - loss: 0.007950078499580767\n",
      "STEP: 740400 - loss: 0.007949283424293246\n",
      "STEP: 740500 - loss: 0.00794848858398336\n",
      "STEP: 740600 - loss: 0.007947693978579643\n",
      "STEP: 740700 - loss: 0.007946899608010597\n",
      "STEP: 740800 - loss: 0.007946105472205063\n",
      "STEP: 740900 - loss: 0.007945311571091417\n",
      "STEP: 741000 - loss: 0.007944517904598438\n",
      "STEP: 741100 - loss: 0.0079437244726545\n",
      "STEP: 741200 - loss: 0.007942931275188605\n",
      "STEP: 741300 - loss: 0.007942138312129497\n",
      "STEP: 741400 - loss: 0.007941345583405594\n",
      "STEP: 741500 - loss: 0.007940553088945746\n",
      "STEP: 741600 - loss: 0.007939760828678718\n",
      "STEP: 741700 - loss: 0.007938968802533431\n",
      "STEP: 741800 - loss: 0.007938177010438585\n",
      "STEP: 741900 - loss: 0.007937385452322986\n",
      "STEP: 742000 - loss: 0.007936594128115482\n",
      "STEP: 742100 - loss: 0.007935803037745012\n",
      "STEP: 742200 - loss: 0.007935012181140334\n",
      "STEP: 742300 - loss: 0.007934221558230545\n",
      "STEP: 742400 - loss: 0.007933431168944616\n",
      "STEP: 742500 - loss: 0.007932641013211195\n",
      "STEP: 742600 - loss: 0.007931851090959563\n",
      "STEP: 742700 - loss: 0.007931061402118592\n",
      "STEP: 742800 - loss: 0.007930271946617325\n",
      "STEP: 742900 - loss: 0.007929482724384724\n",
      "STEP: 743000 - loss: 0.007928693735350194\n",
      "STEP: 743100 - loss: 0.007927904979442155\n",
      "STEP: 743200 - loss: 0.007927116456590354\n",
      "STEP: 743300 - loss: 0.007926328166723602\n",
      "STEP: 743400 - loss: 0.00792554010977132\n",
      "STEP: 743500 - loss: 0.00792475228566239\n",
      "STEP: 743600 - loss: 0.007923964694326015\n",
      "STEP: 743700 - loss: 0.007923177335691653\n",
      "STEP: 743800 - loss: 0.007922390209688285\n",
      "STEP: 743900 - loss: 0.007921603316245512\n",
      "STEP: 744000 - loss: 0.007920816655292286\n",
      "STEP: 744100 - loss: 0.007920030226757985\n",
      "STEP: 744200 - loss: 0.007919244030571974\n",
      "STEP: 744300 - loss: 0.007918458066663595\n",
      "STEP: 744400 - loss: 0.007917672334962485\n",
      "STEP: 744500 - loss: 0.007916886835397288\n",
      "STEP: 744600 - loss: 0.007916101567898265\n",
      "STEP: 744700 - loss: 0.007915316532394318\n",
      "STEP: 744800 - loss: 0.007914531728815165\n",
      "STEP: 744900 - loss: 0.007913747157090135\n",
      "STEP: 745000 - loss: 0.007912962817148657\n",
      "STEP: 745100 - loss: 0.007912178708920475\n",
      "STEP: 745200 - loss: 0.007911394832335\n",
      "STEP: 745300 - loss: 0.0079106111873217\n",
      "STEP: 745400 - loss: 0.007909827773810338\n",
      "STEP: 745500 - loss: 0.00790904459173048\n",
      "STEP: 745600 - loss: 0.007908261641011648\n",
      "STEP: 745700 - loss: 0.007907478921583739\n",
      "STEP: 745800 - loss: 0.007906696433376015\n",
      "STEP: 745900 - loss: 0.007905914176318484\n",
      "STEP: 746000 - loss: 0.007905132150340624\n",
      "STEP: 746100 - loss: 0.007904350355372337\n",
      "STEP: 746200 - loss: 0.007903568791343456\n",
      "STEP: 746300 - loss: 0.007902787458183668\n",
      "STEP: 746400 - loss: 0.00790200635582258\n",
      "STEP: 746500 - loss: 0.007901225484190264\n",
      "STEP: 746600 - loss: 0.007900444843216356\n",
      "STEP: 746700 - loss: 0.007899664432830994\n",
      "STEP: 746800 - loss: 0.007898884252963968\n",
      "STEP: 746900 - loss: 0.007898104303544939\n",
      "STEP: 747000 - loss: 0.007897324584503883\n",
      "STEP: 747100 - loss: 0.007896545095771021\n",
      "STEP: 747200 - loss: 0.007895765837276296\n",
      "STEP: 747300 - loss: 0.00789498680894923\n",
      "STEP: 747400 - loss: 0.007894208010720355\n",
      "STEP: 747500 - loss: 0.00789342944251943\n",
      "STEP: 747600 - loss: 0.007892651104276628\n",
      "STEP: 747700 - loss: 0.007891872995922023\n",
      "STEP: 747800 - loss: 0.007891095117385737\n",
      "STEP: 747900 - loss: 0.007890317468597672\n",
      "STEP: 748000 - loss: 0.007889540049488077\n",
      "STEP: 748100 - loss: 0.00788876285998746\n",
      "STEP: 748200 - loss: 0.007887985900025289\n",
      "STEP: 748300 - loss: 0.007887209169532429\n",
      "STEP: 748400 - loss: 0.007886432668438583\n",
      "STEP: 748500 - loss: 0.007885656396674543\n",
      "STEP: 748600 - loss: 0.007884880354170207\n",
      "STEP: 748700 - loss: 0.007884104540855663\n",
      "STEP: 748800 - loss: 0.007883328956661592\n",
      "STEP: 748900 - loss: 0.007882553601518343\n",
      "STEP: 749000 - loss: 0.007881778475356139\n",
      "STEP: 749100 - loss: 0.0078810035781053\n",
      "STEP: 749200 - loss: 0.007880228909696283\n",
      "STEP: 749300 - loss: 0.007879454470059537\n",
      "STEP: 749400 - loss: 0.007878680259125203\n",
      "STEP: 749500 - loss: 0.007877906276824193\n",
      "STEP: 749600 - loss: 0.007877132523086746\n",
      "STEP: 749700 - loss: 0.007876358997843369\n",
      "STEP: 749800 - loss: 0.007875585701024573\n",
      "STEP: 749900 - loss: 0.007874812632560884\n",
      "STEP: 750000 - loss: 0.007874039792383046\n",
      "STEP: 750100 - loss: 0.0078732671804215\n",
      "STEP: 750200 - loss: 0.00787249479660667\n",
      "STEP: 750300 - loss: 0.007871722640869322\n",
      "STEP: 750400 - loss: 0.007870950713140317\n",
      "STEP: 750500 - loss: 0.007870179013350144\n",
      "STEP: 750600 - loss: 0.007869407541429306\n",
      "STEP: 750700 - loss: 0.007868636297308733\n",
      "STEP: 750800 - loss: 0.007867865280918877\n",
      "STEP: 750900 - loss: 0.007867094492191166\n",
      "STEP: 751000 - loss: 0.007866323931055892\n",
      "STEP: 751100 - loss: 0.007865553597443875\n",
      "STEP: 751200 - loss: 0.007864783491285793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 751300 - loss: 0.007864013612512617\n",
      "STEP: 751400 - loss: 0.007863243961055187\n",
      "STEP: 751500 - loss: 0.007862474536844593\n",
      "STEP: 751600 - loss: 0.007861705339811377\n",
      "STEP: 751700 - loss: 0.007860936369886811\n",
      "STEP: 751800 - loss: 0.007860167627001424\n",
      "STEP: 751900 - loss: 0.007859399111086518\n",
      "STEP: 752000 - loss: 0.007858630822073078\n",
      "STEP: 752100 - loss: 0.007857862759891794\n",
      "STEP: 752200 - loss: 0.007857094924473998\n",
      "STEP: 752300 - loss: 0.007856327315750622\n",
      "STEP: 752400 - loss: 0.007855559933652877\n",
      "STEP: 752500 - loss: 0.007854792778111571\n",
      "STEP: 752600 - loss: 0.007854025849058017\n",
      "STEP: 752700 - loss: 0.007853259146423052\n",
      "STEP: 752800 - loss: 0.007852492670138381\n",
      "STEP: 752900 - loss: 0.007851726420134719\n",
      "STEP: 753000 - loss: 0.007850960396343209\n",
      "STEP: 753100 - loss: 0.007850194598695395\n",
      "STEP: 753200 - loss: 0.007849429027122544\n",
      "STEP: 753300 - loss: 0.007848663681555478\n",
      "STEP: 753400 - loss: 0.007847898561925717\n",
      "STEP: 753500 - loss: 0.007847133668164629\n",
      "STEP: 753600 - loss: 0.007846369000203638\n",
      "STEP: 753700 - loss: 0.007845604557973763\n",
      "STEP: 753800 - loss: 0.007844840341406444\n",
      "STEP: 753900 - loss: 0.007844076350433242\n",
      "STEP: 754000 - loss: 0.007843312584985395\n",
      "STEP: 754100 - loss: 0.007842549044994387\n",
      "STEP: 754200 - loss: 0.007841785730391484\n",
      "STEP: 754300 - loss: 0.00784102264110855\n",
      "STEP: 754400 - loss: 0.007840259777076659\n",
      "STEP: 754500 - loss: 0.007839497138227654\n",
      "STEP: 754600 - loss: 0.007838734724492782\n",
      "STEP: 754700 - loss: 0.007837972535803723\n",
      "STEP: 754800 - loss: 0.007837210572092062\n",
      "STEP: 754900 - loss: 0.007836448833289406\n",
      "STEP: 755000 - loss: 0.007835687319327215\n",
      "STEP: 755100 - loss: 0.007834926030137288\n",
      "STEP: 755200 - loss: 0.007834164965651126\n",
      "STEP: 755300 - loss: 0.007833404125800447\n",
      "STEP: 755400 - loss: 0.007832643510516938\n",
      "STEP: 755500 - loss: 0.007831883119732638\n",
      "STEP: 755600 - loss: 0.007831122953378736\n",
      "STEP: 755700 - loss: 0.007830363011387367\n",
      "STEP: 755800 - loss: 0.007829603293690103\n",
      "STEP: 755900 - loss: 0.00782884380021896\n",
      "STEP: 756000 - loss: 0.007828084530905378\n",
      "STEP: 756100 - loss: 0.007827325485681592\n",
      "STEP: 756200 - loss: 0.00782656666447947\n",
      "STEP: 756300 - loss: 0.007825808067230563\n",
      "STEP: 756400 - loss: 0.007825049693866899\n",
      "STEP: 756500 - loss: 0.007824291544320492\n",
      "STEP: 756600 - loss: 0.00782353361852301\n",
      "STEP: 756700 - loss: 0.00782277591640683\n",
      "STEP: 756800 - loss: 0.007822018437903923\n",
      "STEP: 756900 - loss: 0.007821261182945958\n",
      "STEP: 757000 - loss: 0.007820504151465096\n",
      "STEP: 757100 - loss: 0.007819747343393236\n",
      "STEP: 757200 - loss: 0.0078189907586629\n",
      "STEP: 757300 - loss: 0.007818234397205512\n",
      "STEP: 757400 - loss: 0.007817478258953878\n",
      "STEP: 757500 - loss: 0.007816722343839549\n",
      "STEP: 757600 - loss: 0.007815966651795066\n",
      "STEP: 757700 - loss: 0.007815211182752484\n",
      "STEP: 757800 - loss: 0.007814455936643906\n",
      "STEP: 757900 - loss: 0.007813700913401476\n",
      "STEP: 758000 - loss: 0.007812946112957613\n",
      "STEP: 758100 - loss: 0.007812191535244574\n",
      "STEP: 758200 - loss: 0.007811437180194536\n",
      "STEP: 758300 - loss: 0.00781068304773973\n",
      "STEP: 758400 - loss: 0.007809929137812667\n",
      "STEP: 758500 - loss: 0.007809175450345381\n",
      "STEP: 758600 - loss: 0.007808421985270625\n",
      "STEP: 758700 - loss: 0.007807668742520456\n",
      "STEP: 758800 - loss: 0.007806915722027376\n",
      "STEP: 758900 - loss: 0.00780616292372401\n",
      "STEP: 759000 - loss: 0.007805410347542278\n",
      "STEP: 759100 - loss: 0.007804657993415114\n",
      "STEP: 759200 - loss: 0.007803905861274526\n",
      "STEP: 759300 - loss: 0.007803153951053672\n",
      "STEP: 759400 - loss: 0.007802402262684667\n",
      "STEP: 759500 - loss: 0.007801650796099953\n",
      "STEP: 759600 - loss: 0.0078008995512322665\n",
      "STEP: 759700 - loss: 0.0078001485280142545\n",
      "STEP: 759800 - loss: 0.0077993977263784795\n",
      "STEP: 759900 - loss: 0.007798647146257401\n",
      "STEP: 760000 - loss: 0.007797896787583719\n",
      "STEP: 760100 - loss: 0.007797146650290218\n",
      "STEP: 760200 - loss: 0.007796396734309272\n",
      "STEP: 760300 - loss: 0.007795647039574101\n",
      "STEP: 760400 - loss: 0.007794897566017056\n",
      "STEP: 760500 - loss: 0.0077941483135711784\n",
      "STEP: 760600 - loss: 0.007793399282168575\n",
      "STEP: 760700 - loss: 0.007792650471742796\n",
      "STEP: 760800 - loss: 0.007791901882226229\n",
      "STEP: 760900 - loss: 0.007791153513551738\n",
      "STEP: 761000 - loss: 0.0077904053656523975\n",
      "STEP: 761100 - loss: 0.007789657438460792\n",
      "STEP: 761200 - loss: 0.0077889097319101906\n",
      "STEP: 761300 - loss: 0.007788162245932774\n",
      "STEP: 761400 - loss: 0.007787414980462227\n",
      "STEP: 761500 - loss: 0.007786667935431252\n",
      "STEP: 761600 - loss: 0.007785921110772838\n",
      "STEP: 761700 - loss: 0.007785174506419537\n",
      "STEP: 761800 - loss: 0.007784428122305007\n",
      "STEP: 761900 - loss: 0.007783681958362082\n",
      "STEP: 762000 - loss: 0.007782936014523489\n",
      "STEP: 762100 - loss: 0.00778219029072272\n",
      "STEP: 762200 - loss: 0.007781444786892901\n",
      "STEP: 762300 - loss: 0.007780699502966613\n",
      "STEP: 762400 - loss: 0.007779954438877379\n",
      "STEP: 762500 - loss: 0.007779209594558291\n",
      "STEP: 762600 - loss: 0.0077784649699425435\n",
      "STEP: 762700 - loss: 0.0077777205649632464\n",
      "STEP: 762800 - loss: 0.007776976379553773\n",
      "STEP: 762900 - loss: 0.007776232413646958\n",
      "STEP: 763000 - loss: 0.007775488667176825\n",
      "STEP: 763100 - loss: 0.0077747451400759435\n",
      "STEP: 763200 - loss: 0.007774001832277914\n",
      "STEP: 763300 - loss: 0.007773258743715974\n",
      "STEP: 763400 - loss: 0.007772515874323318\n",
      "STEP: 763500 - loss: 0.007771773224033421\n",
      "STEP: 763600 - loss: 0.007771030792779718\n",
      "STEP: 763700 - loss: 0.007770288580495885\n",
      "STEP: 763800 - loss: 0.007769546587114501\n",
      "STEP: 763900 - loss: 0.007768804812570096\n",
      "STEP: 764000 - loss: 0.007768063256795225\n",
      "STEP: 764100 - loss: 0.007767321919723716\n",
      "STEP: 764200 - loss: 0.00776658080128903\n",
      "STEP: 764300 - loss: 0.007765839901424705\n",
      "STEP: 764400 - loss: 0.007765099220064459\n",
      "STEP: 764500 - loss: 0.007764358757141471\n",
      "STEP: 764600 - loss: 0.007763618512589661\n",
      "STEP: 764700 - loss: 0.007762878486342427\n",
      "STEP: 764800 - loss: 0.007762138678333476\n",
      "STEP: 764900 - loss: 0.007761399088496383\n",
      "STEP: 765000 - loss: 0.007760659716764681\n",
      "STEP: 765100 - loss: 0.007759920563072435\n",
      "STEP: 765200 - loss: 0.007759181627353086\n",
      "STEP: 765300 - loss: 0.007758442909540283\n",
      "STEP: 765400 - loss: 0.0077577044095681075\n",
      "STEP: 765500 - loss: 0.007756966127369813\n",
      "STEP: 765600 - loss: 0.007756228062879557\n",
      "STEP: 765700 - loss: 0.007755490216031081\n",
      "STEP: 765800 - loss: 0.007754752586757963\n",
      "STEP: 765900 - loss: 0.007754015174994471\n",
      "STEP: 766000 - loss: 0.007753277980674047\n",
      "STEP: 766100 - loss: 0.007752541003730758\n",
      "STEP: 766200 - loss: 0.007751804244098754\n",
      "STEP: 766300 - loss: 0.007751067701711474\n",
      "STEP: 766400 - loss: 0.007750331376503319\n",
      "STEP: 766500 - loss: 0.007749595268407873\n",
      "STEP: 766600 - loss: 0.007748859377359312\n",
      "STEP: 766700 - loss: 0.007748123703291402\n",
      "STEP: 766800 - loss: 0.007747388246138544\n",
      "STEP: 766900 - loss: 0.007746653005834378\n",
      "STEP: 767000 - loss: 0.0077459179823136425\n",
      "STEP: 767100 - loss: 0.007745183175509612\n",
      "STEP: 767200 - loss: 0.007744448585356731\n",
      "STEP: 767300 - loss: 0.007743714211789309\n",
      "STEP: 767400 - loss: 0.007742980054741169\n",
      "STEP: 767500 - loss: 0.007742246114146466\n",
      "STEP: 767600 - loss: 0.00774151238993979\n",
      "STEP: 767700 - loss: 0.007740778882055023\n",
      "STEP: 767800 - loss: 0.007740045590426297\n",
      "STEP: 767900 - loss: 0.007739312514988153\n",
      "STEP: 768000 - loss: 0.007738579655674425\n",
      "STEP: 768100 - loss: 0.007737847012420025\n",
      "STEP: 768200 - loss: 0.007737114585158746\n",
      "STEP: 768300 - loss: 0.0077363823738249415\n",
      "STEP: 768400 - loss: 0.0077356503783532595\n",
      "STEP: 768500 - loss: 0.007734918598677762\n",
      "STEP: 768600 - loss: 0.007734187034733095\n",
      "STEP: 768700 - loss: 0.007733455686453353\n",
      "STEP: 768800 - loss: 0.007732724553773064\n",
      "STEP: 768900 - loss: 0.0077319936366268245\n",
      "STEP: 769000 - loss: 0.007731262934949089\n",
      "STEP: 769100 - loss: 0.00773053244867408\n",
      "STEP: 769200 - loss: 0.007729802177736539\n",
      "STEP: 769300 - loss: 0.007729072122070938\n",
      "STEP: 769400 - loss: 0.007728342281611804\n",
      "STEP: 769500 - loss: 0.0077276126562934285\n",
      "STEP: 769600 - loss: 0.007726883246050992\n",
      "STEP: 769700 - loss: 0.007726154050818526\n",
      "STEP: 769800 - loss: 0.007725425070531014\n",
      "STEP: 769900 - loss: 0.007724696305123013\n",
      "STEP: 770000 - loss: 0.007723967754529121\n",
      "STEP: 770100 - loss: 0.007723239418683784\n",
      "STEP: 770200 - loss: 0.007722511297522119\n",
      "STEP: 770300 - loss: 0.007721783390978789\n",
      "STEP: 770400 - loss: 0.007721055698988326\n",
      "STEP: 770500 - loss: 0.007720328221485695\n",
      "STEP: 770600 - loss: 0.007719600958405107\n",
      "STEP: 770700 - loss: 0.007718873909682454\n",
      "STEP: 770800 - loss: 0.007718147075251436\n",
      "STEP: 770900 - loss: 0.007717420455047634\n",
      "STEP: 771000 - loss: 0.007716694049005801\n",
      "STEP: 771100 - loss: 0.007715967857060374\n",
      "STEP: 771200 - loss: 0.00771524187914682\n",
      "STEP: 771300 - loss: 0.007714516115199484\n",
      "STEP: 771400 - loss: 0.007713790565153799\n",
      "STEP: 771500 - loss: 0.007713065228944596\n",
      "STEP: 771600 - loss: 0.007712340106506507\n",
      "STEP: 771700 - loss: 0.007711615197775043\n",
      "STEP: 771800 - loss: 0.007710890502684979\n",
      "STEP: 771900 - loss: 0.007710166021171164\n",
      "STEP: 772000 - loss: 0.007709441753169177\n",
      "STEP: 772100 - loss: 0.0077087176986135484\n",
      "STEP: 772200 - loss: 0.007707993857439761\n",
      "STEP: 772300 - loss: 0.007707270229582593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 772400 - loss: 0.0077065468149776165\n",
      "STEP: 772500 - loss: 0.007705823613559463\n",
      "STEP: 772600 - loss: 0.007705100625263697\n",
      "STEP: 772700 - loss: 0.007704377850025426\n",
      "STEP: 772800 - loss: 0.0077036552877797915\n",
      "STEP: 772900 - loss: 0.0077029329384620746\n",
      "STEP: 773000 - loss: 0.007702210802007498\n",
      "STEP: 773100 - loss: 0.0077014888783514075\n",
      "STEP: 773200 - loss: 0.007700767167428971\n",
      "STEP: 773300 - loss: 0.007700045669175769\n",
      "STEP: 773400 - loss: 0.00769932438352675\n",
      "STEP: 773500 - loss: 0.007698603310417678\n",
      "STEP: 773600 - loss: 0.00769788244978333\n",
      "STEP: 773700 - loss: 0.007697161801559707\n",
      "STEP: 773800 - loss: 0.007696441365682148\n",
      "STEP: 773900 - loss: 0.007695721142085991\n",
      "STEP: 774000 - loss: 0.007695001130706233\n",
      "STEP: 774100 - loss: 0.007694281331479184\n",
      "STEP: 774200 - loss: 0.007693561744339583\n",
      "STEP: 774300 - loss: 0.0076928423692234306\n",
      "STEP: 774400 - loss: 0.007692123206066109\n",
      "STEP: 774500 - loss: 0.0076914042548027915\n",
      "STEP: 774600 - loss: 0.007690685515369689\n",
      "STEP: 774700 - loss: 0.007689966987702019\n",
      "STEP: 774800 - loss: 0.007689248671735328\n",
      "STEP: 774900 - loss: 0.007688530567405738\n",
      "STEP: 775000 - loss: 0.007687812674648162\n",
      "STEP: 775100 - loss: 0.007687094993398968\n",
      "STEP: 775200 - loss: 0.007686377523593427\n",
      "STEP: 775300 - loss: 0.007685660265167089\n",
      "STEP: 775400 - loss: 0.007684943218056299\n",
      "STEP: 775500 - loss: 0.0076842263821962605\n",
      "STEP: 775600 - loss: 0.0076835097575229105\n",
      "STEP: 775700 - loss: 0.007682793343971807\n",
      "STEP: 775800 - loss: 0.007682077141479127\n",
      "STEP: 775900 - loss: 0.007681361149980609\n",
      "STEP: 776000 - loss: 0.007680645369412225\n",
      "STEP: 776100 - loss: 0.007679929799709544\n",
      "STEP: 776200 - loss: 0.007679214440808236\n",
      "STEP: 776300 - loss: 0.007678499292644749\n",
      "STEP: 776400 - loss: 0.007677784355154868\n",
      "STEP: 776500 - loss: 0.007677069628274266\n",
      "STEP: 776600 - loss: 0.00767635511193923\n",
      "STEP: 776700 - loss: 0.007675640806085409\n",
      "STEP: 776800 - loss: 0.007674926710649054\n",
      "STEP: 776900 - loss: 0.007674212825566161\n",
      "STEP: 777000 - loss: 0.007673499150772661\n",
      "STEP: 777100 - loss: 0.007672785686204565\n",
      "STEP: 777200 - loss: 0.007672072431798115\n",
      "STEP: 777300 - loss: 0.007671359387489381\n",
      "STEP: 777400 - loss: 0.007670646553214501\n",
      "STEP: 777500 - loss: 0.007669933928909511\n",
      "STEP: 777600 - loss: 0.007669221514510544\n",
      "STEP: 777700 - loss: 0.007668509309953685\n",
      "STEP: 777800 - loss: 0.007667797315175698\n",
      "STEP: 777900 - loss: 0.00766708553011218\n",
      "STEP: 778000 - loss: 0.007666373954699433\n",
      "STEP: 778100 - loss: 0.007665662588873895\n",
      "STEP: 778200 - loss: 0.007664951432571808\n",
      "STEP: 778300 - loss: 0.007664240485729318\n",
      "STEP: 778400 - loss: 0.007663529748283031\n",
      "STEP: 778500 - loss: 0.007662819220169025\n",
      "STEP: 778600 - loss: 0.0076621089013236806\n",
      "STEP: 778700 - loss: 0.007661398791683377\n",
      "STEP: 778800 - loss: 0.007660688891184564\n",
      "STEP: 778900 - loss: 0.007659979199763521\n",
      "STEP: 779000 - loss: 0.0076592697173568595\n",
      "STEP: 779100 - loss: 0.0076585604439009525\n",
      "STEP: 779200 - loss: 0.007657851379332325\n",
      "STEP: 779300 - loss: 0.0076571425235870474\n",
      "STEP: 779400 - loss: 0.007656433876602116\n",
      "STEP: 779500 - loss: 0.007655725438313948\n",
      "STEP: 779600 - loss: 0.007655017208659026\n",
      "STEP: 779700 - loss: 0.007654309187573804\n",
      "STEP: 779800 - loss: 0.00765360137499511\n",
      "STEP: 779900 - loss: 0.0076528937708593425\n",
      "STEP: 780000 - loss: 0.007652186375103086\n",
      "STEP: 780100 - loss: 0.007651479187663348\n",
      "STEP: 780200 - loss: 0.007650772208476303\n",
      "STEP: 780300 - loss: 0.007650065437478729\n",
      "STEP: 780400 - loss: 0.007649358874607698\n",
      "STEP: 780500 - loss: 0.007648652519799466\n",
      "STEP: 780600 - loss: 0.0076479463729911355\n",
      "STEP: 780700 - loss: 0.007647240434118992\n",
      "STEP: 780800 - loss: 0.007646534703120443\n",
      "STEP: 780900 - loss: 0.007645829179931597\n",
      "STEP: 781000 - loss: 0.007645123864489912\n",
      "STEP: 781100 - loss: 0.00764441875673178\n",
      "STEP: 781200 - loss: 0.00764371385659412\n",
      "STEP: 781300 - loss: 0.007643009164013985\n",
      "STEP: 781400 - loss: 0.007642304678928241\n",
      "STEP: 781500 - loss: 0.0076416004012735935\n",
      "STEP: 781600 - loss: 0.0076408963309870905\n",
      "STEP: 781700 - loss: 0.007640192468005655\n",
      "STEP: 781800 - loss: 0.0076394888122663165\n",
      "STEP: 781900 - loss: 0.007638785363705769\n",
      "STEP: 782000 - loss: 0.007638082122261397\n",
      "STEP: 782100 - loss: 0.007637379087870192\n",
      "STEP: 782200 - loss: 0.0076366762604689106\n",
      "STEP: 782300 - loss: 0.007635973639994828\n",
      "STEP: 782400 - loss: 0.007635271226384991\n",
      "STEP: 782500 - loss: 0.007634569019576698\n",
      "STEP: 782600 - loss: 0.0076338670195068336\n",
      "STEP: 782700 - loss: 0.0076331652261123996\n",
      "STEP: 782800 - loss: 0.007632463639330618\n",
      "STEP: 782900 - loss: 0.00763176225909905\n",
      "STEP: 783000 - loss: 0.007631061085354526\n",
      "STEP: 783100 - loss: 0.007630360118034014\n",
      "STEP: 783200 - loss: 0.007629659357075398\n",
      "STEP: 783300 - loss: 0.007628958802415532\n",
      "STEP: 783400 - loss: 0.007628258453991714\n",
      "STEP: 783500 - loss: 0.007627558311741412\n",
      "STEP: 783600 - loss: 0.007626858375601742\n",
      "STEP: 783700 - loss: 0.0076261586455099225\n",
      "STEP: 783800 - loss: 0.0076254591214038134\n",
      "STEP: 783900 - loss: 0.007624759803220267\n",
      "STEP: 784000 - loss: 0.007624060690896721\n",
      "STEP: 784100 - loss: 0.007623361784371072\n",
      "STEP: 784200 - loss: 0.007622663083579806\n",
      "STEP: 784300 - loss: 0.007621964588461362\n",
      "STEP: 784400 - loss: 0.007621266298952684\n",
      "STEP: 784500 - loss: 0.007620568214991225\n",
      "STEP: 784600 - loss: 0.007619870336514749\n",
      "STEP: 784700 - loss: 0.007619172663460748\n",
      "STEP: 784800 - loss: 0.007618475195766481\n",
      "STEP: 784900 - loss: 0.0076177779333695805\n",
      "STEP: 785000 - loss: 0.00761708087620788\n",
      "STEP: 785100 - loss: 0.007616384024218781\n",
      "STEP: 785200 - loss: 0.007615687377339832\n",
      "STEP: 785300 - loss: 0.007614990935508834\n",
      "STEP: 785400 - loss: 0.007614294698663243\n",
      "STEP: 785500 - loss: 0.007613598666740945\n",
      "STEP: 785600 - loss: 0.007612902839679513\n",
      "STEP: 785700 - loss: 0.007612207217416782\n",
      "STEP: 785800 - loss: 0.007611511799890319\n",
      "STEP: 785900 - loss: 0.007610816587037707\n",
      "STEP: 786000 - loss: 0.0076101215787970754\n",
      "STEP: 786100 - loss: 0.007609426775106162\n",
      "STEP: 786200 - loss: 0.007608732175902852\n",
      "STEP: 786300 - loss: 0.007608037781124492\n",
      "STEP: 786400 - loss: 0.007607343590709218\n",
      "STEP: 786500 - loss: 0.007606649604594881\n",
      "STEP: 786600 - loss: 0.007605955822719468\n",
      "STEP: 786700 - loss: 0.007605262245020883\n",
      "STEP: 786800 - loss: 0.0076045688714366675\n",
      "STEP: 786900 - loss: 0.007603875701905208\n",
      "STEP: 787000 - loss: 0.007603182736364171\n",
      "STEP: 787100 - loss: 0.007602489974751941\n",
      "STEP: 787200 - loss: 0.007601797417005908\n",
      "STEP: 787300 - loss: 0.007601105063064548\n",
      "STEP: 787400 - loss: 0.007600412912865668\n",
      "STEP: 787500 - loss: 0.007599720966347416\n",
      "STEP: 787600 - loss: 0.0075990292234479565\n",
      "STEP: 787700 - loss: 0.00759833768410473\n",
      "STEP: 787800 - loss: 0.007597646348256841\n",
      "STEP: 787900 - loss: 0.0075969552158419345\n",
      "STEP: 788000 - loss: 0.007596264286797813\n",
      "STEP: 788100 - loss: 0.007595573561063251\n",
      "STEP: 788200 - loss: 0.007594883038576085\n",
      "STEP: 788300 - loss: 0.007594192719274732\n",
      "STEP: 788400 - loss: 0.007593502603096912\n",
      "STEP: 788500 - loss: 0.007592812689981456\n",
      "STEP: 788600 - loss: 0.007592122979866325\n",
      "STEP: 788700 - loss: 0.007591433472689905\n",
      "STEP: 788800 - loss: 0.007590744168390477\n",
      "STEP: 788900 - loss: 0.007590055066906262\n",
      "STEP: 789000 - loss: 0.007589366168175524\n",
      "STEP: 789100 - loss: 0.0075886774721367015\n",
      "STEP: 789200 - loss: 0.0075879889787283635\n",
      "STEP: 789300 - loss: 0.007587300687888607\n",
      "STEP: 789400 - loss: 0.0075866125995559695\n",
      "STEP: 789500 - loss: 0.0075859247136689165\n",
      "STEP: 789600 - loss: 0.007585237030165763\n",
      "STEP: 789700 - loss: 0.007584549548985008\n",
      "STEP: 789800 - loss: 0.007583862270065399\n",
      "STEP: 789900 - loss: 0.007583175193344944\n",
      "STEP: 790000 - loss: 0.00758248831876254\n",
      "STEP: 790100 - loss: 0.007581801646256599\n",
      "STEP: 790200 - loss: 0.007581115175765698\n",
      "STEP: 790300 - loss: 0.007580428907228423\n",
      "STEP: 790400 - loss: 0.007579742840583351\n",
      "STEP: 790500 - loss: 0.007579056975768809\n",
      "STEP: 790600 - loss: 0.007578371312724245\n",
      "STEP: 790700 - loss: 0.007577685851387269\n",
      "STEP: 790800 - loss: 0.007577000591697582\n",
      "STEP: 790900 - loss: 0.007576315533592943\n",
      "STEP: 791000 - loss: 0.0075756306770124295\n",
      "STEP: 791100 - loss: 0.007574946021895121\n",
      "STEP: 791200 - loss: 0.007574261568179105\n",
      "STEP: 791300 - loss: 0.007573577315803388\n",
      "STEP: 791400 - loss: 0.007572893264707052\n",
      "STEP: 791500 - loss: 0.007572209414828658\n",
      "STEP: 791600 - loss: 0.00757152576610696\n",
      "STEP: 791700 - loss: 0.007570842318480975\n",
      "STEP: 791800 - loss: 0.0075701590718893885\n",
      "STEP: 791900 - loss: 0.007569476026271239\n",
      "STEP: 792000 - loss: 0.007568793181565048\n",
      "STEP: 792100 - loss: 0.007568110537710374\n",
      "STEP: 792200 - loss: 0.007567428094645433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 792300 - loss: 0.007566745852309676\n",
      "STEP: 792400 - loss: 0.0075660638106418735\n",
      "STEP: 792500 - loss: 0.007565381969581002\n",
      "STEP: 792600 - loss: 0.007564700329066203\n",
      "STEP: 792700 - loss: 0.007564018889036366\n",
      "STEP: 792800 - loss: 0.007563337649430476\n",
      "STEP: 792900 - loss: 0.00756265661018767\n",
      "STEP: 793000 - loss: 0.007561975771247334\n",
      "STEP: 793100 - loss: 0.007561295132547774\n",
      "STEP: 793200 - loss: 0.00756061469402908\n",
      "STEP: 793300 - loss: 0.0075599344556297205\n",
      "STEP: 793400 - loss: 0.007559254417288705\n",
      "STEP: 793500 - loss: 0.007558574578946016\n",
      "STEP: 793600 - loss: 0.0075578949405401274\n",
      "STEP: 793700 - loss: 0.007557215502010346\n",
      "STEP: 793800 - loss: 0.0075565362632963404\n",
      "STEP: 793900 - loss: 0.007555857224336653\n",
      "STEP: 794000 - loss: 0.00755517838507124\n",
      "STEP: 794100 - loss: 0.007554499745439061\n",
      "STEP: 794200 - loss: 0.007553821305379127\n",
      "STEP: 794300 - loss: 0.007553143064831317\n",
      "STEP: 794400 - loss: 0.007552465023734673\n",
      "STEP: 794500 - loss: 0.007551787182028683\n",
      "STEP: 794600 - loss: 0.007551109539652201\n",
      "STEP: 794700 - loss: 0.00755043209654546\n",
      "STEP: 794800 - loss: 0.007549754852647211\n",
      "STEP: 794900 - loss: 0.007549077807897324\n",
      "STEP: 795000 - loss: 0.007548400962235001\n",
      "STEP: 795100 - loss: 0.007547724315599559\n",
      "STEP: 795200 - loss: 0.007547047867930675\n",
      "STEP: 795300 - loss: 0.00754637161916802\n",
      "STEP: 795400 - loss: 0.007545695569250839\n",
      "STEP: 795500 - loss: 0.007545019718118849\n",
      "STEP: 795600 - loss: 0.007544344065711491\n",
      "STEP: 795700 - loss: 0.007543668611968359\n",
      "STEP: 795800 - loss: 0.007542993356828967\n",
      "STEP: 795900 - loss: 0.007542318300233336\n",
      "STEP: 796000 - loss: 0.007541643442120552\n",
      "STEP: 796100 - loss: 0.007540968782430502\n",
      "STEP: 796200 - loss: 0.007540294321103008\n",
      "STEP: 796300 - loss: 0.00753962005807738\n",
      "STEP: 796400 - loss: 0.007538945993293705\n",
      "STEP: 796500 - loss: 0.007538272126691605\n",
      "STEP: 796600 - loss: 0.007537598458210686\n",
      "STEP: 796700 - loss: 0.007536924987790848\n",
      "STEP: 796800 - loss: 0.007536251715371639\n",
      "STEP: 796900 - loss: 0.007535578640893169\n",
      "STEP: 797000 - loss: 0.007534905764295057\n",
      "STEP: 797100 - loss: 0.0075342330855169976\n",
      "STEP: 797200 - loss: 0.007533560604499263\n",
      "STEP: 797300 - loss: 0.0075328883211812775\n",
      "STEP: 797400 - loss: 0.007532216235503325\n",
      "STEP: 797500 - loss: 0.007531544347405145\n",
      "STEP: 797600 - loss: 0.007530872656826714\n",
      "STEP: 797700 - loss: 0.007530201163707569\n",
      "STEP: 797800 - loss: 0.007529529867988223\n",
      "STEP: 797900 - loss: 0.007528858769608371\n",
      "STEP: 798000 - loss: 0.007528187868507985\n",
      "STEP: 798100 - loss: 0.00752751716462752\n",
      "STEP: 798200 - loss: 0.007526846657906532\n",
      "STEP: 798300 - loss: 0.0075261763482849225\n",
      "STEP: 798400 - loss: 0.007525506235703259\n",
      "STEP: 798500 - loss: 0.007524836320101605\n",
      "STEP: 798600 - loss: 0.007524166601419413\n",
      "STEP: 798700 - loss: 0.0075234970795975565\n",
      "STEP: 798800 - loss: 0.007522827754575941\n",
      "STEP: 798900 - loss: 0.007522158626294427\n",
      "STEP: 799000 - loss: 0.00752148969469362\n",
      "STEP: 799100 - loss: 0.007520820959713609\n",
      "STEP: 799200 - loss: 0.007520152421294333\n",
      "STEP: 799300 - loss: 0.00751948407937644\n",
      "STEP: 799400 - loss: 0.0075188159338999165\n",
      "STEP: 799500 - loss: 0.00751814798480518\n",
      "STEP: 799600 - loss: 0.0075174802320324115\n",
      "STEP: 799700 - loss: 0.007516812675521746\n",
      "STEP: 799800 - loss: 0.007516145315213854\n",
      "STEP: 799900 - loss: 0.007515478151048978\n",
      "STEP: 800000 - loss: 0.007514811182967341\n",
      "STEP: 800100 - loss: 0.007514144410909302\n",
      "STEP: 800200 - loss: 0.007513477834815559\n",
      "STEP: 800300 - loss: 0.007512811454626118\n",
      "STEP: 800400 - loss: 0.0075121452702819405\n",
      "STEP: 800500 - loss: 0.0075114792817229615\n",
      "STEP: 800600 - loss: 0.007510813488889915\n",
      "STEP: 800700 - loss: 0.0075101478917231675\n",
      "STEP: 800800 - loss: 0.007509482490163517\n",
      "STEP: 800900 - loss: 0.007508817284151101\n",
      "STEP: 801000 - loss: 0.0075081522736266375\n",
      "STEP: 801100 - loss: 0.007507487458530615\n",
      "STEP: 801200 - loss: 0.007506822838803576\n",
      "STEP: 801300 - loss: 0.007506158414386609\n",
      "STEP: 801400 - loss: 0.007505494185219454\n",
      "STEP: 801500 - loss: 0.007504830151243503\n",
      "STEP: 801600 - loss: 0.007504166312399039\n",
      "STEP: 801700 - loss: 0.007503502668626827\n",
      "STEP: 801800 - loss: 0.007502839219867686\n",
      "STEP: 801900 - loss: 0.007502175966062024\n",
      "STEP: 802000 - loss: 0.007501512907150762\n",
      "STEP: 802100 - loss: 0.007500850043074561\n",
      "STEP: 802200 - loss: 0.007500187373774228\n",
      "STEP: 802300 - loss: 0.007499524899190644\n",
      "STEP: 802400 - loss: 0.007498862619264362\n",
      "STEP: 802500 - loss: 0.0074982005339362\n",
      "STEP: 802600 - loss: 0.007497538643147242\n",
      "STEP: 802700 - loss: 0.007496876946838521\n",
      "STEP: 802800 - loss: 0.007496215444950221\n",
      "STEP: 802900 - loss: 0.007495554137423673\n",
      "STEP: 803000 - loss: 0.007494893024199629\n",
      "STEP: 803100 - loss: 0.007494232105219383\n",
      "STEP: 803200 - loss: 0.007493571380423311\n",
      "STEP: 803300 - loss: 0.007492910849752657\n",
      "STEP: 803400 - loss: 0.007492250513148413\n",
      "STEP: 803500 - loss: 0.007491590370551536\n",
      "STEP: 803600 - loss: 0.00749093042190316\n",
      "STEP: 803700 - loss: 0.00749027066714415\n",
      "STEP: 803800 - loss: 0.007489611106215488\n",
      "STEP: 803900 - loss: 0.00748895173905838\n",
      "STEP: 804000 - loss: 0.0074882925656137595\n",
      "STEP: 804100 - loss: 0.007487633585822988\n",
      "STEP: 804200 - loss: 0.00748697479962705\n",
      "STEP: 804300 - loss: 0.007486316206967122\n",
      "STEP: 804400 - loss: 0.007485657807784152\n",
      "STEP: 804500 - loss: 0.007484999602019487\n",
      "STEP: 804600 - loss: 0.007484341589614464\n",
      "STEP: 804700 - loss: 0.007483683770509976\n",
      "STEP: 804800 - loss: 0.007483026144647327\n",
      "STEP: 804900 - loss: 0.0074823687119679695\n",
      "STEP: 805000 - loss: 0.00748171147241293\n",
      "STEP: 805100 - loss: 0.007481054425923682\n",
      "STEP: 805200 - loss: 0.007480397572441263\n",
      "STEP: 805300 - loss: 0.007479740911907426\n",
      "STEP: 805400 - loss: 0.007479084444263072\n",
      "STEP: 805500 - loss: 0.007478428169449819\n",
      "STEP: 805600 - loss: 0.007477772087408739\n",
      "STEP: 805700 - loss: 0.007477116198081414\n",
      "STEP: 805800 - loss: 0.007476460501409448\n",
      "STEP: 805900 - loss: 0.007475804997333808\n",
      "STEP: 806000 - loss: 0.007475149685796356\n",
      "STEP: 806100 - loss: 0.007474494566738187\n",
      "STEP: 806200 - loss: 0.007473839640101229\n",
      "STEP: 806300 - loss: 0.007473184905826635\n",
      "STEP: 806400 - loss: 0.007472530363855899\n",
      "STEP: 806500 - loss: 0.00747187601413081\n",
      "STEP: 806600 - loss: 0.007471221856592715\n",
      "STEP: 806700 - loss: 0.007470567891183004\n",
      "STEP: 806800 - loss: 0.007469914117843581\n",
      "STEP: 806900 - loss: 0.007469260536516182\n",
      "STEP: 807000 - loss: 0.007468607147141833\n",
      "STEP: 807100 - loss: 0.007467953949662649\n",
      "STEP: 807200 - loss: 0.007467300944020143\n",
      "STEP: 807300 - loss: 0.007466648130156061\n",
      "STEP: 807400 - loss: 0.007465995508012126\n",
      "STEP: 807500 - loss: 0.007465343077529751\n",
      "STEP: 807600 - loss: 0.007464690838650941\n",
      "STEP: 807700 - loss: 0.007464038791317291\n",
      "STEP: 807800 - loss: 0.007463386935470463\n",
      "STEP: 807900 - loss: 0.007462735271052852\n",
      "STEP: 808000 - loss: 0.007462083798005179\n",
      "STEP: 808100 - loss: 0.007461432516270497\n",
      "STEP: 808200 - loss: 0.007460781425789677\n",
      "STEP: 808300 - loss: 0.007460130526505031\n",
      "STEP: 808400 - loss: 0.00745947981835812\n",
      "STEP: 808500 - loss: 0.007458829301291134\n",
      "STEP: 808600 - loss: 0.0074581789752459285\n",
      "STEP: 808700 - loss: 0.0074575288401644195\n",
      "STEP: 808800 - loss: 0.007456878895988195\n",
      "STEP: 808900 - loss: 0.007456229142659479\n",
      "STEP: 809000 - loss: 0.007455579580120363\n",
      "STEP: 809100 - loss: 0.007454930208312528\n",
      "STEP: 809200 - loss: 0.007454281027178602\n",
      "STEP: 809300 - loss: 0.0074536320366600075\n",
      "STEP: 809400 - loss: 0.0074529832366987655\n",
      "STEP: 809500 - loss: 0.007452334627237342\n",
      "STEP: 809600 - loss: 0.007451686208217521\n",
      "STEP: 809700 - loss: 0.007451037979581518\n",
      "STEP: 809800 - loss: 0.007450389941271475\n",
      "STEP: 809900 - loss: 0.007449742093229311\n",
      "STEP: 810000 - loss: 0.007449094435397612\n",
      "STEP: 810100 - loss: 0.00744844696771792\n",
      "STEP: 810200 - loss: 0.007447799690133154\n",
      "STEP: 810300 - loss: 0.007447152602584575\n",
      "STEP: 810400 - loss: 0.00744650570501528\n",
      "STEP: 810500 - loss: 0.007445858997366964\n",
      "STEP: 810600 - loss: 0.007445212479582431\n",
      "STEP: 810700 - loss: 0.007444566151603431\n",
      "STEP: 810800 - loss: 0.007443920013372428\n",
      "STEP: 810900 - loss: 0.007443274064831763\n",
      "STEP: 811000 - loss: 0.0074426283059236655\n",
      "STEP: 811100 - loss: 0.0074419827365906285\n",
      "STEP: 811200 - loss: 0.007441337356774856\n",
      "STEP: 811300 - loss: 0.00744069216641881\n",
      "STEP: 811400 - loss: 0.007440047165464865\n",
      "STEP: 811500 - loss: 0.007439402353855531\n",
      "STEP: 811600 - loss: 0.007438757731533079\n",
      "STEP: 811700 - loss: 0.007438113298439986\n",
      "STEP: 811800 - loss: 0.00743746905451885\n",
      "STEP: 811900 - loss: 0.007436824999712007\n",
      "STEP: 812000 - loss: 0.007436181133962017\n",
      "STEP: 812100 - loss: 0.00743553745721137\n",
      "STEP: 812200 - loss: 0.007434893969402619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 812300 - loss: 0.007434250670478149\n",
      "STEP: 812400 - loss: 0.0074336075603810715\n",
      "STEP: 812500 - loss: 0.007432964639053433\n",
      "STEP: 812600 - loss: 0.007432321906437882\n",
      "STEP: 812700 - loss: 0.0074316793624772115\n",
      "STEP: 812800 - loss: 0.007431037007113968\n",
      "STEP: 812900 - loss: 0.0074303948402909105\n",
      "STEP: 813000 - loss: 0.007429752861950673\n",
      "STEP: 813100 - loss: 0.007429111072035968\n",
      "STEP: 813200 - loss: 0.007428469470489303\n",
      "STEP: 813300 - loss: 0.007427828057253842\n",
      "STEP: 813400 - loss: 0.0074271868322718275\n",
      "STEP: 813500 - loss: 0.007426545795486463\n",
      "STEP: 813600 - loss: 0.007425904946839956\n",
      "STEP: 813700 - loss: 0.007425264286275731\n",
      "STEP: 813800 - loss: 0.007424623813736273\n",
      "STEP: 813900 - loss: 0.007423983529164575\n",
      "STEP: 814000 - loss: 0.007423343432503174\n",
      "STEP: 814100 - loss: 0.007422703523695388\n",
      "STEP: 814200 - loss: 0.007422063802683576\n",
      "STEP: 814300 - loss: 0.007421424269410955\n",
      "STEP: 814400 - loss: 0.007420784923820667\n",
      "STEP: 814500 - loss: 0.007420145765855176\n",
      "STEP: 814600 - loss: 0.007419506795457657\n",
      "STEP: 814700 - loss: 0.007418868012571213\n",
      "STEP: 814800 - loss: 0.007418229417138604\n",
      "STEP: 814900 - loss: 0.007417591009102821\n",
      "STEP: 815000 - loss: 0.007416952788407001\n",
      "STEP: 815100 - loss: 0.007416314754994284\n",
      "STEP: 815200 - loss: 0.007415676908807583\n",
      "STEP: 815300 - loss: 0.007415039249789935\n",
      "STEP: 815400 - loss: 0.0074144017778845425\n",
      "STEP: 815500 - loss: 0.0074137644930344455\n",
      "STEP: 815600 - loss: 0.007413127395182595\n",
      "STEP: 815700 - loss: 0.007412490484272609\n",
      "STEP: 815800 - loss: 0.007411853760247321\n",
      "STEP: 815900 - loss: 0.007411217223050094\n",
      "STEP: 816000 - loss: 0.007410580872623811\n",
      "STEP: 816100 - loss: 0.007409944708911762\n",
      "STEP: 816200 - loss: 0.007409308731857392\n",
      "STEP: 816300 - loss: 0.007408672941403771\n",
      "STEP: 816400 - loss: 0.007408037337494419\n",
      "STEP: 816500 - loss: 0.0074074019200723136\n",
      "STEP: 816600 - loss: 0.007406766689080846\n",
      "STEP: 816700 - loss: 0.007406131644463352\n",
      "STEP: 816800 - loss: 0.00740549678616315\n",
      "STEP: 816900 - loss: 0.007404862114123438\n",
      "STEP: 817000 - loss: 0.007404227628287933\n",
      "STEP: 817100 - loss: 0.007403593328600046\n",
      "STEP: 817200 - loss: 0.00740295921500257\n",
      "STEP: 817300 - loss: 0.007402325287439546\n",
      "STEP: 817400 - loss: 0.007401691545854154\n",
      "STEP: 817500 - loss: 0.007401057990189785\n",
      "STEP: 817600 - loss: 0.007400424620390117\n",
      "STEP: 817700 - loss: 0.00739979143639855\n",
      "STEP: 817800 - loss: 0.007399158438158413\n",
      "STEP: 817900 - loss: 0.007398525625613518\n",
      "STEP: 818000 - loss: 0.007397892998707375\n",
      "STEP: 818100 - loss: 0.007397260557383416\n",
      "STEP: 818200 - loss: 0.007396628301584954\n",
      "STEP: 818300 - loss: 0.007395996231256243\n",
      "STEP: 818400 - loss: 0.007395364346340251\n",
      "STEP: 818500 - loss: 0.007394732646781104\n",
      "STEP: 818600 - loss: 0.007394101132521887\n",
      "STEP: 818700 - loss: 0.0073934698035066655\n",
      "STEP: 818800 - loss: 0.007392838659679257\n",
      "STEP: 818900 - loss: 0.0073922077009828365\n",
      "STEP: 819000 - loss: 0.007391576927361773\n",
      "STEP: 819100 - loss: 0.0073909463387591325\n",
      "STEP: 819200 - loss: 0.007390315935118891\n",
      "STEP: 819300 - loss: 0.0073896857163854215\n",
      "STEP: 819400 - loss: 0.007389055682501288\n",
      "STEP: 819500 - loss: 0.0073884258334112355\n",
      "STEP: 819600 - loss: 0.0073877961690588705\n",
      "STEP: 819700 - loss: 0.007387166689387851\n",
      "STEP: 819800 - loss: 0.007386537394342319\n",
      "STEP: 819900 - loss: 0.007385908283865669\n",
      "STEP: 820000 - loss: 0.007385279357902383\n",
      "STEP: 820100 - loss: 0.007384650616395848\n",
      "STEP: 820200 - loss: 0.007384022059290225\n",
      "STEP: 820300 - loss: 0.007383393686529395\n",
      "STEP: 820400 - loss: 0.007382765498057209\n",
      "STEP: 820500 - loss: 0.007382137493817885\n",
      "STEP: 820600 - loss: 0.007381509673755224\n",
      "STEP: 820700 - loss: 0.007380882037813409\n",
      "STEP: 820800 - loss: 0.0073802545859361895\n",
      "STEP: 820900 - loss: 0.007379627318067618\n",
      "STEP: 821000 - loss: 0.007379000234152083\n",
      "STEP: 821100 - loss: 0.007378373334133205\n",
      "STEP: 821200 - loss: 0.007377746617955386\n",
      "STEP: 821300 - loss: 0.007377120085562822\n",
      "STEP: 821400 - loss: 0.007376493736899174\n",
      "STEP: 821500 - loss: 0.007375867571909113\n",
      "STEP: 821600 - loss: 0.007375241590536407\n",
      "STEP: 821700 - loss: 0.007374615792725338\n",
      "STEP: 821800 - loss: 0.007373990178419993\n",
      "STEP: 821900 - loss: 0.007373364747564978\n",
      "STEP: 822000 - loss: 0.007372739500104207\n",
      "STEP: 822100 - loss: 0.007372114435981738\n",
      "STEP: 822200 - loss: 0.007371489555142237\n",
      "STEP: 822300 - loss: 0.007370864857529862\n",
      "STEP: 822400 - loss: 0.007370240343088639\n",
      "STEP: 822500 - loss: 0.007369616011763084\n",
      "STEP: 822600 - loss: 0.007368991863497608\n",
      "STEP: 822700 - loss: 0.007368367898236414\n",
      "STEP: 822800 - loss: 0.007367744115923868\n",
      "STEP: 822900 - loss: 0.007367120516504339\n",
      "STEP: 823000 - loss: 0.007366497099922287\n",
      "STEP: 823100 - loss: 0.007365873866122025\n",
      "STEP: 823200 - loss: 0.007365250815048135\n",
      "STEP: 823300 - loss: 0.007364627946644935\n",
      "STEP: 823400 - loss: 0.00736400526085679\n",
      "STEP: 823500 - loss: 0.0073633827576285015\n",
      "STEP: 823600 - loss: 0.0073627604369042145\n",
      "STEP: 823700 - loss: 0.007362138298628684\n",
      "STEP: 823800 - loss: 0.007361516342746332\n",
      "STEP: 823900 - loss: 0.007360894569201845\n",
      "STEP: 824000 - loss: 0.007360272977939468\n",
      "STEP: 824100 - loss: 0.007359651568904058\n",
      "STEP: 824200 - loss: 0.007359030342039989\n",
      "STEP: 824300 - loss: 0.007358409297292144\n",
      "STEP: 824400 - loss: 0.007357788434604946\n",
      "STEP: 824500 - loss: 0.0073571677539230815\n",
      "STEP: 824600 - loss: 0.007356547255191435\n",
      "STEP: 824700 - loss: 0.007355926938354079\n",
      "STEP: 824800 - loss: 0.007355306803356362\n",
      "STEP: 824900 - loss: 0.007354686850142749\n",
      "STEP: 825000 - loss: 0.007354067078658079\n",
      "STEP: 825100 - loss: 0.007353447488846966\n",
      "STEP: 825200 - loss: 0.007352828080654121\n",
      "STEP: 825300 - loss: 0.007352208854024343\n",
      "STEP: 825400 - loss: 0.007351589808902439\n",
      "STEP: 825500 - loss: 0.007350970945233531\n",
      "STEP: 825600 - loss: 0.007350352262961925\n",
      "STEP: 825700 - loss: 0.007349733762033001\n",
      "STEP: 825800 - loss: 0.007349115442391234\n",
      "STEP: 825900 - loss: 0.00734849730398186\n",
      "STEP: 826000 - loss: 0.007347879346748888\n",
      "STEP: 826100 - loss: 0.007347261570638542\n",
      "STEP: 826200 - loss: 0.007346643975594852\n",
      "STEP: 826300 - loss: 0.007346026561563078\n",
      "STEP: 826400 - loss: 0.007345409328488104\n",
      "STEP: 826500 - loss: 0.007344792276314863\n",
      "STEP: 826600 - loss: 0.007344175404988361\n",
      "STEP: 826700 - loss: 0.007343558714453838\n",
      "STEP: 826800 - loss: 0.007342942204656212\n",
      "STEP: 826900 - loss: 0.007342325875540403\n",
      "STEP: 827000 - loss: 0.007341709727051536\n",
      "STEP: 827100 - loss: 0.007341093759134558\n",
      "STEP: 827200 - loss: 0.0073404779717349085\n",
      "STEP: 827300 - loss: 0.007339862364797464\n",
      "STEP: 827400 - loss: 0.007339246938267335\n",
      "STEP: 827500 - loss: 0.007338631692089941\n",
      "STEP: 827600 - loss: 0.007338016626210213\n",
      "STEP: 827700 - loss: 0.0073374017405731335\n",
      "STEP: 827800 - loss: 0.007336787035124331\n",
      "STEP: 827900 - loss: 0.007336172509808762\n",
      "STEP: 828000 - loss: 0.007335558164571774\n",
      "STEP: 828100 - loss: 0.007334943999358502\n",
      "STEP: 828200 - loss: 0.007334330014114406\n",
      "STEP: 828300 - loss: 0.007333716208784401\n",
      "STEP: 828400 - loss: 0.0073331025833142565\n",
      "STEP: 828500 - loss: 0.007332489137648969\n",
      "STEP: 828600 - loss: 0.007331875871733892\n",
      "STEP: 828700 - loss: 0.007331262785514481\n",
      "STEP: 828800 - loss: 0.007330649878936108\n",
      "STEP: 828900 - loss: 0.007330037151944151\n",
      "STEP: 829000 - loss: 0.007329424604483751\n",
      "STEP: 829100 - loss: 0.007328812236500694\n",
      "STEP: 829200 - loss: 0.007328200047940133\n",
      "STEP: 829300 - loss: 0.007327588038747723\n",
      "STEP: 829400 - loss: 0.007326976208868835\n",
      "STEP: 829500 - loss: 0.007326364558249088\n",
      "STEP: 829600 - loss: 0.007325753086833556\n",
      "STEP: 829700 - loss: 0.007325141794568375\n",
      "STEP: 829800 - loss: 0.007324530681398536\n",
      "STEP: 829900 - loss: 0.007323919747269686\n",
      "STEP: 830000 - loss: 0.007323308992127721\n",
      "STEP: 830100 - loss: 0.00732269841591805\n",
      "STEP: 830200 - loss: 0.007322088018586153\n",
      "STEP: 830300 - loss: 0.007321477800077557\n",
      "STEP: 830400 - loss: 0.007320867760338229\n",
      "STEP: 830500 - loss: 0.007320257899313452\n",
      "STEP: 830600 - loss: 0.007319648216949132\n",
      "STEP: 830700 - loss: 0.007319038713191152\n",
      "STEP: 830800 - loss: 0.007318429387984581\n",
      "STEP: 830900 - loss: 0.007317820241275734\n",
      "STEP: 831000 - loss: 0.007317211273010019\n",
      "STEP: 831100 - loss: 0.007316602483133169\n",
      "STEP: 831200 - loss: 0.007315993871591269\n",
      "STEP: 831300 - loss: 0.007315385438329769\n",
      "STEP: 831400 - loss: 0.007314777183294506\n",
      "STEP: 831500 - loss: 0.007314169106431567\n",
      "STEP: 831600 - loss: 0.007313561207686346\n",
      "STEP: 831700 - loss: 0.007312953487004897\n",
      "STEP: 831800 - loss: 0.007312345944333406\n",
      "STEP: 831900 - loss: 0.007311738579617083\n",
      "STEP: 832000 - loss: 0.00731113139280241\n",
      "STEP: 832100 - loss: 0.007310524383835025\n",
      "STEP: 832200 - loss: 0.007309917552660796\n",
      "STEP: 832300 - loss: 0.007309310899225924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 832400 - loss: 0.007308704423476024\n",
      "STEP: 832500 - loss: 0.007308098125357458\n",
      "STEP: 832600 - loss: 0.007307492004815915\n",
      "STEP: 832700 - loss: 0.007306886061797709\n",
      "STEP: 832800 - loss: 0.007306280296248352\n",
      "STEP: 832900 - loss: 0.007305674708114428\n",
      "STEP: 833000 - loss: 0.00730506929734184\n",
      "STEP: 833100 - loss: 0.007304464063876641\n",
      "STEP: 833200 - loss: 0.007303859007664841\n",
      "STEP: 833300 - loss: 0.007303254128652311\n",
      "STEP: 833400 - loss: 0.007302649426785707\n",
      "STEP: 833500 - loss: 0.007302044902010814\n",
      "STEP: 833600 - loss: 0.0073014405542741355\n",
      "STEP: 833700 - loss: 0.007300836383521371\n",
      "STEP: 833800 - loss: 0.007300232389698914\n",
      "STEP: 833900 - loss: 0.007299628572753009\n",
      "STEP: 834000 - loss: 0.0072990249326301045\n",
      "STEP: 834100 - loss: 0.0072984214692760774\n",
      "STEP: 834200 - loss: 0.007297818182637164\n",
      "STEP: 834300 - loss: 0.007297215072659893\n",
      "STEP: 834400 - loss: 0.007296612139290577\n",
      "STEP: 834500 - loss: 0.007296009382475181\n",
      "STEP: 834600 - loss: 0.007295406802160449\n",
      "STEP: 834700 - loss: 0.0072948043982922735\n",
      "STEP: 834800 - loss: 0.007294202170817395\n",
      "STEP: 834900 - loss: 0.007293600119682016\n",
      "STEP: 835000 - loss: 0.007292998244832491\n",
      "STEP: 835100 - loss: 0.007292396546215276\n",
      "STEP: 835200 - loss: 0.007291795023776766\n",
      "STEP: 835300 - loss: 0.007291193677463498\n",
      "STEP: 835400 - loss: 0.007290592507221747\n",
      "STEP: 835500 - loss: 0.007289991512998037\n",
      "STEP: 835600 - loss: 0.007289390694738773\n",
      "STEP: 835700 - loss: 0.007288790052390847\n",
      "STEP: 835800 - loss: 0.00728818958590031\n",
      "STEP: 835900 - loss: 0.007287589295214089\n",
      "STEP: 836000 - loss: 0.007286989180278217\n",
      "STEP: 836100 - loss: 0.007286389241040003\n",
      "STEP: 836200 - loss: 0.007285789477445159\n",
      "STEP: 836300 - loss: 0.0072851898894408865\n",
      "STEP: 836400 - loss: 0.007284590476973425\n",
      "STEP: 836500 - loss: 0.007283991239989918\n",
      "STEP: 836600 - loss: 0.007283392178436708\n",
      "STEP: 836700 - loss: 0.00728279329226012\n",
      "STEP: 836800 - loss: 0.007282194581407511\n",
      "STEP: 836900 - loss: 0.007281596045825088\n",
      "STEP: 837000 - loss: 0.007280997685459608\n",
      "STEP: 837100 - loss: 0.00728039950025811\n",
      "STEP: 837200 - loss: 0.007279801490166894\n",
      "STEP: 837300 - loss: 0.007279203655133143\n",
      "STEP: 837400 - loss: 0.007278605995103533\n",
      "STEP: 837500 - loss: 0.0072780085100243395\n",
      "STEP: 837600 - loss: 0.007277411199843232\n",
      "STEP: 837700 - loss: 0.007276814064506358\n",
      "STEP: 837800 - loss: 0.00727621710396098\n",
      "STEP: 837900 - loss: 0.0072756203181536085\n",
      "STEP: 838000 - loss: 0.0072750237070312975\n",
      "STEP: 838100 - loss: 0.007274427270541147\n",
      "STEP: 838200 - loss: 0.007273831008629716\n",
      "STEP: 838300 - loss: 0.007273234921243705\n",
      "STEP: 838400 - loss: 0.0072726390083308265\n",
      "STEP: 838500 - loss: 0.007272043269837618\n",
      "STEP: 838600 - loss: 0.007271447705710879\n",
      "STEP: 838700 - loss: 0.007270852315897735\n",
      "STEP: 838800 - loss: 0.007270257100345246\n",
      "STEP: 838900 - loss: 0.007269662059000462\n",
      "STEP: 839000 - loss: 0.007269067191810474\n",
      "STEP: 839100 - loss: 0.007268472498721812\n",
      "STEP: 839200 - loss: 0.0072678779796822674\n",
      "STEP: 839300 - loss: 0.007267283634638327\n",
      "STEP: 839400 - loss: 0.007266689463537564\n",
      "STEP: 839500 - loss: 0.00726609546632704\n",
      "STEP: 839600 - loss: 0.007265501642953337\n",
      "STEP: 839700 - loss: 0.007264907993364383\n",
      "STEP: 839800 - loss: 0.007264314517506881\n",
      "STEP: 839900 - loss: 0.007263721215327897\n",
      "STEP: 840000 - loss: 0.007263128086774937\n",
      "STEP: 840100 - loss: 0.007262535131795128\n",
      "STEP: 840200 - loss: 0.007261942350335457\n",
      "STEP: 840300 - loss: 0.007261349742343516\n",
      "STEP: 840400 - loss: 0.0072607573077667405\n",
      "STEP: 840500 - loss: 0.007260165046551506\n",
      "STEP: 840600 - loss: 0.007259572958645972\n",
      "STEP: 840700 - loss: 0.0072589810439972575\n",
      "STEP: 840800 - loss: 0.007258389302552515\n",
      "STEP: 840900 - loss: 0.0072577977342591984\n",
      "STEP: 841000 - loss: 0.007257206339064681\n",
      "STEP: 841100 - loss: 0.007256615116916175\n",
      "STEP: 841200 - loss: 0.007256024067761248\n",
      "STEP: 841300 - loss: 0.007255433191547013\n",
      "STEP: 841400 - loss: 0.007254842488221422\n",
      "STEP: 841500 - loss: 0.0072542519577315865\n",
      "STEP: 841600 - loss: 0.007253661600024537\n",
      "STEP: 841700 - loss: 0.007253071415048648\n",
      "STEP: 841800 - loss: 0.0072524814027509835\n",
      "STEP: 841900 - loss: 0.0072518915630787025\n",
      "STEP: 842000 - loss: 0.007251301895979653\n",
      "STEP: 842100 - loss: 0.007250712401401323\n",
      "STEP: 842200 - loss: 0.007250123079291332\n",
      "STEP: 842300 - loss: 0.007249533929597228\n",
      "STEP: 842400 - loss: 0.007248944952266472\n",
      "STEP: 842500 - loss: 0.007248356147246698\n",
      "STEP: 842600 - loss: 0.007247767514485627\n",
      "STEP: 842700 - loss: 0.007247179053930767\n",
      "STEP: 842800 - loss: 0.007246590765529905\n",
      "STEP: 842900 - loss: 0.007246002649230468\n",
      "STEP: 843000 - loss: 0.0072454147049803245\n",
      "STEP: 843100 - loss: 0.00724482693272688\n",
      "STEP: 843200 - loss: 0.007244239332418236\n",
      "STEP: 843300 - loss: 0.007243651904001912\n",
      "STEP: 843400 - loss: 0.007243064647425919\n",
      "STEP: 843500 - loss: 0.007242477562637494\n",
      "STEP: 843600 - loss: 0.007241890649584913\n",
      "STEP: 843700 - loss: 0.0072413039082155\n",
      "STEP: 843800 - loss: 0.007240717338477297\n",
      "STEP: 843900 - loss: 0.0072401309403182556\n",
      "STEP: 844000 - loss: 0.007239544713686194\n",
      "STEP: 844100 - loss: 0.00723895865852856\n",
      "STEP: 844200 - loss: 0.007238372774793713\n",
      "STEP: 844300 - loss: 0.007237787062429106\n",
      "STEP: 844400 - loss: 0.007237201521383069\n",
      "STEP: 844500 - loss: 0.007236616151603257\n",
      "STEP: 844600 - loss: 0.007236030953037616\n",
      "STEP: 844700 - loss: 0.00723544592563419\n",
      "STEP: 844800 - loss: 0.007234861069340651\n",
      "STEP: 844900 - loss: 0.00723427638410562\n",
      "STEP: 845000 - loss: 0.007233691869876461\n",
      "STEP: 845100 - loss: 0.007233107526601311\n",
      "STEP: 845200 - loss: 0.007232523354228385\n",
      "STEP: 845300 - loss: 0.007231939352705384\n",
      "STEP: 845400 - loss: 0.007231355521980884\n",
      "STEP: 845500 - loss: 0.007230771862002416\n",
      "STEP: 845600 - loss: 0.0072301883727184935\n",
      "STEP: 845700 - loss: 0.00722960505407694\n",
      "STEP: 845800 - loss: 0.007229021906026085\n",
      "STEP: 845900 - loss: 0.007228438928514096\n",
      "STEP: 846000 - loss: 0.007227856121488812\n",
      "STEP: 846100 - loss: 0.007227273484898576\n",
      "STEP: 846200 - loss: 0.007226691018691486\n",
      "STEP: 846300 - loss: 0.007226108722816073\n",
      "STEP: 846400 - loss: 0.0072255265972199275\n",
      "STEP: 846500 - loss: 0.007224944641852192\n",
      "STEP: 846600 - loss: 0.007224362856659933\n",
      "STEP: 846700 - loss: 0.0072237812415926585\n",
      "STEP: 846800 - loss: 0.007223199796597675\n",
      "STEP: 846900 - loss: 0.007222618521623589\n",
      "STEP: 847000 - loss: 0.007222037416619002\n",
      "STEP: 847100 - loss: 0.00722145648153171\n",
      "STEP: 847200 - loss: 0.007220875716310783\n",
      "STEP: 847300 - loss: 0.007220295120903814\n",
      "STEP: 847400 - loss: 0.00721971469525963\n",
      "STEP: 847500 - loss: 0.007219134439326355\n",
      "STEP: 847600 - loss: 0.007218554353052731\n",
      "STEP: 847700 - loss: 0.007217974436386785\n",
      "STEP: 847800 - loss: 0.007217394689277325\n",
      "STEP: 847900 - loss: 0.007216815111672374\n",
      "STEP: 848000 - loss: 0.007216235703520801\n",
      "STEP: 848100 - loss: 0.0072156564647708225\n",
      "STEP: 848200 - loss: 0.0072150773953712204\n",
      "STEP: 848300 - loss: 0.00721449849526991\n",
      "STEP: 848400 - loss: 0.007213919764416281\n",
      "STEP: 848500 - loss: 0.0072133412027582\n",
      "STEP: 848600 - loss: 0.007212762810244562\n",
      "STEP: 848700 - loss: 0.0072121845868235055\n",
      "STEP: 848800 - loss: 0.007211606532444415\n",
      "STEP: 848900 - loss: 0.007211028647055276\n",
      "STEP: 849000 - loss: 0.007210450930604741\n",
      "STEP: 849100 - loss: 0.007209873383041484\n",
      "STEP: 849200 - loss: 0.007209296004314608\n",
      "STEP: 849300 - loss: 0.007208718794372219\n",
      "STEP: 849400 - loss: 0.00720814175316313\n",
      "STEP: 849500 - loss: 0.0072075648806361565\n",
      "STEP: 849600 - loss: 0.007206988176740023\n",
      "STEP: 849700 - loss: 0.0072064116414234354\n",
      "STEP: 849800 - loss: 0.007205835274635221\n",
      "STEP: 849900 - loss: 0.007205259076323934\n",
      "STEP: 850000 - loss: 0.007204683046438294\n",
      "STEP: 850100 - loss: 0.007204107184927448\n",
      "STEP: 850200 - loss: 0.007203531491739923\n",
      "STEP: 850300 - loss: 0.007202955966824845\n",
      "STEP: 850400 - loss: 0.007202380610130549\n",
      "STEP: 850500 - loss: 0.007201805421606258\n",
      "STEP: 850600 - loss: 0.007201230401201024\n",
      "STEP: 850700 - loss: 0.007200655548863312\n",
      "STEP: 850800 - loss: 0.007200080864542153\n",
      "STEP: 850900 - loss: 0.007199506348186778\n",
      "STEP: 851000 - loss: 0.007198931999745449\n",
      "STEP: 851100 - loss: 0.007198357819167745\n",
      "STEP: 851200 - loss: 0.007197783806402486\n",
      "STEP: 851300 - loss: 0.0071972099613983225\n",
      "STEP: 851400 - loss: 0.0071966362841047755\n",
      "STEP: 851500 - loss: 0.007196062774470454\n",
      "STEP: 851600 - loss: 0.007195489432444413\n",
      "STEP: 851700 - loss: 0.0071949162579759834\n",
      "STEP: 851800 - loss: 0.00719434325101387\n",
      "STEP: 851900 - loss: 0.007193770411507506\n",
      "STEP: 852000 - loss: 0.007193197739405644\n",
      "STEP: 852100 - loss: 0.007192625234657595\n",
      "STEP: 852200 - loss: 0.007192052897212406\n",
      "STEP: 852300 - loss: 0.007191480727019234\n",
      "STEP: 852400 - loss: 0.007190908724027255\n",
      "STEP: 852500 - loss: 0.007190336888185403\n",
      "STEP: 852600 - loss: 0.0071897652194431215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 852700 - loss: 0.007189193717749684\n",
      "STEP: 852800 - loss: 0.007188622383054187\n",
      "STEP: 852900 - loss: 0.007188051215305727\n",
      "STEP: 853000 - loss: 0.007187480214453747\n",
      "STEP: 853100 - loss: 0.007186909380447251\n",
      "STEP: 853200 - loss: 0.007186338713235691\n",
      "STEP: 853300 - loss: 0.0071857682127684525\n",
      "STEP: 853400 - loss: 0.007185197878994941\n",
      "STEP: 853500 - loss: 0.007184627711863742\n",
      "STEP: 853600 - loss: 0.0071840577113252615\n",
      "STEP: 853700 - loss: 0.007183487877327932\n",
      "STEP: 853800 - loss: 0.007182918209821606\n",
      "STEP: 853900 - loss: 0.0071823487087556235\n",
      "STEP: 854000 - loss: 0.00718177937407917\n",
      "STEP: 854100 - loss: 0.007181210205742012\n",
      "STEP: 854200 - loss: 0.007180641203693135\n",
      "STEP: 854300 - loss: 0.007180072367882507\n",
      "STEP: 854400 - loss: 0.007179503698259227\n",
      "STEP: 854500 - loss: 0.007178935194772816\n",
      "STEP: 854600 - loss: 0.00717836685737278\n",
      "STEP: 854700 - loss: 0.007177798686008404\n",
      "STEP: 854800 - loss: 0.007177230680629673\n",
      "STEP: 854900 - loss: 0.007176662841185832\n",
      "STEP: 855000 - loss: 0.007176095167626783\n",
      "STEP: 855100 - loss: 0.007175527659901539\n",
      "STEP: 855200 - loss: 0.007174960317959664\n",
      "STEP: 855300 - loss: 0.007174393141751509\n",
      "STEP: 855400 - loss: 0.007173826131225891\n",
      "STEP: 855500 - loss: 0.007173259286333192\n",
      "STEP: 855600 - loss: 0.0071726926070222\n",
      "STEP: 855700 - loss: 0.007172126093243013\n",
      "STEP: 855800 - loss: 0.007171559744945622\n",
      "STEP: 855900 - loss: 0.007170993562079267\n",
      "STEP: 856000 - loss: 0.007170427544593476\n",
      "STEP: 856100 - loss: 0.0071698616924386295\n",
      "STEP: 856200 - loss: 0.007169296005563921\n",
      "STEP: 856300 - loss: 0.007168730483919523\n",
      "STEP: 856400 - loss: 0.007168165127454609\n",
      "STEP: 856500 - loss: 0.007167599936119695\n",
      "STEP: 856600 - loss: 0.007167034909863823\n",
      "STEP: 856700 - loss: 0.0071664700486373575\n",
      "STEP: 856800 - loss: 0.007165905352389793\n",
      "STEP: 856900 - loss: 0.007165340821071363\n",
      "STEP: 857000 - loss: 0.007164776454631627\n",
      "STEP: 857100 - loss: 0.007164212253020624\n",
      "STEP: 857200 - loss: 0.0071636482161878835\n",
      "STEP: 857300 - loss: 0.007163084344083659\n",
      "STEP: 857400 - loss: 0.007162520636657792\n",
      "STEP: 857500 - loss: 0.007161957093860029\n",
      "STEP: 857600 - loss: 0.007161393715640983\n",
      "STEP: 857700 - loss: 0.007160830501949852\n",
      "STEP: 857800 - loss: 0.007160267452736575\n",
      "STEP: 857900 - loss: 0.007159704567951929\n",
      "STEP: 858000 - loss: 0.007159141847545335\n",
      "STEP: 858100 - loss: 0.007158579291466708\n",
      "STEP: 858200 - loss: 0.00715801689966652\n",
      "STEP: 858300 - loss: 0.007157454672094531\n",
      "STEP: 858400 - loss: 0.007156892608701074\n",
      "STEP: 858500 - loss: 0.0071563307094359594\n",
      "STEP: 858600 - loss: 0.007155768974249238\n",
      "STEP: 858700 - loss: 0.007155207403091335\n",
      "STEP: 858800 - loss: 0.00715464599591222\n",
      "STEP: 858900 - loss: 0.0071540847526622734\n",
      "STEP: 859000 - loss: 0.007153523673291082\n",
      "STEP: 859100 - loss: 0.0071529627577494315\n",
      "STEP: 859200 - loss: 0.007152402005987181\n",
      "STEP: 859300 - loss: 0.007151841417954445\n",
      "STEP: 859400 - loss: 0.007151280993601827\n",
      "STEP: 859500 - loss: 0.007150720732879442\n",
      "STEP: 859600 - loss: 0.0071501606357371895\n",
      "STEP: 859700 - loss: 0.007149600702125787\n",
      "STEP: 859800 - loss: 0.007149040931995217\n",
      "STEP: 859900 - loss: 0.007148481325295983\n",
      "STEP: 860000 - loss: 0.00714792188197833\n",
      "STEP: 860100 - loss: 0.007147362601992682\n",
      "STEP: 860200 - loss: 0.007146803485289112\n",
      "STEP: 860300 - loss: 0.00714624453181803\n",
      "STEP: 860400 - loss: 0.007145685741530142\n",
      "STEP: 860500 - loss: 0.007145127114375549\n",
      "STEP: 860600 - loss: 0.007144568650304601\n",
      "STEP: 860700 - loss: 0.007144010349267866\n",
      "STEP: 860800 - loss: 0.007143452211215725\n",
      "STEP: 860900 - loss: 0.007142894236099007\n",
      "STEP: 861000 - loss: 0.007142336423867292\n",
      "STEP: 861100 - loss: 0.007141778774471856\n",
      "STEP: 861200 - loss: 0.0071412212878630395\n",
      "STEP: 861300 - loss: 0.00714066396399111\n",
      "STEP: 861400 - loss: 0.007140106802806646\n",
      "STEP: 861500 - loss: 0.007139549804260419\n",
      "STEP: 861600 - loss: 0.007138992968302798\n",
      "STEP: 861700 - loss: 0.0071384362948841455\n",
      "STEP: 861800 - loss: 0.007137879783955479\n",
      "STEP: 861900 - loss: 0.007137323435467339\n",
      "STEP: 862000 - loss: 0.007136767249369959\n",
      "STEP: 862100 - loss: 0.007136211225614308\n",
      "STEP: 862200 - loss: 0.007135655364150952\n",
      "STEP: 862300 - loss: 0.007135099664930526\n",
      "STEP: 862400 - loss: 0.007134544127903687\n",
      "STEP: 862500 - loss: 0.007133988753021051\n",
      "STEP: 862600 - loss: 0.007133433540233776\n",
      "STEP: 862700 - loss: 0.007132878489491958\n",
      "STEP: 862800 - loss: 0.00713232360074664\n",
      "STEP: 862900 - loss: 0.007131768873948542\n",
      "STEP: 863000 - loss: 0.0071312143090483116\n",
      "STEP: 863100 - loss: 0.007130659905996918\n",
      "STEP: 863200 - loss: 0.0071301056647451195\n",
      "STEP: 863300 - loss: 0.007129551585243803\n",
      "STEP: 863400 - loss: 0.007128997667443344\n",
      "STEP: 863500 - loss: 0.007128443911295275\n",
      "STEP: 863600 - loss: 0.0071278903167496955\n",
      "STEP: 863700 - loss: 0.007127336883758197\n",
      "STEP: 863800 - loss: 0.0071267836122713675\n",
      "STEP: 863900 - loss: 0.007126230502239923\n",
      "STEP: 864000 - loss: 0.007125677553615104\n",
      "STEP: 864100 - loss: 0.007125124766347415\n",
      "STEP: 864200 - loss: 0.00712457214038812\n",
      "STEP: 864300 - loss: 0.007124019675688112\n",
      "STEP: 864400 - loss: 0.007123467372198424\n",
      "STEP: 864500 - loss: 0.007122915229870119\n",
      "STEP: 864600 - loss: 0.007122363248653956\n",
      "STEP: 864700 - loss: 0.007121811428500963\n",
      "STEP: 864800 - loss: 0.007121259769362421\n",
      "STEP: 864900 - loss: 0.007120708271189045\n",
      "STEP: 865000 - loss: 0.007120156933932149\n",
      "STEP: 865100 - loss: 0.007119605757542913\n",
      "STEP: 865200 - loss: 0.007119054741971963\n",
      "STEP: 865300 - loss: 0.0071185038871709825\n",
      "STEP: 865400 - loss: 0.007117953193090695\n",
      "STEP: 865500 - loss: 0.0071174026596825075\n",
      "STEP: 865600 - loss: 0.007116852286897224\n",
      "STEP: 865700 - loss: 0.007116302074686462\n",
      "STEP: 865800 - loss: 0.007115752023001058\n",
      "STEP: 865900 - loss: 0.007115202131792337\n",
      "STEP: 866000 - loss: 0.007114652401011464\n",
      "STEP: 866100 - loss: 0.007114102830609625\n",
      "STEP: 866200 - loss: 0.007113553420538182\n",
      "STEP: 866300 - loss: 0.007113004170748427\n",
      "STEP: 866400 - loss: 0.007112455081191337\n",
      "STEP: 866500 - loss: 0.00711190615181863\n",
      "STEP: 866600 - loss: 0.007111357382581294\n",
      "STEP: 866700 - loss: 0.007110808773430652\n",
      "STEP: 866800 - loss: 0.0071102603243181906\n",
      "STEP: 866900 - loss: 0.00710971203519536\n",
      "STEP: 867000 - loss: 0.007109163906013095\n",
      "STEP: 867100 - loss: 0.007108615936723293\n",
      "STEP: 867200 - loss: 0.0071080681272767164\n",
      "STEP: 867300 - loss: 0.0071075204776253915\n",
      "STEP: 867400 - loss: 0.007106972987720378\n",
      "STEP: 867500 - loss: 0.007106425657512905\n",
      "STEP: 867600 - loss: 0.00710587848695522\n",
      "STEP: 867700 - loss: 0.0071053314759981345\n",
      "STEP: 867800 - loss: 0.007104784624593306\n",
      "STEP: 867900 - loss: 0.007104237932691879\n",
      "STEP: 868000 - loss: 0.007103691400246075\n",
      "STEP: 868100 - loss: 0.0071031450272070595\n",
      "STEP: 868200 - loss: 0.007102598813525979\n",
      "STEP: 868300 - loss: 0.007102052759154781\n",
      "STEP: 868400 - loss: 0.007101506864045366\n",
      "STEP: 868500 - loss: 0.007100961128148758\n",
      "STEP: 868600 - loss: 0.007100415551416573\n",
      "STEP: 868700 - loss: 0.007099870133800879\n",
      "STEP: 868800 - loss: 0.007099324875252676\n",
      "STEP: 868900 - loss: 0.007098779775724351\n",
      "STEP: 869000 - loss: 0.007098234835166717\n",
      "STEP: 869100 - loss: 0.007097690053532256\n",
      "STEP: 869200 - loss: 0.007097145430772273\n",
      "STEP: 869300 - loss: 0.007096600966838415\n",
      "STEP: 869400 - loss: 0.0070960566616822\n",
      "STEP: 869500 - loss: 0.00709551251525601\n",
      "STEP: 869600 - loss: 0.007094968527510942\n",
      "STEP: 869700 - loss: 0.007094424698398926\n",
      "STEP: 869800 - loss: 0.007093881027871865\n",
      "STEP: 869900 - loss: 0.0070933375158816645\n",
      "STEP: 870000 - loss: 0.007092794162379822\n",
      "STEP: 870100 - loss: 0.007092250967318257\n",
      "STEP: 870200 - loss: 0.00709170793064925\n",
      "STEP: 870300 - loss: 0.007091165052323753\n",
      "STEP: 870400 - loss: 0.007090622332294345\n",
      "STEP: 870500 - loss: 0.007090079770512324\n",
      "STEP: 870600 - loss: 0.007089537366930347\n",
      "STEP: 870700 - loss: 0.0070889951214997155\n",
      "STEP: 870800 - loss: 0.007088453034172435\n",
      "STEP: 870900 - loss: 0.007087911104900839\n",
      "STEP: 871000 - loss: 0.0070873693336360715\n",
      "STEP: 871100 - loss: 0.0070868277203309\n",
      "STEP: 871200 - loss: 0.007086286264937121\n",
      "STEP: 871300 - loss: 0.007085744967406399\n",
      "STEP: 871400 - loss: 0.007085203827691125\n",
      "STEP: 871500 - loss: 0.007084662845743053\n",
      "STEP: 871600 - loss: 0.0070841220215143025\n",
      "STEP: 871700 - loss: 0.007083581354956821\n",
      "STEP: 871800 - loss: 0.007083040846022691\n",
      "STEP: 871900 - loss: 0.007082500494664401\n",
      "STEP: 872000 - loss: 0.007081960300833587\n",
      "STEP: 872100 - loss: 0.007081420264482462\n",
      "STEP: 872200 - loss: 0.007080880385563304\n",
      "STEP: 872300 - loss: 0.007080340664028147\n",
      "STEP: 872400 - loss: 0.007079801099829024\n",
      "STEP: 872500 - loss: 0.00707926169291839\n",
      "STEP: 872600 - loss: 0.007078722443248024\n",
      "STEP: 872700 - loss: 0.0070781833507703685\n",
      "STEP: 872800 - loss: 0.007077644415437931\n",
      "STEP: 872900 - loss: 0.007077105637202459\n",
      "STEP: 873000 - loss: 0.007076567016016101\n",
      "STEP: 873100 - loss: 0.007076028551831634\n",
      "STEP: 873200 - loss: 0.007075490244601114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 873300 - loss: 0.007074952094276582\n",
      "STEP: 873400 - loss: 0.0070744141008106665\n",
      "STEP: 873500 - loss: 0.0070738762641554985\n",
      "STEP: 873600 - loss: 0.007073338584263319\n",
      "STEP: 873700 - loss: 0.007072801061086679\n",
      "STEP: 873800 - loss: 0.007072263694578053\n",
      "STEP: 873900 - loss: 0.007071726484689461\n",
      "STEP: 874000 - loss: 0.007071189431373556\n",
      "STEP: 874100 - loss: 0.007070652534582442\n",
      "STEP: 874200 - loss: 0.0070701157942689215\n",
      "STEP: 874300 - loss: 0.0070695792103849684\n",
      "STEP: 874400 - loss: 0.007069042782883443\n",
      "STEP: 874500 - loss: 0.007068506511716484\n",
      "STEP: 874600 - loss: 0.007067970396837066\n",
      "STEP: 874700 - loss: 0.007067434438197258\n",
      "STEP: 874800 - loss: 0.007066898635749455\n",
      "STEP: 874900 - loss: 0.007066362989446318\n",
      "STEP: 875000 - loss: 0.007065827499240468\n",
      "STEP: 875100 - loss: 0.007065292165084375\n",
      "STEP: 875200 - loss: 0.007064756986930707\n",
      "STEP: 875300 - loss: 0.007064221964731797\n",
      "STEP: 875400 - loss: 0.007063687098440652\n",
      "STEP: 875500 - loss: 0.007063152388009352\n",
      "STEP: 875600 - loss: 0.0070626178333907125\n",
      "STEP: 875700 - loss: 0.0070620834345376925\n",
      "STEP: 875800 - loss: 0.007061549191402408\n",
      "STEP: 875900 - loss: 0.007061015103937987\n",
      "STEP: 876000 - loss: 0.0070604811720967765\n",
      "STEP: 876100 - loss: 0.007059947395831698\n",
      "STEP: 876200 - loss: 0.007059413775095466\n",
      "STEP: 876300 - loss: 0.007058880309840088\n",
      "STEP: 876400 - loss: 0.0070583470000194935\n",
      "STEP: 876500 - loss: 0.007057813845585367\n",
      "STEP: 876600 - loss: 0.007057280846491066\n",
      "STEP: 876700 - loss: 0.007056748002689387\n",
      "STEP: 876800 - loss: 0.007056215314132928\n",
      "STEP: 876900 - loss: 0.007055682780774371\n",
      "STEP: 877000 - loss: 0.007055150402566749\n",
      "STEP: 877100 - loss: 0.00705461817946292\n",
      "STEP: 877200 - loss: 0.0070540861114153615\n",
      "STEP: 877300 - loss: 0.007053554198377441\n",
      "STEP: 877400 - loss: 0.00705302244030185\n",
      "STEP: 877500 - loss: 0.007052490837141228\n",
      "STEP: 877600 - loss: 0.007051959388848836\n",
      "STEP: 877700 - loss: 0.007051428095377503\n",
      "STEP: 877800 - loss: 0.007050896956679588\n",
      "STEP: 877900 - loss: 0.007050365972709095\n",
      "STEP: 878000 - loss: 0.007049835143418358\n",
      "STEP: 878100 - loss: 0.007049304468760317\n",
      "STEP: 878200 - loss: 0.007048773948688092\n",
      "STEP: 878300 - loss: 0.007048243583154495\n",
      "STEP: 878400 - loss: 0.007047713372112972\n",
      "STEP: 878500 - loss: 0.007047183315516153\n",
      "STEP: 878600 - loss: 0.007046653413317207\n",
      "STEP: 878700 - loss: 0.007046123665469627\n",
      "STEP: 878800 - loss: 0.007045594071925912\n",
      "STEP: 878900 - loss: 0.007045064632639315\n",
      "STEP: 879000 - loss: 0.007044535347562568\n",
      "STEP: 879100 - loss: 0.007044006216649681\n",
      "STEP: 879200 - loss: 0.007043477239853086\n",
      "STEP: 879300 - loss: 0.007042948417126228\n",
      "STEP: 879400 - loss: 0.007042419748422189\n",
      "STEP: 879500 - loss: 0.007041891233694089\n",
      "STEP: 879600 - loss: 0.007041362872894997\n",
      "STEP: 879700 - loss: 0.00704083466597827\n",
      "STEP: 879800 - loss: 0.007040306612897425\n",
      "STEP: 879900 - loss: 0.007039778713605475\n",
      "STEP: 880000 - loss: 0.007039250968055223\n",
      "STEP: 880100 - loss: 0.007038723376200567\n",
      "STEP: 880200 - loss: 0.007038195937994156\n",
      "STEP: 880300 - loss: 0.007037668653389955\n",
      "STEP: 880400 - loss: 0.0070371415223408965\n",
      "STEP: 880500 - loss: 0.007036614544800197\n",
      "STEP: 880600 - loss: 0.007036087720721634\n",
      "STEP: 880700 - loss: 0.007035561050058139\n",
      "STEP: 880800 - loss: 0.00703503453276314\n",
      "STEP: 880900 - loss: 0.0070345081687902015\n",
      "STEP: 881000 - loss: 0.007033981958092526\n",
      "STEP: 881100 - loss: 0.007033455900623688\n",
      "STEP: 881200 - loss: 0.007032929996336706\n",
      "STEP: 881300 - loss: 0.007032404245185478\n",
      "STEP: 881400 - loss: 0.0070318786471231425\n",
      "STEP: 881500 - loss: 0.007031353202103352\n",
      "STEP: 881600 - loss: 0.007030827910079751\n",
      "STEP: 881700 - loss: 0.007030302771005426\n",
      "STEP: 881800 - loss: 0.007029777784833881\n",
      "STEP: 881900 - loss: 0.007029252951519053\n",
      "STEP: 882000 - loss: 0.007028728271014193\n",
      "STEP: 882100 - loss: 0.007028203743272799\n",
      "STEP: 882200 - loss: 0.007027679368248574\n",
      "STEP: 882300 - loss: 0.007027155145894914\n",
      "STEP: 882400 - loss: 0.007026631076165598\n",
      "STEP: 882500 - loss: 0.007026107159014039\n",
      "STEP: 882600 - loss: 0.007025583394394084\n",
      "STEP: 882700 - loss: 0.0070250597822591925\n",
      "STEP: 882800 - loss: 0.00702453632256299\n",
      "STEP: 882900 - loss: 0.007024013015259271\n",
      "STEP: 883000 - loss: 0.0070234898603014195\n",
      "STEP: 883100 - loss: 0.0070229668576435676\n",
      "STEP: 883200 - loss: 0.007022444007239177\n",
      "STEP: 883300 - loss: 0.007021921309041888\n",
      "STEP: 883400 - loss: 0.007021398763005766\n",
      "STEP: 883500 - loss: 0.007020876369083986\n",
      "STEP: 883600 - loss: 0.007020354127230598\n",
      "STEP: 883700 - loss: 0.007019832037399597\n",
      "STEP: 883800 - loss: 0.007019310099544378\n",
      "STEP: 883900 - loss: 0.007018788313618983\n",
      "STEP: 884000 - loss: 0.007018266679577069\n",
      "STEP: 884100 - loss: 0.007017745197372728\n",
      "STEP: 884200 - loss: 0.007017223866959609\n",
      "STEP: 884300 - loss: 0.007016702688291504\n",
      "STEP: 884400 - loss: 0.007016181661322308\n",
      "STEP: 884500 - loss: 0.007015660786006082\n",
      "STEP: 884600 - loss: 0.007015140062296395\n",
      "STEP: 884700 - loss: 0.007014619490147668\n",
      "STEP: 884800 - loss: 0.007014099069513444\n",
      "STEP: 884900 - loss: 0.007013578800347729\n",
      "STEP: 885000 - loss: 0.007013058682604327\n",
      "STEP: 885100 - loss: 0.007012538716237587\n",
      "STEP: 885200 - loss: 0.007012018901201102\n",
      "STEP: 885300 - loss: 0.007011499237449083\n",
      "STEP: 885400 - loss: 0.007010979724935489\n",
      "STEP: 885500 - loss: 0.007010460363614385\n",
      "STEP: 885600 - loss: 0.007009941153439706\n",
      "STEP: 885700 - loss: 0.007009422094365577\n",
      "STEP: 885800 - loss: 0.007008903186345956\n",
      "STEP: 885900 - loss: 0.007008384429334921\n",
      "STEP: 886000 - loss: 0.007007865823287121\n",
      "STEP: 886100 - loss: 0.007007347368155781\n",
      "STEP: 886200 - loss: 0.0070068290638955295\n",
      "STEP: 886300 - loss: 0.007006310910460706\n",
      "STEP: 886400 - loss: 0.007005792907804768\n",
      "STEP: 886500 - loss: 0.007005275055882681\n",
      "STEP: 886600 - loss: 0.007004757354647825\n",
      "STEP: 886700 - loss: 0.007004239804054986\n",
      "STEP: 886800 - loss: 0.007003722404058306\n",
      "STEP: 886900 - loss: 0.00700320515461157\n",
      "STEP: 887000 - loss: 0.007002688055669286\n",
      "STEP: 887100 - loss: 0.007002171107185774\n",
      "STEP: 887200 - loss: 0.007001654309115609\n",
      "STEP: 887300 - loss: 0.007001137661412239\n",
      "STEP: 887400 - loss: 0.0070006211640305845\n",
      "STEP: 887500 - loss: 0.007000104816924665\n",
      "STEP: 887600 - loss: 0.0069995886200490505\n",
      "STEP: 887700 - loss: 0.006999072573357754\n",
      "STEP: 887800 - loss: 0.006998556676805566\n",
      "STEP: 887900 - loss: 0.0069980409303462485\n",
      "STEP: 888000 - loss: 0.0069975253339347645\n",
      "STEP: 888100 - loss: 0.006997009887524828\n",
      "STEP: 888200 - loss: 0.006996494591071667\n",
      "STEP: 888300 - loss: 0.006995979444529269\n",
      "STEP: 888400 - loss: 0.0069954644478517234\n",
      "STEP: 888500 - loss: 0.006994949600994199\n",
      "STEP: 888600 - loss: 0.00699443490391054\n",
      "STEP: 888700 - loss: 0.006993920356555184\n",
      "STEP: 888800 - loss: 0.006993405958883196\n",
      "STEP: 888900 - loss: 0.006992891710848421\n",
      "STEP: 889000 - loss: 0.006992377612406189\n",
      "STEP: 889100 - loss: 0.00699186366351021\n",
      "STEP: 889200 - loss: 0.006991349864115216\n",
      "STEP: 889300 - loss: 0.006990836214175848\n",
      "STEP: 889400 - loss: 0.00699032271364707\n",
      "STEP: 889500 - loss: 0.006989809362482667\n",
      "STEP: 889600 - loss: 0.006989296160637769\n",
      "STEP: 889700 - loss: 0.006988783108067218\n",
      "STEP: 889800 - loss: 0.006988270204724922\n",
      "STEP: 889900 - loss: 0.006987757450566066\n",
      "STEP: 890000 - loss: 0.006987244845544707\n",
      "STEP: 890100 - loss: 0.006986732389616272\n",
      "STEP: 890200 - loss: 0.006986220082734952\n",
      "STEP: 890300 - loss: 0.006985707924855669\n",
      "STEP: 890400 - loss: 0.006985195915932904\n",
      "STEP: 890500 - loss: 0.006984684055921633\n",
      "STEP: 890600 - loss: 0.006984172344776582\n",
      "STEP: 890700 - loss: 0.00698366078245202\n",
      "STEP: 890800 - loss: 0.006983149368902981\n",
      "STEP: 890900 - loss: 0.006982638104084289\n",
      "STEP: 891000 - loss: 0.006982126987951104\n",
      "STEP: 891100 - loss: 0.006981616020457662\n",
      "STEP: 891200 - loss: 0.006981105201558921\n",
      "STEP: 891300 - loss: 0.006980594531209838\n",
      "STEP: 891400 - loss: 0.0069800840093650115\n",
      "STEP: 891500 - loss: 0.006979573635979669\n",
      "STEP: 891600 - loss: 0.0069790634110083695\n",
      "STEP: 891700 - loss: 0.006978553334406078\n",
      "STEP: 891800 - loss: 0.006978043406127848\n",
      "STEP: 891900 - loss: 0.006977533626128278\n",
      "STEP: 892000 - loss: 0.0069770239943626345\n",
      "STEP: 892100 - loss: 0.006976514510785485\n",
      "STEP: 892200 - loss: 0.006976005175352155\n",
      "STEP: 892300 - loss: 0.006975495988017366\n",
      "STEP: 892400 - loss: 0.006974986948736231\n",
      "STEP: 892500 - loss: 0.006974478057463658\n",
      "STEP: 892600 - loss: 0.006973969314154428\n",
      "STEP: 892700 - loss: 0.006973460718764054\n",
      "STEP: 892800 - loss: 0.006972952271247344\n",
      "STEP: 892900 - loss: 0.0069724439715590945\n",
      "STEP: 893000 - loss: 0.006971935819654778\n",
      "STEP: 893100 - loss: 0.006971427815489276\n",
      "STEP: 893200 - loss: 0.006970919959017626\n",
      "STEP: 893300 - loss: 0.006970412250195061\n",
      "STEP: 893400 - loss: 0.006969904688976525\n",
      "STEP: 893500 - loss: 0.006969397275317315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 893600 - loss: 0.006968890009172423\n",
      "STEP: 893700 - loss: 0.006968382890497145\n",
      "STEP: 893800 - loss: 0.00696787591924645\n",
      "STEP: 893900 - loss: 0.006967369095375667\n",
      "STEP: 894000 - loss: 0.00696686241884042\n",
      "STEP: 894100 - loss: 0.0069663558895950205\n",
      "STEP: 894200 - loss: 0.006965849507595158\n",
      "STEP: 894300 - loss: 0.006965343272796144\n",
      "STEP: 894400 - loss: 0.0069648371851533854\n",
      "STEP: 894500 - loss: 0.006964331244621596\n",
      "STEP: 894600 - loss: 0.006963825451156624\n",
      "STEP: 894700 - loss: 0.0069633198047132205\n",
      "STEP: 894800 - loss: 0.006962814305247268\n",
      "STEP: 894900 - loss: 0.006962308952713841\n",
      "STEP: 895000 - loss: 0.00696180374706797\n",
      "STEP: 895100 - loss: 0.006961298688265481\n",
      "STEP: 895200 - loss: 0.00696079377626127\n",
      "STEP: 895300 - loss: 0.006960289011011052\n",
      "STEP: 895400 - loss: 0.006959784392470187\n",
      "STEP: 895500 - loss: 0.006959279920594266\n",
      "STEP: 895600 - loss: 0.006958775595338066\n",
      "STEP: 895700 - loss: 0.006958271416657541\n",
      "STEP: 895800 - loss: 0.00695776738450773\n",
      "STEP: 895900 - loss: 0.006957263498844547\n",
      "STEP: 896000 - loss: 0.006956759759623335\n",
      "STEP: 896100 - loss: 0.006956256166799254\n",
      "STEP: 896200 - loss: 0.00695575272032845\n",
      "STEP: 896300 - loss: 0.006955249420165845\n",
      "STEP: 896400 - loss: 0.006954746266267103\n",
      "STEP: 896500 - loss: 0.0069542432585878115\n",
      "STEP: 896600 - loss: 0.0069537403970835725\n",
      "STEP: 896700 - loss: 0.006953237681709741\n",
      "STEP: 896800 - loss: 0.006952735112422353\n",
      "STEP: 896900 - loss: 0.0069522326891765405\n",
      "STEP: 897000 - loss: 0.006951730411927851\n",
      "STEP: 897100 - loss: 0.0069512282806321974\n",
      "STEP: 897200 - loss: 0.006950726295245254\n",
      "STEP: 897300 - loss: 0.006950224455722391\n",
      "STEP: 897400 - loss: 0.006949722762019571\n",
      "STEP: 897500 - loss: 0.00694922121409221\n",
      "STEP: 897600 - loss: 0.00694871981189601\n",
      "STEP: 897700 - loss: 0.006948218555386974\n",
      "STEP: 897800 - loss: 0.00694771744452044\n",
      "STEP: 897900 - loss: 0.006947216479252308\n",
      "STEP: 898000 - loss: 0.006946715659538136\n",
      "STEP: 898100 - loss: 0.00694621498533389\n",
      "STEP: 898200 - loss: 0.0069457144565954546\n",
      "STEP: 898300 - loss: 0.006945214073278251\n",
      "STEP: 898400 - loss: 0.006944713835338309\n",
      "STEP: 898500 - loss: 0.006944213742731534\n",
      "STEP: 898600 - loss: 0.006943713795413213\n",
      "STEP: 898700 - loss: 0.006943213993339807\n",
      "STEP: 898800 - loss: 0.006942714336466873\n",
      "STEP: 898900 - loss: 0.0069422148247503205\n",
      "STEP: 899000 - loss: 0.006941715458145971\n",
      "STEP: 899100 - loss: 0.006941216236609852\n",
      "STEP: 899200 - loss: 0.00694071716009762\n",
      "STEP: 899300 - loss: 0.006940218228565205\n",
      "STEP: 899400 - loss: 0.006939719441969092\n",
      "STEP: 899500 - loss: 0.006939220800264467\n",
      "STEP: 899600 - loss: 0.00693872230340759\n",
      "STEP: 899700 - loss: 0.006938223951354683\n",
      "STEP: 899800 - loss: 0.006937725744061175\n",
      "STEP: 899900 - loss: 0.006937227681483529\n",
      "STEP: 900000 - loss: 0.006936729763577616\n",
      "STEP: 900100 - loss: 0.006936231990299531\n",
      "STEP: 900200 - loss: 0.006935734361604884\n",
      "STEP: 900300 - loss: 0.00693523687745042\n",
      "STEP: 900400 - loss: 0.006934739537791834\n",
      "STEP: 900500 - loss: 0.0069342423425848565\n",
      "STEP: 900600 - loss: 0.006933745291786107\n",
      "STEP: 900700 - loss: 0.006933248385351629\n",
      "STEP: 900800 - loss: 0.00693275162323725\n",
      "STEP: 900900 - loss: 0.006932255005399175\n",
      "STEP: 901000 - loss: 0.00693175853179393\n",
      "STEP: 901100 - loss: 0.006931262202377076\n",
      "STEP: 901200 - loss: 0.006930766017105359\n",
      "STEP: 901300 - loss: 0.0069302699759341765\n",
      "STEP: 901400 - loss: 0.006929774078820716\n",
      "STEP: 901500 - loss: 0.006929278325720449\n",
      "STEP: 901600 - loss: 0.0069287827165898765\n",
      "STEP: 901700 - loss: 0.0069282872513852815\n",
      "STEP: 901800 - loss: 0.006927791930062524\n",
      "STEP: 901900 - loss: 0.006927296752578182\n",
      "STEP: 902000 - loss: 0.006926801718888839\n",
      "STEP: 902100 - loss: 0.006926306828950388\n",
      "STEP: 902200 - loss: 0.006925812082719073\n",
      "STEP: 902300 - loss: 0.006925317480151354\n",
      "STEP: 902400 - loss: 0.006924823021203419\n",
      "STEP: 902500 - loss: 0.006924328705831792\n",
      "STEP: 902600 - loss: 0.006923834533993064\n",
      "STEP: 902700 - loss: 0.006923340505642918\n",
      "STEP: 902800 - loss: 0.006922846620738094\n",
      "STEP: 902900 - loss: 0.006922352879235369\n",
      "STEP: 903000 - loss: 0.006921859281090549\n",
      "STEP: 903100 - loss: 0.00692136582626021\n",
      "STEP: 903200 - loss: 0.006920872514701089\n",
      "STEP: 903300 - loss: 0.006920379346369241\n",
      "STEP: 903400 - loss: 0.006919886321221376\n",
      "STEP: 903500 - loss: 0.006919393439213782\n",
      "STEP: 903600 - loss: 0.006918900700303073\n",
      "STEP: 903700 - loss: 0.006918408104445878\n",
      "STEP: 903800 - loss: 0.006917915651598659\n",
      "STEP: 903900 - loss: 0.006917423341717641\n",
      "STEP: 904000 - loss: 0.006916931174759611\n",
      "STEP: 904100 - loss: 0.006916439150680872\n",
      "STEP: 904200 - loss: 0.00691594726943841\n",
      "STEP: 904300 - loss: 0.006915455530988689\n",
      "STEP: 904400 - loss: 0.006914963935287949\n",
      "STEP: 904500 - loss: 0.006914472482293453\n",
      "STEP: 904600 - loss: 0.0069139811719609225\n",
      "STEP: 904700 - loss: 0.006913490004247817\n",
      "STEP: 904800 - loss: 0.0069129989791104724\n",
      "STEP: 904900 - loss: 0.006912508096505257\n",
      "STEP: 905000 - loss: 0.00691201735638919\n",
      "STEP: 905100 - loss: 0.0069115267587190055\n",
      "STEP: 905200 - loss: 0.006911036303451105\n",
      "STEP: 905300 - loss: 0.006910545990542599\n",
      "STEP: 905400 - loss: 0.006910055819949845\n",
      "STEP: 905500 - loss: 0.006909565791629835\n",
      "STEP: 905600 - loss: 0.006909075905538936\n",
      "STEP: 905700 - loss: 0.00690858616163427\n",
      "STEP: 905800 - loss: 0.006908096559872798\n",
      "STEP: 905900 - loss: 0.006907607100210743\n",
      "STEP: 906000 - loss: 0.00690711778260554\n",
      "STEP: 906100 - loss: 0.006906628607013204\n",
      "STEP: 906200 - loss: 0.006906139573391272\n",
      "STEP: 906300 - loss: 0.006905650681696051\n",
      "STEP: 906400 - loss: 0.006905161931885087\n",
      "STEP: 906500 - loss: 0.006904673323914835\n",
      "STEP: 906600 - loss: 0.006904184857741952\n",
      "STEP: 906700 - loss: 0.006903696533323796\n",
      "STEP: 906800 - loss: 0.006903208350616824\n",
      "STEP: 906900 - loss: 0.006902720309578147\n",
      "STEP: 907000 - loss: 0.006902232410164889\n",
      "STEP: 907100 - loss: 0.006901744652333845\n",
      "STEP: 907200 - loss: 0.006901257036042061\n",
      "STEP: 907300 - loss: 0.006900769561246316\n",
      "STEP: 907400 - loss: 0.006900282227903518\n",
      "STEP: 907500 - loss: 0.006899795035971062\n",
      "STEP: 907600 - loss: 0.00689930798540586\n",
      "STEP: 907700 - loss: 0.0068988210761647545\n",
      "STEP: 907800 - loss: 0.006898334308204783\n",
      "STEP: 907900 - loss: 0.0068978476814831645\n",
      "STEP: 908000 - loss: 0.006897361195956785\n",
      "STEP: 908100 - loss: 0.006896874851582921\n",
      "STEP: 908200 - loss: 0.006896388648318545\n",
      "STEP: 908300 - loss: 0.006895902586120506\n",
      "STEP: 908400 - loss: 0.006895416664946754\n",
      "STEP: 908500 - loss: 0.006894930884753328\n",
      "STEP: 908600 - loss: 0.006894445245498151\n",
      "STEP: 908700 - loss: 0.006893959747138262\n",
      "STEP: 908800 - loss: 0.006893474389630528\n",
      "STEP: 908900 - loss: 0.006892989172932053\n",
      "STEP: 909000 - loss: 0.00689250409700051\n",
      "STEP: 909100 - loss: 0.006892019161793175\n",
      "STEP: 909200 - loss: 0.006891534367266623\n",
      "STEP: 909300 - loss: 0.0068910497133783805\n",
      "STEP: 909400 - loss: 0.006890565200086136\n",
      "STEP: 909500 - loss: 0.006890080827346419\n",
      "STEP: 909600 - loss: 0.006889596595117044\n",
      "STEP: 909700 - loss: 0.006889112503354888\n",
      "STEP: 909800 - loss: 0.006888628552017727\n",
      "STEP: 909900 - loss: 0.006888144741062562\n",
      "STEP: 910000 - loss: 0.006887661070446768\n",
      "STEP: 910100 - loss: 0.006887177540127712\n",
      "STEP: 910200 - loss: 0.006886694150062649\n",
      "STEP: 910300 - loss: 0.00688621090020909\n",
      "STEP: 910400 - loss: 0.006885727790524129\n",
      "STEP: 910500 - loss: 0.006885244820965568\n",
      "STEP: 910600 - loss: 0.006884761991490766\n",
      "STEP: 910700 - loss: 0.006884279302056808\n",
      "STEP: 910800 - loss: 0.006883796752621271\n",
      "STEP: 910900 - loss: 0.006883314343141568\n",
      "STEP: 911000 - loss: 0.006882832073575431\n",
      "STEP: 911100 - loss: 0.006882349943880114\n",
      "STEP: 911200 - loss: 0.0068818679540131366\n",
      "STEP: 911300 - loss: 0.006881386103931536\n",
      "STEP: 911400 - loss: 0.006880904393593597\n",
      "STEP: 911500 - loss: 0.00688042282295646\n",
      "STEP: 911600 - loss: 0.006879941391977717\n",
      "STEP: 911700 - loss: 0.006879460100614615\n",
      "STEP: 911800 - loss: 0.00687897894882539\n",
      "STEP: 911900 - loss: 0.006878497936566906\n",
      "STEP: 912000 - loss: 0.006878017063796965\n",
      "STEP: 912100 - loss: 0.006877536330473481\n",
      "STEP: 912200 - loss: 0.006877055736553695\n",
      "STEP: 912300 - loss: 0.006876575281995563\n",
      "STEP: 912400 - loss: 0.0068760949667564615\n",
      "STEP: 912500 - loss: 0.0068756147907938865\n",
      "STEP: 912600 - loss: 0.006875134754066\n",
      "STEP: 912700 - loss: 0.006874654856529621\n",
      "STEP: 912800 - loss: 0.006874175098143584\n",
      "STEP: 912900 - loss: 0.006873695478864792\n",
      "STEP: 913000 - loss: 0.006873215998651277\n",
      "STEP: 913100 - loss: 0.006872736657460672\n",
      "STEP: 913200 - loss: 0.006872257455250751\n",
      "STEP: 913300 - loss: 0.0068717783919790484\n",
      "STEP: 913400 - loss: 0.006871299467603337\n",
      "STEP: 913500 - loss: 0.006870820682081924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 913600 - loss: 0.006870342035372059\n",
      "STEP: 913700 - loss: 0.006869863527431892\n",
      "STEP: 913800 - loss: 0.006869385158219075\n",
      "STEP: 913900 - loss: 0.0068689069276912495\n",
      "STEP: 914000 - loss: 0.0068684288358064035\n",
      "STEP: 914100 - loss: 0.0068679508825228944\n",
      "STEP: 914200 - loss: 0.006867473067797697\n",
      "STEP: 914300 - loss: 0.006866995391589229\n",
      "STEP: 914400 - loss: 0.006866517853855361\n",
      "STEP: 914500 - loss: 0.006866040454553664\n",
      "STEP: 914600 - loss: 0.0068655631936424865\n",
      "STEP: 914700 - loss: 0.006865086071079646\n",
      "STEP: 914800 - loss: 0.006864609086823091\n",
      "STEP: 914900 - loss: 0.006864132240830681\n",
      "STEP: 915000 - loss: 0.006863655533060295\n",
      "STEP: 915100 - loss: 0.006863178963469987\n",
      "STEP: 915200 - loss: 0.006862702532018034\n",
      "STEP: 915300 - loss: 0.006862226238661981\n",
      "STEP: 915400 - loss: 0.006861750083360524\n",
      "STEP: 915500 - loss: 0.006861274066071068\n",
      "STEP: 915600 - loss: 0.006860798186751655\n",
      "STEP: 915700 - loss: 0.006860322445360392\n",
      "STEP: 915800 - loss: 0.006859846841855727\n",
      "STEP: 915900 - loss: 0.006859371376195441\n",
      "STEP: 916000 - loss: 0.006858896048338038\n",
      "STEP: 916100 - loss: 0.006858420858241045\n",
      "STEP: 916200 - loss: 0.00685794580586308\n",
      "STEP: 916300 - loss: 0.006857470891161654\n",
      "STEP: 916400 - loss: 0.00685699611409527\n",
      "STEP: 916500 - loss: 0.0068565214746225235\n",
      "STEP: 916600 - loss: 0.006856046972700843\n",
      "STEP: 916700 - loss: 0.006855572608288843\n",
      "STEP: 916800 - loss: 0.0068550983813449665\n",
      "STEP: 916900 - loss: 0.0068546242918267\n",
      "STEP: 917000 - loss: 0.006854150339692851\n",
      "STEP: 917100 - loss: 0.00685367652490157\n",
      "STEP: 917200 - loss: 0.0068532028474107805\n",
      "STEP: 917300 - loss: 0.006852729307178872\n",
      "STEP: 917400 - loss: 0.00685225590416462\n",
      "STEP: 917500 - loss: 0.006851782638325707\n",
      "STEP: 917600 - loss: 0.0068513095096206286\n",
      "STEP: 917700 - loss: 0.006850836518007747\n",
      "STEP: 917800 - loss: 0.006850363663445447\n",
      "STEP: 917900 - loss: 0.006849890945891906\n",
      "STEP: 918000 - loss: 0.006849418365305458\n",
      "STEP: 918100 - loss: 0.006848945921644799\n",
      "STEP: 918200 - loss: 0.006848473614868072\n",
      "STEP: 918300 - loss: 0.006848001444933478\n",
      "STEP: 918400 - loss: 0.0068475294117996115\n",
      "STEP: 918500 - loss: 0.006847057515425017\n",
      "STEP: 918600 - loss: 0.006846585755767853\n",
      "STEP: 918700 - loss: 0.006846114132786843\n",
      "STEP: 918800 - loss: 0.006845642646440148\n",
      "STEP: 918900 - loss: 0.006845171296686442\n",
      "STEP: 919000 - loss: 0.006844700083484351\n",
      "STEP: 919100 - loss: 0.006844229006791705\n",
      "STEP: 919200 - loss: 0.006843758066567764\n",
      "STEP: 919300 - loss: 0.006843287262770828\n",
      "STEP: 919400 - loss: 0.0068428165953588445\n",
      "STEP: 919500 - loss: 0.0068423460642914715\n",
      "STEP: 919600 - loss: 0.006841875669526399\n",
      "STEP: 919700 - loss: 0.006841405411022518\n",
      "STEP: 919800 - loss: 0.006840935288738003\n",
      "STEP: 919900 - loss: 0.006840465302631935\n",
      "STEP: 920000 - loss: 0.006839995452663001\n",
      "STEP: 920100 - loss: 0.0068395257387893135\n",
      "STEP: 920200 - loss: 0.006839056160970069\n",
      "STEP: 920300 - loss: 0.006838586719162961\n",
      "STEP: 920400 - loss: 0.006838117413328089\n",
      "STEP: 920500 - loss: 0.00683764824342273\n",
      "STEP: 920600 - loss: 0.006837179209406373\n",
      "STEP: 920700 - loss: 0.006836710311237522\n",
      "STEP: 920800 - loss: 0.006836241548874744\n",
      "STEP: 920900 - loss: 0.0068357729222769115\n",
      "STEP: 921000 - loss: 0.0068353044314026805\n",
      "STEP: 921100 - loss: 0.006834836076210811\n",
      "STEP: 921200 - loss: 0.006834367856660227\n",
      "STEP: 921300 - loss: 0.006833899772709362\n",
      "STEP: 921400 - loss: 0.006833431824317263\n",
      "STEP: 921500 - loss: 0.006832964011442633\n",
      "STEP: 921600 - loss: 0.006832496334044226\n",
      "STEP: 921700 - loss: 0.00683202879208088\n",
      "STEP: 921800 - loss: 0.006831561385511493\n",
      "STEP: 921900 - loss: 0.006831094114294922\n",
      "STEP: 922000 - loss: 0.006830626978390003\n",
      "STEP: 922100 - loss: 0.00683015997775558\n",
      "STEP: 922200 - loss: 0.0068296931123503725\n",
      "STEP: 922300 - loss: 0.0068292263821335105\n",
      "STEP: 922400 - loss: 0.006828759787063964\n",
      "STEP: 922500 - loss: 0.006828293327100529\n",
      "STEP: 922600 - loss: 0.006827827002201926\n",
      "STEP: 922700 - loss: 0.006827360812327363\n",
      "STEP: 922800 - loss: 0.006826894757435691\n",
      "STEP: 922900 - loss: 0.006826428837486001\n",
      "STEP: 923000 - loss: 0.006825963052437147\n",
      "STEP: 923100 - loss: 0.006825497402248115\n",
      "STEP: 923200 - loss: 0.006825031886878166\n",
      "STEP: 923300 - loss: 0.00682456650628598\n",
      "STEP: 923400 - loss: 0.006824101260430657\n",
      "STEP: 923500 - loss: 0.006823636149271023\n",
      "STEP: 923600 - loss: 0.006823171172766861\n",
      "STEP: 923700 - loss: 0.006822706330876891\n",
      "STEP: 923800 - loss: 0.006822241623559518\n",
      "STEP: 923900 - loss: 0.006821777050774855\n",
      "STEP: 924000 - loss: 0.00682131261248143\n",
      "STEP: 924100 - loss: 0.006820848308638593\n",
      "STEP: 924200 - loss: 0.006820384139205211\n",
      "STEP: 924300 - loss: 0.006819920104140831\n",
      "STEP: 924400 - loss: 0.006819456203404168\n",
      "STEP: 924500 - loss: 0.006818992436954707\n",
      "STEP: 924600 - loss: 0.006818528804751313\n",
      "STEP: 924700 - loss: 0.006818065306753644\n",
      "STEP: 924800 - loss: 0.006817601942920248\n",
      "STEP: 924900 - loss: 0.006817138713210982\n",
      "STEP: 925000 - loss: 0.006816675617584743\n",
      "STEP: 925100 - loss: 0.006816212656001163\n",
      "STEP: 925200 - loss: 0.006815749828419005\n",
      "STEP: 925300 - loss: 0.0068152871347976806\n",
      "STEP: 925400 - loss: 0.006814824575096514\n",
      "STEP: 925500 - loss: 0.006814362149274758\n",
      "STEP: 925600 - loss: 0.006813899857291769\n",
      "STEP: 925700 - loss: 0.00681343769910705\n",
      "STEP: 925800 - loss: 0.006812975674679507\n",
      "STEP: 925900 - loss: 0.006812513783968956\n",
      "STEP: 926000 - loss: 0.00681205202693398\n",
      "STEP: 926100 - loss: 0.006811590403534977\n",
      "STEP: 926200 - loss: 0.006811128913730422\n",
      "STEP: 926300 - loss: 0.006810667557480314\n",
      "STEP: 926400 - loss: 0.00681020633474399\n",
      "STEP: 926500 - loss: 0.006809745245480591\n",
      "STEP: 926600 - loss: 0.006809284289649366\n",
      "STEP: 926700 - loss: 0.006808823467210516\n",
      "STEP: 926800 - loss: 0.006808362778122669\n",
      "STEP: 926900 - loss: 0.006807902222345832\n",
      "STEP: 927000 - loss: 0.006807441799839223\n",
      "STEP: 927100 - loss: 0.006806981510562765\n",
      "STEP: 927200 - loss: 0.006806521354475065\n",
      "STEP: 927300 - loss: 0.006806061331536437\n",
      "STEP: 927400 - loss: 0.006805601441706143\n",
      "STEP: 927500 - loss: 0.006805141684943654\n",
      "STEP: 927600 - loss: 0.006804682061208748\n",
      "STEP: 927700 - loss: 0.0068042225704607285\n",
      "STEP: 927800 - loss: 0.006803763212659355\n",
      "STEP: 927900 - loss: 0.006803303987764281\n",
      "STEP: 928000 - loss: 0.0068028448957346875\n",
      "STEP: 928100 - loss: 0.00680238593653056\n",
      "STEP: 928200 - loss: 0.00680192711011141\n",
      "STEP: 928300 - loss: 0.006801468416436961\n",
      "STEP: 928400 - loss: 0.006801009855466969\n",
      "STEP: 928500 - loss: 0.00680055142716071\n",
      "STEP: 928600 - loss: 0.006800093131477905\n",
      "STEP: 928700 - loss: 0.006799634968378571\n",
      "STEP: 928800 - loss: 0.006799176937822492\n",
      "STEP: 928900 - loss: 0.006798719039768754\n",
      "STEP: 929000 - loss: 0.0067982612741776275\n",
      "STEP: 929100 - loss: 0.006797803641008548\n",
      "STEP: 929200 - loss: 0.006797346140221661\n",
      "STEP: 929300 - loss: 0.006796888771776033\n",
      "STEP: 929400 - loss: 0.006796431535632331\n",
      "STEP: 929500 - loss: 0.0067959744317495725\n",
      "STEP: 929600 - loss: 0.006795517460087943\n",
      "STEP: 929700 - loss: 0.006795060620607205\n",
      "STEP: 929800 - loss: 0.006794603913267072\n",
      "STEP: 929900 - loss: 0.006794147338027287\n",
      "STEP: 930000 - loss: 0.006793690894848224\n",
      "STEP: 930100 - loss: 0.006793234583689076\n",
      "STEP: 930200 - loss: 0.0067927784045101235\n",
      "STEP: 930300 - loss: 0.006792322357270894\n",
      "STEP: 930400 - loss: 0.0067918664419317765\n",
      "STEP: 930500 - loss: 0.006791410658452289\n",
      "STEP: 930600 - loss: 0.006790955006792626\n",
      "STEP: 930700 - loss: 0.006790499486912342\n",
      "STEP: 930800 - loss: 0.006790044098771694\n",
      "STEP: 930900 - loss: 0.006789588842330603\n",
      "STEP: 931000 - loss: 0.006789133717548958\n",
      "STEP: 931100 - loss: 0.006788678724386643\n",
      "STEP: 931200 - loss: 0.006788223862804149\n",
      "STEP: 931300 - loss: 0.006787769132760785\n",
      "STEP: 931400 - loss: 0.006787314534216977\n",
      "STEP: 931500 - loss: 0.006786860067132654\n",
      "STEP: 931600 - loss: 0.006786405731467954\n",
      "STEP: 931700 - loss: 0.006785951527182546\n",
      "STEP: 931800 - loss: 0.006785497454237165\n",
      "STEP: 931900 - loss: 0.006785043512591156\n",
      "STEP: 932000 - loss: 0.006784589702205209\n",
      "STEP: 932100 - loss: 0.006784136023039067\n",
      "STEP: 932200 - loss: 0.006783682475052999\n",
      "STEP: 932300 - loss: 0.006783229058207075\n",
      "STEP: 932400 - loss: 0.006782775772461551\n",
      "STEP: 932500 - loss: 0.006782322617776423\n",
      "STEP: 932600 - loss: 0.0067818695941116985\n",
      "STEP: 932700 - loss: 0.006781416701427827\n",
      "STEP: 932800 - loss: 0.006780963939684985\n",
      "STEP: 932900 - loss: 0.0067805113088432924\n",
      "STEP: 933000 - loss: 0.006780058808863147\n",
      "STEP: 933100 - loss: 0.006779606439704221\n",
      "STEP: 933200 - loss: 0.006779154201327487\n",
      "STEP: 933300 - loss: 0.006778702093692792\n",
      "STEP: 933400 - loss: 0.006778250116760295\n",
      "STEP: 933500 - loss: 0.006777798270490454\n",
      "STEP: 933600 - loss: 0.006777346554843605\n",
      "STEP: 933700 - loss: 0.006776894969779787\n",
      "STEP: 933800 - loss: 0.006776443515259417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 933900 - loss: 0.006775992191243123\n",
      "STEP: 934000 - loss: 0.006775540997690814\n",
      "STEP: 934100 - loss: 0.006775089934562854\n",
      "STEP: 934200 - loss: 0.006774639001819967\n",
      "STEP: 934300 - loss: 0.006774188199422042\n",
      "STEP: 934400 - loss: 0.006773737527330058\n",
      "STEP: 934500 - loss: 0.006773286985503701\n",
      "STEP: 934600 - loss: 0.006772836573904153\n",
      "STEP: 934700 - loss: 0.006772386292491108\n",
      "STEP: 934800 - loss: 0.006771936141225325\n",
      "STEP: 934900 - loss: 0.006771486120067176\n",
      "STEP: 935000 - loss: 0.0067710362289771465\n",
      "STEP: 935100 - loss: 0.006770586467915956\n",
      "STEP: 935200 - loss: 0.006770136836843509\n",
      "STEP: 935300 - loss: 0.006769687335720499\n",
      "STEP: 935400 - loss: 0.006769237964507966\n",
      "STEP: 935500 - loss: 0.006768788723165622\n",
      "STEP: 935600 - loss: 0.00676833961165472\n",
      "STEP: 935700 - loss: 0.0067678906299350455\n",
      "STEP: 935800 - loss: 0.006767441777967764\n",
      "STEP: 935900 - loss: 0.006766993055713226\n",
      "STEP: 936000 - loss: 0.006766544463131819\n",
      "STEP: 936100 - loss: 0.006766096000184625\n",
      "STEP: 936200 - loss: 0.006765647666831613\n",
      "STEP: 936300 - loss: 0.006765199463033736\n",
      "STEP: 936400 - loss: 0.006764751388751572\n",
      "STEP: 936500 - loss: 0.0067643034439458976\n",
      "STEP: 936600 - loss: 0.006763855628577146\n",
      "STEP: 936700 - loss: 0.006763407942605801\n",
      "STEP: 936800 - loss: 0.006762960385992783\n",
      "STEP: 936900 - loss: 0.006762512958699199\n",
      "STEP: 937000 - loss: 0.006762065660685062\n",
      "STEP: 937100 - loss: 0.006761618491911004\n",
      "STEP: 937200 - loss: 0.0067611714523384174\n",
      "STEP: 937300 - loss: 0.006760724541927748\n",
      "STEP: 937400 - loss: 0.0067602777606392914\n",
      "STEP: 937500 - loss: 0.006759831108434438\n",
      "STEP: 937600 - loss: 0.0067593845852734315\n",
      "STEP: 937700 - loss: 0.006758938191117249\n",
      "STEP: 937800 - loss: 0.006758491925926748\n",
      "STEP: 937900 - loss: 0.006758045789662841\n",
      "STEP: 938000 - loss: 0.006757599782286197\n",
      "STEP: 938100 - loss: 0.006757153903757548\n",
      "STEP: 938200 - loss: 0.006756708154037688\n",
      "STEP: 938300 - loss: 0.006756262533087818\n",
      "STEP: 938400 - loss: 0.006755817040868356\n",
      "STEP: 938500 - loss: 0.006755371677340502\n",
      "STEP: 938600 - loss: 0.0067549264424651995\n",
      "STEP: 938700 - loss: 0.006754481336202841\n",
      "STEP: 938800 - loss: 0.00675403635851492\n",
      "STEP: 938900 - loss: 0.00675359150936173\n",
      "STEP: 939000 - loss: 0.006753146788705017\n",
      "STEP: 939100 - loss: 0.006752702196505057\n",
      "STEP: 939200 - loss: 0.006752257732722711\n",
      "STEP: 939300 - loss: 0.006751813397319598\n",
      "STEP: 939400 - loss: 0.006751369190256348\n",
      "STEP: 939500 - loss: 0.006750925111493997\n",
      "STEP: 939600 - loss: 0.00675048116099311\n",
      "STEP: 939700 - loss: 0.006750037338715387\n",
      "STEP: 939800 - loss: 0.006749593644621609\n",
      "STEP: 939900 - loss: 0.006749150078672786\n",
      "STEP: 940000 - loss: 0.006748706640829766\n",
      "STEP: 940100 - loss: 0.006748263331053737\n",
      "STEP: 940200 - loss: 0.006747820149306277\n",
      "STEP: 940300 - loss: 0.006747377095547321\n",
      "STEP: 940400 - loss: 0.006746934169739082\n",
      "STEP: 940500 - loss: 0.0067464913718421875\n",
      "STEP: 940600 - loss: 0.006746048701817528\n",
      "STEP: 940700 - loss: 0.006745606159626744\n",
      "STEP: 940800 - loss: 0.006745163745230874\n",
      "STEP: 940900 - loss: 0.006744721458590876\n",
      "STEP: 941000 - loss: 0.006744279299668109\n",
      "STEP: 941100 - loss: 0.006743837268423454\n",
      "STEP: 941200 - loss: 0.006743395364818078\n",
      "STEP: 941300 - loss: 0.006742953588813806\n",
      "STEP: 941400 - loss: 0.006742511940371371\n",
      "STEP: 941500 - loss: 0.006742070419451895\n",
      "STEP: 941600 - loss: 0.006741629026017115\n",
      "STEP: 941700 - loss: 0.006741187760027646\n",
      "STEP: 941800 - loss: 0.006740746621445066\n",
      "STEP: 941900 - loss: 0.006740305610230778\n",
      "STEP: 942000 - loss: 0.006739864726345765\n",
      "STEP: 942100 - loss: 0.0067394239697516085\n",
      "STEP: 942200 - loss: 0.006738983340409267\n",
      "STEP: 942300 - loss: 0.006738542838280688\n",
      "STEP: 942400 - loss: 0.006738102463326588\n",
      "STEP: 942500 - loss: 0.0067376622155087065\n",
      "STEP: 942600 - loss: 0.0067372220947880725\n",
      "STEP: 942700 - loss: 0.006736782101126\n",
      "STEP: 942800 - loss: 0.006736342234484327\n",
      "STEP: 942900 - loss: 0.006735902494824092\n",
      "STEP: 943000 - loss: 0.006735462882106867\n",
      "STEP: 943100 - loss: 0.006735023396293993\n",
      "STEP: 943200 - loss: 0.006734584037346831\n",
      "STEP: 943300 - loss: 0.0067341448052270474\n",
      "STEP: 943400 - loss: 0.006733705699895884\n",
      "STEP: 943500 - loss: 0.006733266721314639\n",
      "STEP: 943600 - loss: 0.006732827869445096\n",
      "STEP: 943700 - loss: 0.00673238914424869\n",
      "STEP: 943800 - loss: 0.006731950545687043\n",
      "STEP: 943900 - loss: 0.006731512073721227\n",
      "STEP: 944000 - loss: 0.006731073728313138\n",
      "STEP: 944100 - loss: 0.006730635509424129\n",
      "STEP: 944200 - loss: 0.006730197417015769\n",
      "STEP: 944300 - loss: 0.0067297594510498885\n",
      "STEP: 944400 - loss: 0.006729321611487498\n",
      "STEP: 944500 - loss: 0.006728883898290766\n",
      "STEP: 944600 - loss: 0.006728446311421084\n",
      "STEP: 944700 - loss: 0.006728008850839703\n",
      "STEP: 944800 - loss: 0.006727571516508659\n",
      "STEP: 944900 - loss: 0.00672713430838949\n",
      "STEP: 945000 - loss: 0.006726697226443354\n",
      "STEP: 945100 - loss: 0.006726260270632915\n",
      "STEP: 945200 - loss: 0.006725823440918986\n",
      "STEP: 945300 - loss: 0.006725386737263414\n",
      "STEP: 945400 - loss: 0.006724950159628172\n",
      "STEP: 945500 - loss: 0.00672451370797462\n",
      "STEP: 945600 - loss: 0.0067240773822646056\n",
      "STEP: 945700 - loss: 0.00672364118245968\n",
      "STEP: 945800 - loss: 0.006723205108522206\n",
      "STEP: 945900 - loss: 0.006722769160413029\n",
      "STEP: 946000 - loss: 0.006722333338094556\n",
      "STEP: 946100 - loss: 0.006721897641528331\n",
      "STEP: 946200 - loss: 0.006721462070675754\n",
      "STEP: 946300 - loss: 0.006721026625499271\n",
      "STEP: 946400 - loss: 0.006720591305960211\n",
      "STEP: 946500 - loss: 0.006720156112020809\n",
      "STEP: 946600 - loss: 0.006719721043642536\n",
      "STEP: 946700 - loss: 0.006719286100787395\n",
      "STEP: 946800 - loss: 0.006718851283417082\n",
      "STEP: 946900 - loss: 0.006718416591493489\n",
      "STEP: 947000 - loss: 0.006717982024978655\n",
      "STEP: 947100 - loss: 0.006717547583834346\n",
      "STEP: 947200 - loss: 0.006717113268022275\n",
      "STEP: 947300 - loss: 0.006716679077504806\n",
      "STEP: 947400 - loss: 0.006716245012243415\n",
      "STEP: 947500 - loss: 0.0067158110722004335\n",
      "STEP: 947600 - loss: 0.006715377257337172\n",
      "STEP: 947700 - loss: 0.006714943567616327\n",
      "STEP: 947800 - loss: 0.0067145100029989664\n",
      "STEP: 947900 - loss: 0.006714076563448213\n",
      "STEP: 948000 - loss: 0.006713643248925254\n",
      "STEP: 948100 - loss: 0.0067132100593923095\n",
      "STEP: 948200 - loss: 0.006712776994811279\n",
      "STEP: 948300 - loss: 0.006712344055144129\n",
      "STEP: 948400 - loss: 0.00671191124035318\n",
      "STEP: 948500 - loss: 0.006711478550400218\n",
      "STEP: 948600 - loss: 0.0067110459852476115\n",
      "STEP: 948700 - loss: 0.006710613544856939\n",
      "STEP: 948800 - loss: 0.006710181229190445\n",
      "STEP: 948900 - loss: 0.006709749038210646\n",
      "STEP: 949000 - loss: 0.0067093169718794015\n",
      "STEP: 949100 - loss: 0.006708885030158542\n",
      "STEP: 949200 - loss: 0.006708453213010319\n",
      "STEP: 949300 - loss: 0.006708021520397281\n",
      "STEP: 949400 - loss: 0.006707589952280858\n",
      "STEP: 949500 - loss: 0.006707158508623604\n",
      "STEP: 949600 - loss: 0.00670672718938779\n",
      "STEP: 949700 - loss: 0.006706295994535504\n",
      "STEP: 949800 - loss: 0.006705864924029024\n",
      "STEP: 949900 - loss: 0.00670543397783029\n",
      "STEP: 950000 - loss: 0.006705003155901637\n",
      "STEP: 950100 - loss: 0.006704572458205553\n",
      "STEP: 950200 - loss: 0.006704141884703885\n",
      "STEP: 950300 - loss: 0.0067037114353590225\n",
      "STEP: 950400 - loss: 0.006703281110133517\n",
      "STEP: 950500 - loss: 0.006702850908988906\n",
      "STEP: 950600 - loss: 0.006702420831888334\n",
      "STEP: 950700 - loss: 0.006701990878793672\n",
      "STEP: 950800 - loss: 0.006701561049667394\n",
      "STEP: 950900 - loss: 0.006701131344471393\n",
      "STEP: 951000 - loss: 0.006700701763168566\n",
      "STEP: 951100 - loss: 0.006700272305721013\n",
      "STEP: 951200 - loss: 0.006699842972090776\n",
      "STEP: 951300 - loss: 0.006699413762240755\n",
      "STEP: 951400 - loss: 0.006698984676132903\n",
      "STEP: 951500 - loss: 0.006698555713729898\n",
      "STEP: 951600 - loss: 0.0066981268749941245\n",
      "STEP: 951700 - loss: 0.006697698159887791\n",
      "STEP: 951800 - loss: 0.006697269568373593\n",
      "STEP: 951900 - loss: 0.006696841100413617\n",
      "STEP: 952000 - loss: 0.006696412755970613\n",
      "STEP: 952100 - loss: 0.0066959845350070876\n",
      "STEP: 952200 - loss: 0.006695556437485095\n",
      "STEP: 952300 - loss: 0.006695128463367256\n",
      "STEP: 952400 - loss: 0.0066947006126162455\n",
      "STEP: 952500 - loss: 0.006694272885194792\n",
      "STEP: 952600 - loss: 0.006693845281064677\n",
      "STEP: 952700 - loss: 0.0066934178001890774\n",
      "STEP: 952800 - loss: 0.006692990442530229\n",
      "STEP: 952900 - loss: 0.0066925632080508625\n",
      "STEP: 953000 - loss: 0.006692136096713397\n",
      "STEP: 953100 - loss: 0.006691709108480494\n",
      "STEP: 953200 - loss: 0.006691282243314477\n",
      "STEP: 953300 - loss: 0.006690855501178201\n",
      "STEP: 953400 - loss: 0.006690428882034595\n",
      "STEP: 953500 - loss: 0.006690002385845659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 953600 - loss: 0.0066895760125742915\n",
      "STEP: 953700 - loss: 0.006689149762182975\n",
      "STEP: 953800 - loss: 0.006688723634634479\n",
      "STEP: 953900 - loss: 0.0066882976298916\n",
      "STEP: 954000 - loss: 0.006687871747916944\n",
      "STEP: 954100 - loss: 0.006687445988672941\n",
      "STEP: 954200 - loss: 0.0066870203521226775\n",
      "STEP: 954300 - loss: 0.006686594838228454\n",
      "STEP: 954400 - loss: 0.006686169446953251\n",
      "STEP: 954500 - loss: 0.006685744178259739\n",
      "STEP: 954600 - loss: 0.006685319032110762\n",
      "STEP: 954700 - loss: 0.006684894008468974\n",
      "STEP: 954800 - loss: 0.00668446910729697\n",
      "STEP: 954900 - loss: 0.006684044328558009\n",
      "STEP: 955000 - loss: 0.006683619672214235\n",
      "STEP: 955100 - loss: 0.0066831951382289746\n",
      "STEP: 955200 - loss: 0.00668277072656479\n",
      "STEP: 955300 - loss: 0.00668234643718469\n",
      "STEP: 955400 - loss: 0.006681922270051099\n",
      "STEP: 955500 - loss: 0.006681498225127476\n",
      "STEP: 955600 - loss: 0.006681074302375983\n",
      "STEP: 955700 - loss: 0.006680650501759996\n",
      "STEP: 955800 - loss: 0.006680226823242158\n",
      "STEP: 955900 - loss: 0.006679803266785711\n",
      "STEP: 956000 - loss: 0.006679379832353208\n",
      "STEP: 956100 - loss: 0.006678956519907289\n",
      "STEP: 956200 - loss: 0.0066785333294115\n",
      "STEP: 956300 - loss: 0.006678110260828351\n",
      "STEP: 956400 - loss: 0.0066776873141210944\n",
      "STEP: 956500 - loss: 0.0066772644892523075\n",
      "STEP: 956600 - loss: 0.006676841786185213\n",
      "STEP: 956700 - loss: 0.006676419204882898\n",
      "STEP: 956800 - loss: 0.006675996745307858\n",
      "STEP: 956900 - loss: 0.006675574407423753\n",
      "STEP: 957000 - loss: 0.006675152191193437\n",
      "STEP: 957100 - loss: 0.006674730096579438\n",
      "STEP: 957200 - loss: 0.006674308123545092\n",
      "STEP: 957300 - loss: 0.006673886272053729\n",
      "STEP: 957400 - loss: 0.006673464542068052\n",
      "STEP: 957500 - loss: 0.006673042933551182\n",
      "STEP: 957600 - loss: 0.006672621446466286\n",
      "STEP: 957700 - loss: 0.006672200080776322\n",
      "STEP: 957800 - loss: 0.006671778836444911\n",
      "STEP: 957900 - loss: 0.0066713577134344\n",
      "STEP: 958000 - loss: 0.006670936711708241\n",
      "STEP: 958100 - loss: 0.006670515831229719\n",
      "STEP: 958200 - loss: 0.00667009507196193\n",
      "STEP: 958300 - loss: 0.006669674433867627\n",
      "STEP: 958400 - loss: 0.006669253916910517\n",
      "STEP: 958500 - loss: 0.00666883352105368\n",
      "STEP: 958600 - loss: 0.006668413246259895\n",
      "STEP: 958700 - loss: 0.006667993092492981\n",
      "STEP: 958800 - loss: 0.006667573059715934\n",
      "STEP: 958900 - loss: 0.006667153147891297\n",
      "STEP: 959000 - loss: 0.006666733356983573\n",
      "STEP: 959100 - loss: 0.006666313686955182\n",
      "STEP: 959200 - loss: 0.0066658941377694\n",
      "STEP: 959300 - loss: 0.006665474709389581\n",
      "STEP: 959400 - loss: 0.006665055401779303\n",
      "STEP: 959500 - loss: 0.0066646362149012945\n",
      "STEP: 959600 - loss: 0.006664217148719326\n",
      "STEP: 959700 - loss: 0.006663798203196675\n",
      "STEP: 959800 - loss: 0.006663379378296639\n",
      "STEP: 959900 - loss: 0.0066629606739820445\n",
      "STEP: 960000 - loss: 0.006662542090217109\n",
      "STEP: 960100 - loss: 0.006662123626964546\n",
      "STEP: 960200 - loss: 0.006661705284187862\n",
      "STEP: 960300 - loss: 0.006661287061850851\n",
      "STEP: 960400 - loss: 0.006660868959916315\n",
      "STEP: 960500 - loss: 0.006660450978347878\n",
      "STEP: 960600 - loss: 0.006660033117109198\n",
      "STEP: 960700 - loss: 0.0066596153761633815\n",
      "STEP: 960800 - loss: 0.006659197755474103\n",
      "STEP: 960900 - loss: 0.006658780255004515\n",
      "STEP: 961000 - loss: 0.006658362874718217\n",
      "STEP: 961100 - loss: 0.006657945614578985\n",
      "STEP: 961200 - loss: 0.006657528474549907\n",
      "STEP: 961300 - loss: 0.006657111454594429\n",
      "STEP: 961400 - loss: 0.00665669455467625\n",
      "STEP: 961500 - loss: 0.0066562777747589605\n",
      "STEP: 961600 - loss: 0.006655861114806047\n",
      "STEP: 961700 - loss: 0.00665544457478086\n",
      "STEP: 961800 - loss: 0.0066550281546473595\n",
      "STEP: 961900 - loss: 0.00665461185436856\n",
      "STEP: 962000 - loss: 0.006654195673908593\n",
      "STEP: 962100 - loss: 0.006653779613230324\n",
      "STEP: 962200 - loss: 0.006653363672297968\n",
      "STEP: 962300 - loss: 0.00665294785107487\n",
      "STEP: 962400 - loss: 0.0066525321495247034\n",
      "STEP: 962500 - loss: 0.00665211656761137\n",
      "STEP: 962600 - loss: 0.0066517011052977856\n",
      "STEP: 962700 - loss: 0.006651285762548207\n",
      "STEP: 962800 - loss: 0.006650870539326384\n",
      "STEP: 962900 - loss: 0.006650455435595629\n",
      "STEP: 963000 - loss: 0.006650040451319639\n",
      "STEP: 963100 - loss: 0.006649625586462195\n",
      "STEP: 963200 - loss: 0.006649210840986855\n",
      "STEP: 963300 - loss: 0.006648796214857552\n",
      "STEP: 963400 - loss: 0.00664838170803822\n",
      "STEP: 963500 - loss: 0.006647967320492142\n",
      "STEP: 963600 - loss: 0.006647553052183385\n",
      "STEP: 963700 - loss: 0.006647138903075241\n",
      "STEP: 963800 - loss: 0.006646724873131764\n",
      "STEP: 963900 - loss: 0.006646310962317112\n",
      "STEP: 964000 - loss: 0.006645897170594481\n",
      "STEP: 964100 - loss: 0.006645483497928355\n",
      "STEP: 964200 - loss: 0.0066450699442816745\n",
      "STEP: 964300 - loss: 0.006644656509618864\n",
      "STEP: 964400 - loss: 0.00664424319390366\n",
      "STEP: 964500 - loss: 0.0066438299970998605\n",
      "STEP: 964600 - loss: 0.00664341691917153\n",
      "STEP: 964700 - loss: 0.006643003960081881\n",
      "STEP: 964800 - loss: 0.0066425911197954685\n",
      "STEP: 964900 - loss: 0.00664217839827594\n",
      "STEP: 965000 - loss: 0.0066417657954873114\n",
      "STEP: 965100 - loss: 0.0066413533113934805\n",
      "STEP: 965200 - loss: 0.0066409409459581275\n",
      "STEP: 965300 - loss: 0.00664052869914529\n",
      "STEP: 965400 - loss: 0.006640116570919197\n",
      "STEP: 965500 - loss: 0.00663970456124337\n",
      "STEP: 965600 - loss: 0.006639292670082111\n",
      "STEP: 965700 - loss: 0.006638880897399257\n",
      "STEP: 965800 - loss: 0.0066384692431587485\n",
      "STEP: 965900 - loss: 0.006638057707324967\n",
      "STEP: 966000 - loss: 0.006637646289861283\n",
      "STEP: 966100 - loss: 0.006637234990732176\n",
      "STEP: 966200 - loss: 0.006636823809901557\n",
      "STEP: 966300 - loss: 0.006636412747333438\n",
      "STEP: 966400 - loss: 0.006636001802991921\n",
      "STEP: 966500 - loss: 0.0066355909768411585\n",
      "STEP: 966600 - loss: 0.006635180268844821\n",
      "STEP: 966700 - loss: 0.006634769678967561\n",
      "STEP: 966800 - loss: 0.006634359207173149\n",
      "STEP: 966900 - loss: 0.006633948853425786\n",
      "STEP: 967000 - loss: 0.006633538617689687\n",
      "STEP: 967100 - loss: 0.0066331284999287905\n",
      "STEP: 967200 - loss: 0.006632718500107408\n",
      "STEP: 967300 - loss: 0.006632308618189397\n",
      "STEP: 967400 - loss: 0.006631898854139147\n",
      "STEP: 967500 - loss: 0.006631489207920967\n",
      "STEP: 967600 - loss: 0.0066310796794987635\n",
      "STEP: 967700 - loss: 0.0066306702688369275\n",
      "STEP: 967800 - loss: 0.006630260975899693\n",
      "STEP: 967900 - loss: 0.0066298518006510775\n",
      "STEP: 968000 - loss: 0.006629442743055507\n",
      "STEP: 968100 - loss: 0.0066290338030771685\n",
      "STEP: 968200 - loss: 0.006628624980680212\n",
      "STEP: 968300 - loss: 0.006628216275829009\n",
      "STEP: 968400 - loss: 0.006627807688487498\n",
      "STEP: 968500 - loss: 0.006627399218620589\n",
      "STEP: 968600 - loss: 0.0066269908661921425\n",
      "STEP: 968700 - loss: 0.006626582631166803\n",
      "STEP: 968800 - loss: 0.00662617451350835\n",
      "STEP: 968900 - loss: 0.006625766513181463\n",
      "STEP: 969000 - loss: 0.0066253586301505685\n",
      "STEP: 969100 - loss: 0.006624950864379827\n",
      "STEP: 969200 - loss: 0.006624543215833868\n",
      "STEP: 969300 - loss: 0.006624135684476571\n",
      "STEP: 969400 - loss: 0.006623728270272733\n",
      "STEP: 969500 - loss: 0.006623320973186372\n",
      "STEP: 969600 - loss: 0.006622913793182533\n",
      "STEP: 969700 - loss: 0.006622506730224987\n",
      "STEP: 969800 - loss: 0.006622099784278313\n",
      "STEP: 969900 - loss: 0.0066216929553072025\n",
      "STEP: 970000 - loss: 0.006621286243275885\n",
      "STEP: 970100 - loss: 0.0066208796481491234\n",
      "STEP: 970200 - loss: 0.006620473169890784\n",
      "STEP: 970300 - loss: 0.00662006680846577\n",
      "STEP: 970400 - loss: 0.006619660563838498\n",
      "STEP: 970500 - loss: 0.006619254435973633\n",
      "STEP: 970600 - loss: 0.0066188484248352235\n",
      "STEP: 970700 - loss: 0.0066184425303881245\n",
      "STEP: 970800 - loss: 0.00661803675259699\n",
      "STEP: 970900 - loss: 0.006617631091426457\n",
      "STEP: 971000 - loss: 0.006617225546840419\n",
      "STEP: 971100 - loss: 0.006616820118804165\n",
      "STEP: 971200 - loss: 0.006616414807281537\n",
      "STEP: 971300 - loss: 0.006616009612238105\n",
      "STEP: 971400 - loss: 0.006615604533637581\n",
      "STEP: 971500 - loss: 0.006615199571444899\n",
      "STEP: 971600 - loss: 0.006614794725624713\n",
      "STEP: 971700 - loss: 0.00661438999614178\n",
      "STEP: 971800 - loss: 0.006613985382960585\n",
      "STEP: 971900 - loss: 0.006613580886045567\n",
      "STEP: 972000 - loss: 0.006613176505361781\n",
      "STEP: 972100 - loss: 0.006612772240873717\n",
      "STEP: 972200 - loss: 0.006612368092545837\n",
      "STEP: 972300 - loss: 0.006611964060343189\n",
      "STEP: 972400 - loss: 0.006611560144230489\n",
      "STEP: 972500 - loss: 0.006611156344171955\n",
      "STEP: 972600 - loss: 0.00661075266013335\n",
      "STEP: 972700 - loss: 0.006610349092078335\n",
      "STEP: 972800 - loss: 0.006609945639972065\n",
      "STEP: 972900 - loss: 0.006609542303779259\n",
      "STEP: 973000 - loss: 0.006609139083464885\n",
      "STEP: 973100 - loss: 0.006608735978993472\n",
      "STEP: 973200 - loss: 0.006608332990330007\n",
      "STEP: 973300 - loss: 0.006607930117439147\n",
      "STEP: 973400 - loss: 0.006607527360285674\n",
      "STEP: 973500 - loss: 0.006607124718834343\n",
      "STEP: 973600 - loss: 0.006606722193050326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 973700 - loss: 0.006606319782898201\n",
      "STEP: 973800 - loss: 0.00660591748834303\n",
      "STEP: 973900 - loss: 0.0066055153093495965\n",
      "STEP: 974000 - loss: 0.006605113245882481\n",
      "STEP: 974100 - loss: 0.006604711297907001\n",
      "STEP: 974200 - loss: 0.006604309465387838\n",
      "STEP: 974300 - loss: 0.006603907748289821\n",
      "STEP: 974400 - loss: 0.006603506146578115\n",
      "STEP: 974500 - loss: 0.00660310466021746\n",
      "STEP: 974600 - loss: 0.006602703289172894\n",
      "STEP: 974700 - loss: 0.006602302033409428\n",
      "STEP: 974800 - loss: 0.006601900892891739\n",
      "STEP: 974900 - loss: 0.006601499867585186\n",
      "STEP: 975000 - loss: 0.0066010989574542015\n",
      "STEP: 975100 - loss: 0.006600698162464618\n",
      "STEP: 975200 - loss: 0.006600297482580686\n",
      "STEP: 975300 - loss: 0.006599896917767667\n",
      "STEP: 975400 - loss: 0.0065994964679906305\n",
      "STEP: 975500 - loss: 0.00659909613321472\n",
      "STEP: 975600 - loss: 0.006598695913404963\n",
      "STEP: 975700 - loss: 0.006598295808526203\n",
      "STEP: 975800 - loss: 0.0065978958185435865\n",
      "STEP: 975900 - loss: 0.006597495943422242\n",
      "STEP: 976000 - loss: 0.0065970961831270675\n",
      "STEP: 976100 - loss: 0.006596696537623428\n",
      "STEP: 976200 - loss: 0.006596297006876623\n",
      "STEP: 976300 - loss: 0.006595897590851276\n",
      "STEP: 976400 - loss: 0.006595498289512799\n",
      "STEP: 976500 - loss: 0.006595099102826225\n",
      "STEP: 976600 - loss: 0.006594700030756892\n",
      "STEP: 976700 - loss: 0.00659430107326975\n",
      "STEP: 976800 - loss: 0.006593902230330083\n",
      "STEP: 976900 - loss: 0.006593503501902892\n",
      "STEP: 977000 - loss: 0.006593104887953763\n",
      "STEP: 977100 - loss: 0.006592706388447672\n",
      "STEP: 977200 - loss: 0.006592308003349952\n",
      "STEP: 977300 - loss: 0.006591909732625528\n",
      "STEP: 977400 - loss: 0.006591511576239831\n",
      "STEP: 977500 - loss: 0.00659111353415818\n",
      "STEP: 977600 - loss: 0.0065907156063455844\n",
      "STEP: 977700 - loss: 0.0065903177927675305\n",
      "STEP: 977800 - loss: 0.006589920093389591\n",
      "STEP: 977900 - loss: 0.006589522508176439\n",
      "STEP: 978000 - loss: 0.006589125037094\n",
      "STEP: 978100 - loss: 0.006588727680106955\n",
      "STEP: 978200 - loss: 0.006588330437181121\n",
      "STEP: 978300 - loss: 0.006587933308281467\n",
      "STEP: 978400 - loss: 0.006587536293373557\n",
      "STEP: 978500 - loss: 0.006587139392422445\n",
      "STEP: 978600 - loss: 0.006586742605394274\n",
      "STEP: 978700 - loss: 0.00658634593225354\n",
      "STEP: 978800 - loss: 0.006585949372966015\n",
      "STEP: 978900 - loss: 0.0065855529274973425\n",
      "STEP: 979000 - loss: 0.006585156595812357\n",
      "STEP: 979100 - loss: 0.006584760377876612\n",
      "STEP: 979200 - loss: 0.006584364273655975\n",
      "STEP: 979300 - loss: 0.006583968283115829\n",
      "STEP: 979400 - loss: 0.006583572406221055\n",
      "STEP: 979500 - loss: 0.006583176642937662\n",
      "STEP: 979600 - loss: 0.006582780993230715\n",
      "STEP: 979700 - loss: 0.006582385457066178\n",
      "STEP: 979800 - loss: 0.006581990034408802\n",
      "STEP: 979900 - loss: 0.006581594725224997\n",
      "STEP: 980000 - loss: 0.0065811995294798795\n",
      "STEP: 980100 - loss: 0.006580804447138627\n",
      "STEP: 980200 - loss: 0.006580409478167136\n",
      "STEP: 980300 - loss: 0.006580014622531075\n",
      "STEP: 980400 - loss: 0.006579619880195645\n",
      "STEP: 980500 - loss: 0.006579225251126498\n",
      "STEP: 980600 - loss: 0.006578830735289397\n",
      "STEP: 980700 - loss: 0.006578436332649843\n",
      "STEP: 980800 - loss: 0.006578042043173531\n",
      "STEP: 980900 - loss: 0.006577647866825765\n",
      "STEP: 981000 - loss: 0.006577253803572015\n",
      "STEP: 981100 - loss: 0.006576859853378888\n",
      "STEP: 981200 - loss: 0.00657646601621088\n",
      "STEP: 981300 - loss: 0.006576072292034496\n",
      "STEP: 981400 - loss: 0.006575678680814776\n",
      "STEP: 981500 - loss: 0.006575285182517739\n",
      "STEP: 981600 - loss: 0.006574891797108871\n",
      "STEP: 981700 - loss: 0.006574498524554244\n",
      "STEP: 981800 - loss: 0.006574105364818858\n",
      "STEP: 981900 - loss: 0.006573712317869127\n",
      "STEP: 982000 - loss: 0.006573319383670481\n",
      "STEP: 982100 - loss: 0.006572926562188731\n",
      "STEP: 982200 - loss: 0.0065725338533891954\n",
      "STEP: 982300 - loss: 0.006572141257238149\n",
      "STEP: 982400 - loss: 0.006571748773701189\n",
      "STEP: 982500 - loss: 0.006571356402744266\n",
      "STEP: 982600 - loss: 0.006570964144332942\n",
      "STEP: 982700 - loss: 0.00657057199843304\n",
      "STEP: 982800 - loss: 0.006570179965009941\n",
      "STEP: 982900 - loss: 0.00656978804403043\n",
      "STEP: 983000 - loss: 0.006569396235459808\n",
      "STEP: 983100 - loss: 0.006569004539263476\n",
      "STEP: 983200 - loss: 0.006568612955407986\n",
      "STEP: 983300 - loss: 0.006568221483859063\n",
      "STEP: 983400 - loss: 0.00656783012458239\n",
      "STEP: 983500 - loss: 0.006567438877543426\n",
      "STEP: 983600 - loss: 0.0065670477427090925\n",
      "STEP: 983700 - loss: 0.006566656720044422\n",
      "STEP: 983800 - loss: 0.0065662658095156975\n",
      "STEP: 983900 - loss: 0.006565875011088906\n",
      "STEP: 984000 - loss: 0.006565484324729668\n",
      "STEP: 984100 - loss: 0.006565093750404013\n",
      "STEP: 984200 - loss: 0.006564703288077798\n",
      "STEP: 984300 - loss: 0.006564312937717495\n",
      "STEP: 984400 - loss: 0.0065639226992885895\n",
      "STEP: 984500 - loss: 0.0065635325727574885\n",
      "STEP: 984600 - loss: 0.006563142558089886\n",
      "STEP: 984700 - loss: 0.006562752655251286\n",
      "STEP: 984800 - loss: 0.006562362864208542\n",
      "STEP: 984900 - loss: 0.00656197318492747\n",
      "STEP: 985000 - loss: 0.006561583617373664\n",
      "STEP: 985100 - loss: 0.006561194161513869\n",
      "STEP: 985200 - loss: 0.006560804817313465\n",
      "STEP: 985300 - loss: 0.006560415584738817\n",
      "STEP: 985400 - loss: 0.0065600264637561065\n",
      "STEP: 985500 - loss: 0.006559637454331189\n",
      "STEP: 985600 - loss: 0.006559248556430404\n",
      "STEP: 985700 - loss: 0.00655885977001962\n",
      "STEP: 985800 - loss: 0.006558471095065022\n",
      "STEP: 985900 - loss: 0.006558082531532765\n",
      "STEP: 986000 - loss: 0.006557694079389127\n",
      "STEP: 986100 - loss: 0.00655730573860012\n",
      "STEP: 986200 - loss: 0.0065569175091319725\n",
      "STEP: 986300 - loss: 0.006556529390950604\n",
      "STEP: 986400 - loss: 0.006556141384022415\n",
      "STEP: 986500 - loss: 0.006555753488313634\n",
      "STEP: 986600 - loss: 0.006555365703790358\n",
      "STEP: 986700 - loss: 0.006554978030418759\n",
      "STEP: 986800 - loss: 0.006554590468165154\n",
      "STEP: 986900 - loss: 0.006554203016995515\n",
      "STEP: 987000 - loss: 0.006553815676876437\n",
      "STEP: 987100 - loss: 0.006553428447774276\n",
      "STEP: 987200 - loss: 0.006553041329654848\n",
      "STEP: 987300 - loss: 0.006552654322484331\n",
      "STEP: 987400 - loss: 0.006552267426229848\n",
      "STEP: 987500 - loss: 0.006551880640856626\n",
      "STEP: 987600 - loss: 0.006551493966331821\n",
      "STEP: 987700 - loss: 0.0065511074026213655\n",
      "STEP: 987800 - loss: 0.006550720949691486\n",
      "STEP: 987900 - loss: 0.006550334607508522\n",
      "STEP: 988000 - loss: 0.006549948376039193\n",
      "STEP: 988100 - loss: 0.006549562255249379\n",
      "STEP: 988200 - loss: 0.00654917624510575\n",
      "STEP: 988300 - loss: 0.006548790345574662\n",
      "STEP: 988400 - loss: 0.006548404556622395\n",
      "STEP: 988500 - loss: 0.006548018878215507\n",
      "STEP: 988600 - loss: 0.006547633310320024\n",
      "STEP: 988700 - loss: 0.0065472478529029\n",
      "STEP: 988800 - loss: 0.006546862505929681\n",
      "STEP: 988900 - loss: 0.006546477269368028\n",
      "STEP: 989000 - loss: 0.0065460921431833114\n",
      "STEP: 989100 - loss: 0.006545707127342497\n",
      "STEP: 989200 - loss: 0.006545322221811987\n",
      "STEP: 989300 - loss: 0.006544937426558232\n",
      "STEP: 989400 - loss: 0.0065445527415476145\n",
      "STEP: 989500 - loss: 0.006544168166747024\n",
      "STEP: 989600 - loss: 0.006543783702122373\n",
      "STEP: 989700 - loss: 0.0065433993476408745\n",
      "STEP: 989800 - loss: 0.006543015103268203\n",
      "STEP: 989900 - loss: 0.006542630968971561\n",
      "STEP: 990000 - loss: 0.006542246944717138\n",
      "STEP: 990100 - loss: 0.0065418630304718605\n",
      "STEP: 990200 - loss: 0.006541479226201785\n",
      "STEP: 990300 - loss: 0.006541095531874213\n",
      "STEP: 990400 - loss: 0.006540711947455003\n",
      "STEP: 990500 - loss: 0.0065403284729109844\n",
      "STEP: 990600 - loss: 0.006539945108209076\n",
      "STEP: 990700 - loss: 0.00653956185331542\n",
      "STEP: 990800 - loss: 0.006539178708197031\n",
      "STEP: 990900 - loss: 0.00653879567282011\n",
      "STEP: 991000 - loss: 0.006538412747151794\n",
      "STEP: 991100 - loss: 0.006538029931158514\n",
      "STEP: 991200 - loss: 0.0065376472248069055\n",
      "STEP: 991300 - loss: 0.006537264628063669\n",
      "STEP: 991400 - loss: 0.006536882140895556\n",
      "STEP: 991500 - loss: 0.006536499763269222\n",
      "STEP: 991600 - loss: 0.006536117495151268\n",
      "STEP: 991700 - loss: 0.006535735336508586\n",
      "STEP: 991800 - loss: 0.006535353287307961\n",
      "STEP: 991900 - loss: 0.006534971347515935\n",
      "STEP: 992000 - loss: 0.006534589517099029\n",
      "STEP: 992100 - loss: 0.006534207796024793\n",
      "STEP: 992200 - loss: 0.00653382618425923\n",
      "STEP: 992300 - loss: 0.0065334446817693305\n",
      "STEP: 992400 - loss: 0.006533063288522256\n",
      "STEP: 992500 - loss: 0.006532682004484203\n",
      "STEP: 992600 - loss: 0.006532300829622433\n",
      "STEP: 992700 - loss: 0.006531919763903653\n",
      "STEP: 992800 - loss: 0.006531538807294431\n",
      "STEP: 992900 - loss: 0.006531157959761978\n",
      "STEP: 993000 - loss: 0.0065307772212731974\n",
      "STEP: 993100 - loss: 0.006530396591794478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 993200 - loss: 0.006530016071293054\n",
      "STEP: 993300 - loss: 0.006529635659735586\n",
      "STEP: 993400 - loss: 0.006529255357089117\n",
      "STEP: 993500 - loss: 0.006528875163320699\n",
      "STEP: 993600 - loss: 0.0065284950783971035\n",
      "STEP: 993700 - loss: 0.006528115102285135\n",
      "STEP: 993800 - loss: 0.0065277352349515354\n",
      "STEP: 993900 - loss: 0.0065273554763638445\n",
      "STEP: 994000 - loss: 0.006526975826488446\n",
      "STEP: 994100 - loss: 0.006526596285292771\n",
      "STEP: 994200 - loss: 0.006526216852743442\n",
      "STEP: 994300 - loss: 0.006525837528807614\n",
      "STEP: 994400 - loss: 0.006525458313452108\n",
      "STEP: 994500 - loss: 0.0065250792066440435\n",
      "STEP: 994600 - loss: 0.006524700208350405\n",
      "STEP: 994700 - loss: 0.006524321318538187\n",
      "STEP: 994800 - loss: 0.006523942537174808\n",
      "STEP: 994900 - loss: 0.00652356386422679\n",
      "STEP: 995000 - loss: 0.0065231852996612385\n",
      "STEP: 995100 - loss: 0.006522806843445454\n",
      "STEP: 995200 - loss: 0.006522428495546154\n",
      "STEP: 995300 - loss: 0.006522050255931079\n",
      "STEP: 995400 - loss: 0.006521672124566694\n",
      "STEP: 995500 - loss: 0.00652129410141995\n",
      "STEP: 995600 - loss: 0.006520916186458626\n",
      "STEP: 995700 - loss: 0.006520538379649619\n",
      "STEP: 995800 - loss: 0.006520160680959929\n",
      "STEP: 995900 - loss: 0.006519783090356654\n",
      "STEP: 996000 - loss: 0.006519405607807343\n",
      "STEP: 996100 - loss: 0.006519028233278541\n",
      "STEP: 996200 - loss: 0.006518650966737552\n",
      "STEP: 996300 - loss: 0.0065182738081522255\n",
      "STEP: 996400 - loss: 0.006517896757488759\n",
      "STEP: 996500 - loss: 0.0065175198147152695\n",
      "STEP: 996600 - loss: 0.006517142979798432\n",
      "STEP: 996700 - loss: 0.006516766252705338\n",
      "STEP: 996800 - loss: 0.00651638963340367\n",
      "STEP: 996900 - loss: 0.006516013121860736\n",
      "STEP: 997000 - loss: 0.00651563671804301\n",
      "STEP: 997100 - loss: 0.006515260421918734\n",
      "STEP: 997200 - loss: 0.006514884233454278\n",
      "STEP: 997300 - loss: 0.006514508152617468\n",
      "STEP: 997400 - loss: 0.006514132179375484\n",
      "STEP: 997500 - loss: 0.0065137563136956735\n",
      "STEP: 997600 - loss: 0.006513380555545574\n",
      "STEP: 997700 - loss: 0.006513004904892016\n",
      "STEP: 997800 - loss: 0.00651262936170262\n",
      "STEP: 997900 - loss: 0.0065122539259445\n",
      "STEP: 998000 - loss: 0.006511878597585044\n",
      "STEP: 998100 - loss: 0.006511503376591744\n",
      "STEP: 998200 - loss: 0.00651112826293226\n",
      "STEP: 998300 - loss: 0.0065107532565735786\n",
      "STEP: 998400 - loss: 0.006510378357483029\n",
      "STEP: 998500 - loss: 0.00651000356562813\n",
      "STEP: 998600 - loss: 0.006509628880976523\n",
      "STEP: 998700 - loss: 0.006509254303495344\n",
      "STEP: 998800 - loss: 0.0065088798331521345\n",
      "STEP: 998900 - loss: 0.006508505469914334\n",
      "STEP: 999000 - loss: 0.006508131213749204\n",
      "STEP: 999100 - loss: 0.0065077570646245985\n",
      "STEP: 999200 - loss: 0.006507383022507605\n",
      "STEP: 999300 - loss: 0.006507009087366021\n",
      "STEP: 999400 - loss: 0.006506635259166733\n",
      "STEP: 999500 - loss: 0.006506261537878202\n",
      "STEP: 999600 - loss: 0.006505887923467063\n",
      "STEP: 999700 - loss: 0.00650551441590137\n",
      "STEP: 999800 - loss: 0.006505141015148252\n",
      "STEP: 999900 - loss: 0.006504767721175586\n",
      "w0: -0.007287194855839281, w1: -0.8753778901852678, w2: -9.127621185871632, w3: 29.300282184534606\n"
     ]
    }
   ],
   "source": [
    "w=gradient_descent(matr,y,w,cost,gradient,steps=1000000,alpha=0.1)\n",
    "predicted=predict(matr,w)\n",
    "print(f'w0: {w[0]}, w1: {w[1]}, w2: {w[2]}, w3: {w[3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "057c5fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22fb6e935b0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,y,label='true values')\n",
    "plt.plot(x,predicted,color='red',label='prediction')\n",
    "plt.title('feature engineering')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9dd321",
   "metadata": {},
   "source": [
    "# Scikit-Learn linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec1d69a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec312cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=sklearn.datasets.fetch_california_housing()\n",
    "x=data['data'][:,:4]\n",
    "y=data['target']\n",
    "x_features=data['feature_names'][:4]\n",
    "x_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8152f8de",
   "metadata": {},
   "source": [
    "#### scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cc741db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: [ 14.5002      51.         141.06293706  33.73333333]\n",
      "Normalized: [ 7.63258528  4.05236201 57.01555488 71.18248097]\n"
     ]
    }
   ],
   "source": [
    "scaler=StandardScaler()\n",
    "x_norm=scaler.fit_transform(x)\n",
    "print(f'Raw: {np.ptp(x,axis=0)}')\n",
    "print(f'Normalized: {np.ptp(x_norm,axis=0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c65353",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d097cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor()\n"
     ]
    }
   ],
   "source": [
    "sgdr=SGDRegressor(max_iter=1000)\n",
    "sgdr.fit(x_norm,y)\n",
    "print(sgdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d2334",
   "metadata": {},
   "source": [
    "#### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f35165b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [ 0.99248164  0.19804849 -0.5019538   0.46101305], w0: [2.06991803]\n"
     ]
    }
   ],
   "source": [
    "w0_norm=sgdr.intercept_\n",
    "w_norm=sgdr.coef_\n",
    "print(f'w: {w_norm}, w0: {w0_norm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eed36d5",
   "metadata": {},
   "source": [
    "#### prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f068abca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction np.dot() vs sgdr.predict(): True\n",
      "Prediction: [4.20517481 3.97884133 3.60416315 3.26172176]\n",
      "Traget: [4.526 3.585 3.521 3.413]\n"
     ]
    }
   ],
   "source": [
    "y_pred_sgdr=sgdr.predict(x_norm)\n",
    "y_pred=np.dot(x_norm,w_norm)+w0_norm\n",
    "print(f'prediction np.dot() vs sgdr.predict(): {(y_pred==y_pred_sgdr).all()}')\n",
    "print(f'Prediction: {y_pred[:4]}')\n",
    "print(f'Traget: {y[:4]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2747c20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x216 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax=plt.subplots(1,4,figsize=(12,3))\n",
    "for i in range(4):\n",
    "    ax[i].scatter(x[:,i],y,label='target')\n",
    "    ax[i].scatter(x[:,i],y_pred,label='predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8787775e",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63c8253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2efc46",
   "metadata": {},
   "source": [
    "#### cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3bda1c",
   "metadata": {},
   "source": [
    "linear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b306bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(x,y,w,w0):\n",
    "    rows=x.shape[0]\n",
    "    cost=0\n",
    "    for row in range(rows):\n",
    "        cost+=(x[row]*w+b-y[row])**2\n",
    "    cost=cost/(2*rows)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f906b",
   "metadata": {},
   "source": [
    "complex:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f16f31b",
   "metadata": {},
   "source": [
    "x[0] must be 1 to represent w0 as indifferent from x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80c0f47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(x,y,w):\n",
    "    rows=x.shape[0]\n",
    "    cost=0\n",
    "    for row in range(rows):\n",
    "        cost+=(np.dot(x[row],w)-y[row])**2\n",
    "    cost=cost/(2*rows)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1465c4b",
   "metadata": {},
   "source": [
    "#### gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f066e",
   "metadata": {},
   "source": [
    "linear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49444781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x,y,w,w0):\n",
    "    rows,features=x.shape\n",
    "    dw=0\n",
    "    dw0=0\n",
    "    for row in range(rows):\n",
    "        dw+=(x[row]*w+b-y[row])*x[row]\n",
    "        dw0+=(x[row]*w+b-y[row])\n",
    "    dw=dw/rows\n",
    "    dw0=dw0/rows\n",
    "    return dw,dw0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf3939",
   "metadata": {},
   "source": [
    "complex:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bfbffd",
   "metadata": {},
   "source": [
    "x[0] must be 1 to represent w0 as indifferent from x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "99124a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x,y,w):\n",
    "    rows,features=x.shape\n",
    "    dw=np.zeros(features)\n",
    "    dw[0]=1\n",
    "    for feature in range(features):\n",
    "        for row in range(rows):\n",
    "            dw[feature]+=(np.dot(x[row],w)-y[row])*x[row][feature]\n",
    "    dw=dw/rows\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5408cd4f",
   "metadata": {},
   "source": [
    "#### gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9bca200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x,y,w,cost,gradient,steps=1000,alpha=0.001):\n",
    "    for step in range(steps):\n",
    "        dw=gradient(x,y,w)\n",
    "        w-=alpha*dw\n",
    "        loss=cost(x,y,w)\n",
    "        if step%100==0:\n",
    "            print(f'STEP: {step} - loss: {loss}')\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93114ec4",
   "metadata": {},
   "source": [
    "#### example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "60bf13d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([[1,1,2],[1,2,3]])\n",
    "y=np.array([2,4])\n",
    "w=np.zeros(x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "521f2eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 0 - loss: 4.910410000000001\n",
      "STEP: 100 - loss: 0.9199163383995437\n",
      "STEP: 200 - loss: 0.2675496467793942\n",
      "STEP: 300 - loss: 0.13739696339991517\n",
      "STEP: 400 - loss: 0.10342596533235511\n",
      "STEP: 500 - loss: 0.09183565300310557\n",
      "STEP: 600 - loss: 0.0867420851258488\n",
      "STEP: 700 - loss: 0.08382849949731347\n",
      "STEP: 800 - loss: 0.0817120558811526\n",
      "STEP: 900 - loss: 0.07991078787516594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.03238344,  0.41859369,  0.88621025])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=gradient_descent(x,y,w,cost,gradient)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b89a1fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1586307365243806 2\n",
      "3.4634346697759786 4\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y)):\n",
    "    print(np.dot(x[i],w), y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871b3d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
