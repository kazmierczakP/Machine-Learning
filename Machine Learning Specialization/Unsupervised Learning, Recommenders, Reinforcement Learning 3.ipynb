{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fbb9f3f",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8306cb9a",
   "metadata": {},
   "source": [
    "Goal of Reinforcement learning - choose a `policy` that will tell what `action` to take in `state s` so that to max the expected `return`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b16e7c7",
   "metadata": {},
   "source": [
    "`Policy` - $ \\pi $ takes as input state $ s $ and return asction $ \\alpha $ as output based on the reward value it gets (it can be described ad all astions that agent takes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec81a635",
   "metadata": {},
   "source": [
    "$$ \\pi(s) = a $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b9896",
   "metadata": {},
   "source": [
    "`State` -  $ s $ current state (position) of the agent \n",
    "\n",
    "\n",
    "`Reward` - $ r $ the reward that agent gets for being in a given state "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03437af",
   "metadata": {},
   "source": [
    "\\begin{bmatrix} \n",
    "State & 1 & 2 & 4 & 4 \\\\\n",
    "Reward & 0 & 0 & 0 & 100 \\\\\n",
    "\\end{bmatrix} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8328941",
   "metadata": {},
   "source": [
    "`Action` - $ a $ a move (to the right in this example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b449c462",
   "metadata": {},
   "source": [
    "$$ -> $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b538d60",
   "metadata": {},
   "source": [
    "`Return` - $ R $ the final reward thet model gets after taking actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2fc72c",
   "metadata": {},
   "source": [
    "$$ R_1 + R_2 + R_3 + R_4 = R $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b9149",
   "metadata": {},
   "source": [
    "$$ 0 + 0 + 0 + 100 = 100 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bfb8e3",
   "metadata": {},
   "source": [
    "`Discount factor`: - $ \\gamma $ a factor by which we multipy reward (the reward diminished by the amount of moves)\n",
    "\n",
    "$$ \\gamma =0.9 $$\n",
    "\n",
    "$$ 0 + 0 \\cdot 0.9 + 0 \\cdot (0.9)^{2} + 100 \\cdot (0.9)^{3} = 72.9 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b04a6a",
   "metadata": {},
   "source": [
    "`MDP` - Markov Decision Process - the future only depends on the current state (past is not important)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045b068f",
   "metadata": {},
   "source": [
    "#### State action value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc1043",
   "metadata": {},
   "source": [
    "$$ Q(s,\\alpha) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586a713",
   "metadata": {},
   "source": [
    "The aim is to maximize $ Q $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab84a9c",
   "metadata": {},
   "source": [
    "$$ max Q(s,\\alpha) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681de27a",
   "metadata": {},
   "source": [
    "Having computed $ Q(s,a) $ for every state and action, then we can compute optimal policy $ \\pi(s) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8394316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "500d48fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "states=6\n",
    "actions=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e5557adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_left_rewads=100\n",
    "terminal_right_rewards=40\n",
    "each_step_reward=0\n",
    "gamma=0.5\n",
    "\n",
    "misstep_prob=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f2f99231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rewards(num_states,step_reward,terminal_left_reward,terminal_right_reward):\n",
    "    rewards=[step_reward]*num_states\n",
    "    rewards[0]=terminal_left_reward\n",
    "    rewards[-1]=terminal_right_reward\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9cda6d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_prob(num_states,num_actions,misstep_prob=0):\n",
    "    # p[][0][] describes left moves probability\n",
    "    # p[][1][] describes right moves probability\n",
    "    p=np.zeros((num_states,num_actions,num_states))\n",
    "    for i in range(num_states):\n",
    "        if i !=0:\n",
    "            p[i,0,i-1]=1-misstep_prob\n",
    "            p[i,1,i-1]=misstep_prob\n",
    "        if i != num_states-1:\n",
    "            p[i,1,i+1]=1-misstep_prob\n",
    "            p[i,0,i+1]=misstep_prob            \n",
    "    p[0]=np.zeros((num_actions,num_states))\n",
    "    p[-1]=np.zeros((num_actions,num_states))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f10d8aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_value(num_states,rewards,transition_prob,gamma,V_states,state,action):\n",
    "    q_sa=rewards[state]+gamma*sum([transition_prob[state,action,sp]*V_states[sp] for sp in range(num_states)])\n",
    "    return q_sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a2a1fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(num_states,rewards,transition_prob,gamma,policy):\n",
    "    max_policy_eval=10000\n",
    "    treshold=1e-10\n",
    "    V=np.zeros(num_states)\n",
    "    for i in range(max_policy_eval):\n",
    "        delta=0\n",
    "        for s in range(num_states):\n",
    "            v=V[s]\n",
    "            V[s]=q_value(num_states,rewards,transition_prob,gamma,V,s,policy[s])\n",
    "            delta=max(delta,abs(v-V[s]))\n",
    "        if delta<treshold:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d0af53ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_values(num_states,rewards,transition_prob,gamma,optimal_policy):\n",
    "    q_left=np.zeros(num_states)\n",
    "    q_right=np.zeros(num_states)\n",
    "    V_star=evaluate_policy(num_states,rewards,transition_prob,gamma,optimal_policy)\n",
    "    for s in range(num_states):\n",
    "        q_left[s]=q_value(num_states,rewards,transition_prob,gamma,V_star,s,0)\n",
    "        q_right[s]=q_value(num_states,rewards,transition_prob,gamma,V_star,s,1)\n",
    "    return q_left,q_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "082a5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(num_states,num_actions,rewards,transition_prob,gamma,V,policy):\n",
    "    policy_stable=True\n",
    "    for s in range(num_states):\n",
    "        q_best=V[s]\n",
    "        for a in range(num_actions):\n",
    "            q_sa=q_value(num_states,rewards,transition_prob,gamma,V,s,a)\n",
    "            if q_sa>q_best and policy[s]!=a:\n",
    "                policy[s]=a\n",
    "                q_best=q_sa\n",
    "                policy_stable=False\n",
    "    return policy,policy_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "eb301d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_policy(num_states,num_actions,rewards,transition_prob,gamma):\n",
    "    optimal_policy=np.zeros(num_states,dtype=int)\n",
    "    max_policy_iter=10000\n",
    "    for i in range(max_policy_iter):\n",
    "        policy_stable=True\n",
    "        V=evaluate_policy(num_states,rewards,transition_prob,gamma,optimal_policy)\n",
    "        optimal_policy,policy_stable=improve_policy(num_states,num_actions,rewards,transition_prob,gamma,V,optimal_policy)\n",
    "        if policy_stable:\n",
    "            break\n",
    "    return optimal_policy,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "934198c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 0, 0, 0, 0, 40]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards=get_rewards(states,each_step_reward,terminal_left_rewads,terminal_right_rewards)\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "1b0e8dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_prob=get_transition_prob(states,actions,misstep_prob)\n",
    "transition_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab9542",
   "metadata": {},
   "source": [
    "#### probabilities of moves to left side (1 mean move to this state from right state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22439fab",
   "metadata": {},
   "source": [
    "When agent is max left then he will stay so all probabiliriea are 0. (same for max right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2ffbbc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_prob[:,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8fa038",
   "metadata": {},
   "source": [
    "#### probabilities of moves to right side (1 mean move to this state from left state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "538a76ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_prob[:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "0ab43f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy,V=get_optimal_policy(states,actions,rewards,transition_prob,gamma)\n",
    "optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ef0989ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100. ,  50. ,  25. ,  12.5,  20. ,  40. ])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ce103b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_left_star,q_right_star=q_values(states,rewards,transition_prob,gamma,optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ff54682c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100.  ,  50.  ,  25.  ,  12.5 ,   6.25,  40.  ])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_left_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2c970c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100.  ,  12.5 ,   6.25,  10.  ,  20.  ,  40.  ])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_right_star"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08768d6a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398bcc18",
   "metadata": {},
   "source": [
    "## Bellman equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa647695",
   "metadata": {},
   "source": [
    "$$ Q(s,a) = R(s) + \\gamma \\cdot max Q(s^{'},a^{'}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8a7260",
   "metadata": {},
   "source": [
    "$$ Q(1,->) = R_1 + \\gamma \\cdot R_2 + \\gamma{2} \\cdot R_3 + \\gamma{3} \\cdot R_4 = R_1 + \\gamma \\cdot (R_2 + \\gamma \\cdot R_3 + \\gamma{2} \\cdot R_4) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9429f22e",
   "metadata": {},
   "source": [
    "`Q` is the `return` if you start from `state s` and take `action a (once)` and then behave `optimally after that`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "1ad8bb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 0, 0, 0, 0, 40]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards=get_rewards(states,each_step_reward,terminal_left_rewads,terminal_right_rewards)\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "fb50a6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0.9, 0. , 0.1, 0. , 0. , 0. ],\n",
       "        [0.1, 0. , 0.9, 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0.9, 0. , 0.1, 0. , 0. ],\n",
       "        [0. , 0.1, 0. , 0.9, 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0.9, 0. , 0.1, 0. ],\n",
       "        [0. , 0. , 0.1, 0. , 0.9, 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0.9, 0. , 0.1],\n",
       "        [0. , 0. , 0. , 0.1, 0. , 0.9]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. , 0. , 0. ]]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_prob=get_transition_prob(states,actions,misstep_prob=0.1)\n",
    "transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "23980cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy,V=get_optimal_policy(states,actions,rewards,transition_prob,gamma)\n",
    "optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "37075648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100.        ,  46.0626341 ,  21.25268193,  10.4899317 ,\n",
       "        18.52449658,  40.        ])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "9a44a094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left: [100.          46.0626341   21.25268193  10.4899317    6.72046926\n",
      "  40.        ]\n",
      "fight: [100.          14.56370687   7.02360097   9.39865756  18.52449658\n",
      "  40.        ]\n"
     ]
    }
   ],
   "source": [
    "q_left_star,q_right_star=q_values(states,rewards,transition_prob,gamma,optimal_policy)\n",
    "print(f'left: {q_left_star}')\n",
    "print(f'fight: {q_right_star}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc99259",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3426f9e",
   "metadata": {},
   "source": [
    "## Continuous State applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66dbcd5",
   "metadata": {},
   "source": [
    "`DQN` - deep neural network used to calculate the Q function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab43e60c",
   "metadata": {},
   "source": [
    "We use `DQN` to calculate the `Q function` by applying random `actions` in given `states` and then receive the `reward R` and compute the `Q`. Then having enough examples we train DQN to compute Q function and do it `iteratively` keeping 10000 examples in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe537af",
   "metadata": {},
   "source": [
    "$$ x=\n",
    "\\begin{bmatrix} \n",
    "s \\\\\n",
    "a \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22877fa",
   "metadata": {},
   "source": [
    "$$ y = R(s) + \\gamma \\cdot max Q(s^{'},a^{'})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fa03a9",
   "metadata": {},
   "source": [
    "The input to DQN is state s and action a. The output is Q."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7065e69",
   "metadata": {},
   "source": [
    "Or we can make the input s and have as many outputs as actions we can make (Q given action 1, etc...)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a68ebd7",
   "metadata": {},
   "source": [
    "#### $ \\epsilon $- greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf2942d",
   "metadata": {},
   "source": [
    "Take such actions that maximize current $ Q(s,a) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2358f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
