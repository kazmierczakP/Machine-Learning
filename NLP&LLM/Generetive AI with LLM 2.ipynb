{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0d0bea",
   "metadata": {},
   "source": [
    "`Catastrophic forgetting` - fine-tuning can significantly increase the perfomance of the model on specific task, but decrease on other tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362a1f5c",
   "metadata": {},
   "source": [
    "`PEFT` - Parameter Efficent Fine-Tuning set of techniques that preserves the weights of the original LLM and trains only a small number of task-specific adapter layers. Shows greater robustnesss to `Catastrphic forgetting`\n",
    "`PEFT`:\n",
    "- Selective (change only chosen layers)\n",
    "- Reparameterization `LoRA` (Low Rank Adaptation)\n",
    "- Additive (add new trainable layer)\n",
    "\n",
    "`LoRA` decomposes weights into two smaller rank matrices and trains those instead of the full model weights. \n",
    "\n",
    "Best rank for `LoRA` to choose from is between 4 and 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74fc8d8",
   "metadata": {},
   "source": [
    "PETF trains only 10-15% of top layers parameters or adds new trainable layers on top of the existing ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e8101",
   "metadata": {},
   "source": [
    "In order to avoid `Catastrophic forgetting` you can also use multi-task fine-tuning instead of single-task fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7597478",
   "metadata": {},
   "source": [
    "`Soft prompt` - a set of trainable tokens that are added to prompt and ehose values are updated during additional training to improve performance on specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b34d2",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e89a77",
   "metadata": {},
   "source": [
    "`ROUGE` - used for text summarization, compares summary to ona or more reference summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81845ba6",
   "metadata": {},
   "source": [
    "`Recall` = unigram matches / unigrams in reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4a35eb",
   "metadata": {},
   "source": [
    "`Precision` = unigram matches / unigrams in output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b900e0",
   "metadata": {},
   "source": [
    "`ROUGE-2` calcuates Precision and Recall for bigrams instead of unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a1f852",
   "metadata": {},
   "source": [
    "`ROUGE-L` calcuates Precision and Recall for longest common subsequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6163b101",
   "metadata": {},
   "source": [
    "`BLEU` - used for text translation, compares to human-generated translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb94c94",
   "metadata": {},
   "source": [
    "`BLEU` -  average precision across different n-gram sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ac54c2",
   "metadata": {},
   "source": [
    "#### Evaluation benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bdd0b7",
   "metadata": {},
   "source": [
    "`GLUE` - general language ungerstanding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf276b",
   "metadata": {},
   "source": [
    "`Instruction fine-tuning` - involves using many prompt-complexion examples as a labeled training dataset to continue training the model by updating its weights\n",
    "\n",
    "`In-context learning` - involves providing prompt-complexion examples during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c9e5e",
   "metadata": {},
   "source": [
    "### FINE-TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d0f309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
