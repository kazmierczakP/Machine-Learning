{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b005b80",
   "metadata": {},
   "source": [
    "`LLM` - Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c4e576",
   "metadata": {},
   "source": [
    "`RNN` - primary approach to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a505562a",
   "metadata": {},
   "source": [
    "`Transformers` - new approach based on `encoding` and `decoding` the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9bdc14",
   "metadata": {},
   "source": [
    "Each word in text is assigned `TOKEN` (tokenized). Each `TOKEN` is mapped into `vector`.\n",
    "Model analises relations between tokens. \n",
    "The output of model i a vector with probabilites of each word in the vocabulary.\n",
    "\n",
    "String -> TOKEN -> vector\n",
    "\n",
    "'_teacher' -> 3145 -> [-0.0335, 0.0167, 0.0484, ...]\n",
    "\n",
    "`Encoder` - encodes input sequences into deep representation of structure and meaning of input\n",
    "\n",
    "`Decoder` - uses encoder's contextual understanding to generate new tokens (does it in a loop till a `stop condition` is reached). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ecf443",
   "metadata": {},
   "source": [
    "`Prompt` - text that we feed to the model. Improving the promt might be necassary to get better results. This is called Prompt engineering. Prompt can contain `example prompt-complection pair` to help the model generate response for another similar prompt (one-shot). For smaller models a couple of examples might be more helpful (few-shot).\n",
    "\n",
    "`Inference` - act of generating text\n",
    "\n",
    "`Comlection` - output text\n",
    "\n",
    "`Context window` - limit of words that the model can take as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52a88a8",
   "metadata": {},
   "source": [
    "`Inference parameters` - controls that adjust the models behave:\n",
    "- max new tokens - `limits number of tokens` that model will generate\n",
    "- top-k - tells model to choose top k words with `highest probability` (lets the model have some randomness in responses nad avoid repetition)\n",
    "- top-p - limits the random sampling to predictions which `combined probability` does not exceed the p\n",
    "- temperature - describes shape of the probability model that the LLM will use to generate random responses. `The higher the temperature, the higher the randomness.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73ac25d",
   "metadata": {},
   "source": [
    "#### Adapt and align model:\n",
    "- `Prompt engineering` (zero-shot, one-shot, few-shot)\n",
    "- `Fine tuning`\n",
    "- `Align with human feedback` - RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175130ce",
   "metadata": {},
   "source": [
    "#### Models:\n",
    "- `encoder only` (autoencoder models) sentiment analysis, word classification, eg. BERT\n",
    "- `encoder-decoder` (sequence to sequence models) translation, text summarization, question answering, eg. T5, BART\n",
    "- `decoder only` (autoregressive models) text generation, eg. GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f52c13",
   "metadata": {},
   "source": [
    "#### Quantization\n",
    "`Reduction of memory` required to `store and train` the model by `reducing the precision` of the model `weights`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b008d514",
   "metadata": {},
   "source": [
    "#### DDP\n",
    "`Distributed Data Parallel` - in PyTorch allows tou to copy model into `multiple GPUs` and process there sub-batches of data to train big model. After precessing each batch update of weights models are synchronized and updated on each machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed795f8",
   "metadata": {},
   "source": [
    "#### Chincillla law\n",
    "Increasing number of parameters in not the only way of improving model. A good way is to `increase the training dataset`eg. LLaMa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc4993",
   "metadata": {},
   "source": [
    "### PROMPT ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b5931d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pk764\\miniconda3\\envs\\data\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb9f57b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b98e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices = [41, 205]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "454f05c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ee96845",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df11e82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED SENTENCE:\n",
      "tensor([ 363,   97,   19,   34,    6, 3059,   58,    1])\n",
      "\n",
      "DECODED SENTENCE:\n",
      "What time is it, Tom?\n"
     ]
    }
   ],
   "source": [
    "sentence = \"What time is it, Tom?\"\n",
    "\n",
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(\n",
    "        sentence_encoded[\"input_ids\"][0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "print('ENCODED SENTENCE:')\n",
    "print(sentence_encoded[\"input_ids\"][0])\n",
    "print('\\nDECODED SENTENCE:')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9b42df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT PROMPT:\n",
      "#Person1#: Oh dear, my weight has gone up again.\n",
      "#Person2#: I am not surprised, you eat too much.\n",
      "#Person1#: And I suppose sitting at the desk all day at the office doesn't help.\n",
      "#Person2#: No, I wouldn't think so.\n",
      "#Person1#: I do wish I could lose weight.\n",
      "#Person2#: Well, why don't you go on a diet?\n",
      "#Person1#: I've tried diets before but they've never worked.\n",
      "#Person2#: Perhaps you should exercise more. Why don't you go to an exercise class.\n",
      "#Person1#: Yes, maybe I should.\n",
      "---\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# offers #Person1# some suggestions to lose weight.\n",
      "---\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "#Person1#: I'm not surprised, you eat too much. #Person2#: I'm not surprised, you eat too much. #Person1#: I wish I could lose weight. #\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index=205\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "response = model.generate(inputs[\"input_ids\"], max_new_tokens=50,)\n",
    "output = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "\n",
    "print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "print('---')\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print('---')\n",
    "print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e42d00",
   "metadata": {},
   "source": [
    "#### Zero Shot Inference with an Instruction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "269b3c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT PROMPT:\n",
      "#Person1#: Oh dear, my weight has gone up again.\n",
      "#Person2#: I am not surprised, you eat too much.\n",
      "#Person1#: And I suppose sitting at the desk all day at the office doesn't help.\n",
      "#Person2#: No, I wouldn't think so.\n",
      "#Person1#: I do wish I could lose weight.\n",
      "#Person2#: Well, why don't you go on a diet?\n",
      "#Person1#: I've tried diets before but they've never worked.\n",
      "#Person2#: Perhaps you should exercise more. Why don't you go to an exercise class.\n",
      "#Person1#: Yes, maybe I should.\n",
      "---\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# offers #Person1# some suggestions to lose weight.\n",
      "---\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "#Person1#: I'm not sure what to do.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index=205\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "response = model.generate(inputs[\"input_ids\"], max_new_tokens=50,)\n",
    "output = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "\n",
    "print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "print('---')\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print('---')\n",
    "print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e66e2d",
   "metadata": {},
   "source": [
    "#### Zero Shot Inference with the Prompt Template from FLAN-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a042ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT PROMPT:\n",
      "#Person1#: Oh dear, my weight has gone up again.\n",
      "#Person2#: I am not surprised, you eat too much.\n",
      "#Person1#: And I suppose sitting at the desk all day at the office doesn't help.\n",
      "#Person2#: No, I wouldn't think so.\n",
      "#Person1#: I do wish I could lose weight.\n",
      "#Person2#: Well, why don't you go on a diet?\n",
      "#Person1#: I've tried diets before but they've never worked.\n",
      "#Person2#: Perhaps you should exercise more. Why don't you go to an exercise class.\n",
      "#Person1#: Yes, maybe I should.\n",
      "---\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# offers #Person1# some suggestions to lose weight.\n",
      "---\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "Person1 is overweight and has a lot of food.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index=205\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "    \"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "response = model.generate(inputs[\"input_ids\"], max_new_tokens=50,)\n",
    "output = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "\n",
    "print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "print('---')\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print('---')\n",
    "print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff026cb",
   "metadata": {},
   "source": [
    "#### One Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df030ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT PROMPT:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---\n",
      "INPUT PROMPT:\n",
      "#Person1#: Oh dear, my weight has gone up again.\n",
      "#Person2#: I am not surprised, you eat too much.\n",
      "#Person1#: And I suppose sitting at the desk all day at the office doesn't help.\n",
      "#Person2#: No, I wouldn't think so.\n",
      "#Person1#: I do wish I could lose weight.\n",
      "#Person2#: Well, why don't you go on a diet?\n",
      "#Person1#: I've tried diets before but they've never worked.\n",
      "#Person2#: Perhaps you should exercise more. Why don't you go to an exercise class.\n",
      "#Person1#: Yes, maybe I should.\n",
      "---\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# offers #Person1# some suggestions to lose weight.\n",
      "---\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "Person1 is overweight and has a lot of food.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_index=40\n",
    "example_dialogue = dataset['test'][example_index]['dialogue']\n",
    "example_summary = dataset['test'][example_index]['summary']\n",
    "index=205\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{example_dialogue}\n",
    "\n",
    "What was going on?\n",
    "\n",
    "{example_summary}\n",
    "\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "    \"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "response = model.generate(inputs[\"input_ids\"], max_new_tokens=50,)\n",
    "output = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "\n",
    "print(f'INPUT PROMPT:\\n{example_dialogue}')\n",
    "print('---')\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{example_summary}')\n",
    "print('---')\n",
    "print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "print('---')\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print('---')\n",
    "print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c92842",
   "metadata": {},
   "source": [
    "#### Few Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13fd2ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT PROMPT:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---\n",
      "INPUT PROMPT:\n",
      "#Person1#: May, do you mind helping me prepare for the picnic?\n",
      "#Person2#: Sure. Have you checked the weather report?\n",
      "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
      "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
      "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
      "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
      "#Person1#: All set. May, can you help me take all these things to the living room?\n",
      "#Person2#: Yes, madam.\n",
      "#Person1#: Ask Daniel to give you a hand?\n",
      "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
      "---\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Mom asks May to help to prepare for the picnic and May agrees.\n",
      "---\n",
      "INPUT PROMPT:\n",
      "#Person1#: Oh dear, my weight has gone up again.\n",
      "#Person2#: I am not surprised, you eat too much.\n",
      "#Person1#: And I suppose sitting at the desk all day at the office doesn't help.\n",
      "#Person2#: No, I wouldn't think so.\n",
      "#Person1#: I do wish I could lose weight.\n",
      "#Person2#: Well, why don't you go on a diet?\n",
      "#Person1#: I've tried diets before but they've never worked.\n",
      "#Person2#: Perhaps you should exercise more. Why don't you go to an exercise class.\n",
      "#Person1#: Yes, maybe I should.\n",
      "---\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# offers #Person1# some suggestions to lose weight.\n",
      "---\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "Person1 is overweight and has a lot of food.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_index=[40,80]\n",
    "prompt=\"\"\n",
    "examples=dict()\n",
    "for i in example_index:\n",
    "    example_dialogue = dataset['test'][i]['dialogue']\n",
    "    example_summary = dataset['test'][i]['summary']\n",
    "    examples[f'dialogue{i}']=example_dialogue\n",
    "    examples[f'summary{i}']=example_summary\n",
    "    prompt+=f\"\"\"\n",
    "    Dialogue:\n",
    "\n",
    "    {example_dialogue}\n",
    "\n",
    "    What was going on?\n",
    "\n",
    "    {example_summary}\n",
    "    \"\"\"\n",
    "\n",
    "index=205\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "    \"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "response = model.generate(inputs[\"input_ids\"], max_new_tokens=50,)\n",
    "output = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "print(f'INPUT PROMPT:\\n{examples[\"dialogue40\"]}')\n",
    "print('---')\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{examples[\"summary40\"]}')\n",
    "print('---')\n",
    "print(f'INPUT PROMPT:\\n{examples[\"dialogue80\"]}')\n",
    "print('---')\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{examples[\"summary80\"]}')\n",
    "print('---')\n",
    "print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "print('---')\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print('---')\n",
    "print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1260610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
