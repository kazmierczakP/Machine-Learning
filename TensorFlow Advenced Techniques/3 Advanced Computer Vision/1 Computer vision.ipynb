{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0010a69a-9ab3-4227-89a6-778e8167bd50",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb24c54b-4af1-49fe-9d42-2a9cef1d0db6",
   "metadata": {},
   "source": [
    "- multi-class classification (detects single object in a picture)\n",
    "- multi-label classification (detects multiple different objects in hte picture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51981921-83d9-47cb-a342-45b4f326594c",
   "metadata": {},
   "source": [
    "#### Image segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de808e67-744a-4100-aae0-67176852d4d2",
   "metadata": {},
   "source": [
    "Selecting all pixels that belong to a segment (object)\n",
    "- semantic segmentation (all objects form single segment, each pixel associated to one segemnt) \n",
    "- instance segmentation (objects of the same type are treated as different object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe203ea-9730-4ed1-9a5f-340b7d20734d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218268a-5444-41e7-bb8b-84ae483fa78f",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d30b26c-f276-493e-988a-17be82f99a22",
   "metadata": {},
   "source": [
    "Applying `weights` of one model to another model `instead of initializing weights randomly`. Can be made with different but similar datasets (like cat vs dog and cow vs horse)\n",
    "- saves time & cost\n",
    "- improved performance on small datasets\n",
    "\n",
    "In order to use data of a different size (images) we have to rezise it to the dimentions that pretrained model expects.\n",
    "\n",
    "`resize = tf.keras.layers.UpSampling2D(size=(7,7))(inputs)`\n",
    "\n",
    "where size=(7,7) means multipylng th size of image by 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e1efe9-4186-42ce-bad4-34e4363c6525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc8cba1-bbd1-47b9-8ff3-65f9982a73b7",
   "metadata": {},
   "source": [
    "#### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e7e2122-ddea-41cf-86e2-5b2a8f5d2db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\"\n",
    "data_file_name = \"catsdogs.zip\"\n",
    "download_dir = './tmp/'\n",
    "urllib.request.urlretrieve(data_url, data_file_name)\n",
    "zip_ref = zipfile.ZipFile(data_file_name, 'r')\n",
    "zip_ref.extractall(download_dir)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd91c005-3df5-443b-863b-bf9bf04a542b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cat images: 12501\n",
      "Number of dog images: 12501\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of cat images:\",len(os.listdir('/tmp/PetImages/Cat/')))\n",
    "print(\"Number of dog images:\", len(os.listdir('/tmp/PetImages/Dog/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e4fe8c3-a013-4856-92e8-e67763a7a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('/tmp/cats-v-dogs')\n",
    "os.mkdir('/tmp/cats-v-dogs/training')\n",
    "os.mkdir('/tmp/cats-v-dogs/testing')\n",
    "os.mkdir('/tmp/cats-v-dogs/training/cats')\n",
    "os.mkdir('/tmp/cats-v-dogs/training/dogs')\n",
    "os.mkdir('/tmp/cats-v-dogs/testing/cats')\n",
    "os.mkdir('/tmp/cats-v-dogs/testing/dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3540de82-baae-4309-bcee-733749d7e3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666.jpg is zero length, so ignoring.\n",
      "11702.jpg is zero length, so ignoring.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from shutil import copyfile\n",
    "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
    "    files = []\n",
    "    for filename in os.listdir(SOURCE):\n",
    "        file = SOURCE + filename\n",
    "        if os.path.getsize(file) > 0:\n",
    "            files.append(filename)\n",
    "        else:\n",
    "            print(filename + \" is zero length, so ignoring.\")\n",
    "    training_length = int(len(files) * SPLIT_SIZE)\n",
    "    testing_length = int(len(files) - training_length)\n",
    "    shuffled_set = random.sample(files, len(files))\n",
    "    training_set = shuffled_set[0:training_length]\n",
    "    testing_set = shuffled_set[training_length:]\n",
    "    for filename in training_set:\n",
    "        this_file = SOURCE + filename\n",
    "        destination = TRAINING + filename\n",
    "        copyfile(this_file, destination)\n",
    "    for filename in testing_set:\n",
    "        this_file = SOURCE + filename\n",
    "        destination = TESTING + filename\n",
    "        copyfile(this_file, destination)\n",
    "\n",
    "CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n",
    "DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n",
    "TRAINING_CATS_DIR = \"/tmp/cats-v-dogs/training/cats/\"\n",
    "TRAINING_DOGS_DIR = \"/tmp/cats-v-dogs/training/dogs/\"\n",
    "TESTING_CATS_DIR = \"/tmp/cats-v-dogs/testing/cats/\"\n",
    "TESTING_DOGS_DIR = \"/tmp/cats-v-dogs/testing/dogs/\"\n",
    "\n",
    "split_size = .9\n",
    "split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n",
    "split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dd54117-1afa-4106-bccc-3250ca93d07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22498 images belonging to 2 classes.\n",
      "Found 2500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = \"/tmp/cats-v-dogs/training/\"\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,rotation_range=40,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.2,zoom_range=0.2, horizontal_flip=True,fill_mode='nearest')\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, batch_size=100, class_mode='binary', target_size=(150, 150))\n",
    "\n",
    "VALIDATION_DIR = \"/tmp/cats-v-dogs/testing/\"\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,batch_size=100,class_mode='binary',target_size=(150, 150))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a802872-a384-4230-ac79-9f13b8a5b344",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dcf082f-049e-4248-ab3b-d203928a1f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('inception_v3.h5', <http.client.HTTPMessage at 0x1ced8492eb0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_url = \"https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "weights_file = \"inception_v3.h5\"\n",
    "urllib.request.urlretrieve(weights_url, weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "972c7cdc-d016-4a68-8229-f2596fc94ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last layer output shape:  (None, 7, 7, 768)\n"
     ]
    }
   ],
   "source": [
    "pre_trained_model = InceptionV3(input_shape=(150, 150, 3),include_top=False,weights=None)\n",
    "pre_trained_model.load_weights(weights_file)\n",
    "\n",
    "for layer in pre_trained_model.layers:\n",
    "    layer.trainable = False\n",
    "# pre_trained_model.summary()\n",
    "\n",
    "last_layer = pre_trained_model.get_layer('mixed7')\n",
    "print('last layer output shape: ', last_layer.output_shape)\n",
    "last_output = last_layer.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c0df10-f27d-40d4-a6d6-6a9b4da6df03",
   "metadata": {},
   "source": [
    "`Create model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1983a12-8c77-4006-9da3-80470c6bd5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Flatten()(last_output)\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.models.Model(pre_trained_model.input, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc2a892-4f51-4129-ba11-2008c603acb4",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bae2730b-9e66-423b-8e5b-f0197ff56374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pk764\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizer_v2\\rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2389 - acc: 0.9094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pk764\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\PIL\\TiffImagePlugin.py:870: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 808s 4s/step - loss: 0.2389 - acc: 0.9094 - val_loss: 0.0799 - val_acc: 0.9676\n",
      "Epoch 2/2\n",
      "225/225 [==============================] - 587s 3s/step - loss: 0.1522 - acc: 0.9371 - val_loss: 0.0846 - val_acc: 0.9696\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),loss='binary_crossentropy',metrics=['acc'])\n",
    "history = model.fit(train_generator,validation_data=validation_generator,epochs=2,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8b4678-0d6b-430d-9520-4da9de5cda35",
   "metadata": {},
   "source": [
    "#### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00abd732-5936-4354-b003-dfa1f39faf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.198254e-09]\n",
      " is a cat\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "path='/tmp/cats-v-dogs/testing/cats/13.jpg'\n",
    "#path='/tmp/cats-v-dogs/testing/dogs/11.jpg'\n",
    "img = load_img(path, target_size=(150, 150))\n",
    "x = img_to_array(img)\n",
    "x /= 255\n",
    "x = np.expand_dims(x, axis=0)\n",
    "image_tensor = np.vstack([x])\n",
    "classes = model.predict(image_tensor)\n",
    "print(classes[0])\n",
    "if classes[0]>0.5:\n",
    "    print(\" is a dog\")\n",
    "else:\n",
    "    print(\" is a cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ad83b7-1f0e-451b-8fe4-74cd3fc2af43",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f1f348-1a0e-4d97-9625-278fa4fbb2c2",
   "metadata": {},
   "source": [
    "# Object localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e56c9271-3ec8-43d4-99a9-1855c8a8adf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pk764\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import PIL.Image, PIL.ImageFont, PIL.ImageDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe404906-c539-4a6f-8474-49cdf36a5041",
   "metadata": {},
   "source": [
    "#### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0803fdab-fc0e-405b-90de-4b6c5ff0bbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_tfds(image, label):\n",
    "    xmin = tf.random.uniform((), 0 , 48, dtype=tf.int32)\n",
    "    ymin = tf.random.uniform((), 0 , 48, dtype=tf.int32)\n",
    "    image = tf.reshape(image, (28,28,1,))\n",
    "    image = tf.image.pad_to_bounding_box(image, ymin, xmin, 75, 75)\n",
    "    image = tf.cast(image, tf.float32)/255.0\n",
    "    xmin = tf.cast(xmin, tf.float32)\n",
    "    ymin = tf.cast(ymin, tf.float32)\n",
    "    xmax = (xmin + 28) / 75\n",
    "    ymax = (ymin + 28) / 75\n",
    "    xmin = xmin / 75\n",
    "    ymin = ymin / 75\n",
    "    return image, (tf.one_hot(label, 10), [xmin, ymin, xmax, ymax])\n",
    "  \n",
    "def get_training_dataset():\n",
    "      with  strategy.scope():\n",
    "        dataset = tfds.load(\"mnist\", split=\"train\", as_supervised=True, try_gcs=True)\n",
    "        dataset = dataset.map(read_image_tfds, num_parallel_calls=16)\n",
    "        dataset = dataset.shuffle(5000, reshuffle_each_iteration=True)\n",
    "        dataset = dataset.repeat() # Mandatory for Keras for now\n",
    "        dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) # drop_remainder is important on TPU, batch size must be fixed\n",
    "        dataset = dataset.prefetch(-1)  # fetch next batches while training on the current one (-1: autotune prefetch buffer size)\n",
    "      return dataset\n",
    "\n",
    "def get_validation_dataset():\n",
    "    dataset = tfds.load(\"mnist\", split=\"test\", as_supervised=True, try_gcs=True)\n",
    "    dataset = dataset.map(read_image_tfds, num_parallel_calls=16)\n",
    "\n",
    "    #dataset = dataset.cache() # this small dataset can be entirely cached in RAM\n",
    "    dataset = dataset.batch(10000, drop_remainder=True) # 10000 items in eval dataset, all in one batch\n",
    "    dataset = dataset.repeat() # Mandatory for Keras for now\n",
    "    return dataset\n",
    "\n",
    "with strategy.scope():\n",
    "  training_dataset = get_training_dataset()\n",
    "  validation_dataset = get_validation_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592dbb49-eedf-45f5-8e78-cc58e4570728",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a5ca557-ecda-40b1-ac69-4923e32c4dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(inputs):\n",
    "    x = tf.keras.layers.Conv2D(16, activation='relu', kernel_size=3, input_shape=(75, 75, 1))(inputs)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Conv2D(32,kernel_size=3,activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Conv2D(64,kernel_size=3,activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1e688cb-9d7e-4c4d-b29c-3e3c03e19626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layers(inputs):\n",
    "  x = tf.keras.layers.Flatten()(inputs)\n",
    "  x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e27b2c52-7402-44df-8b13-8013b8b8b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(inputs):\n",
    "  classification_output = tf.keras.layers.Dense(10, activation='softmax', name = 'classification')(inputs)\n",
    "  return classification_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb68e57b-0a85-4ecf-9887-71966dbdbb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounding_box_regression(inputs):\n",
    "    bounding_box_regression_output = tf.keras.layers.Dense(units = '4', name = 'bounding_box')(inputs)\n",
    "    return bounding_box_regression_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "badfe3e0-51e1-4893-b679-51efc6a4c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model(inputs):\n",
    "    feature_cnn = feature_extractor(inputs)\n",
    "    dense_output = dense_layers(feature_cnn)\n",
    "    classification_output = classifier(dense_output)\n",
    "    bounding_box_output = bounding_box_regression(dense_output)\n",
    "    model = tf.keras.Model(inputs = inputs, outputs = [classification_output, bounding_box_output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf2cdaa7-63ec-4732-8217-93310df30138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 75, 75, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)             (None, 73, 73, 16)   160         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " average_pooling2d_9 (AveragePo  (None, 36, 36, 16)  0           ['conv2d_94[0][0]']              \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_95 (Conv2D)             (None, 34, 34, 32)   4640        ['average_pooling2d_9[0][0]']    \n",
      "                                                                                                  \n",
      " average_pooling2d_10 (AverageP  (None, 17, 17, 32)  0           ['conv2d_95[0][0]']              \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_96 (Conv2D)             (None, 15, 15, 64)   18496       ['average_pooling2d_10[0][0]']   \n",
      "                                                                                                  \n",
      " average_pooling2d_11 (AverageP  (None, 7, 7, 64)    0           ['conv2d_96[0][0]']              \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 3136)         0           ['average_pooling2d_11[0][0]']   \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 128)          401536      ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " classification (Dense)         (None, 10)           1290        ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " bounding_box (Dense)           (None, 4)            516         ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 426,638\n",
      "Trainable params: 426,638\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def define_and_compile_model(inputs):\n",
    "  model = final_model(inputs)\n",
    "  model.compile(optimizer='adam', \n",
    "              loss = {'classification' : 'categorical_crossentropy','bounding_box' : 'mse'},\n",
    "              metrics = {'classification' : 'accuracy','bounding_box' : 'mse'})\n",
    "  return model\n",
    "\n",
    "with strategy.scope():\n",
    "  inputs = tf.keras.layers.Input(shape=(75, 75, 1,))\n",
    "  model = define_and_compile_model(inputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedbcc7c-090d-4b95-aa21-9758335b490b",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7adc41b-167b-4a64-b0d1-acebb939da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.get_strategy()\n",
    "BATCH_SIZE = 64 * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb85a2ac-fec4-44cb-9ccb-7bc864790b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "937/937 [==============================] - 125s 132ms/step - loss: 1.0195 - classification_loss: 1.0040 - bounding_box_loss: 0.0155 - classification_accuracy: 0.6455 - bounding_box_mse: 0.0155 - val_loss: 0.3207 - val_classification_loss: 0.3109 - val_bounding_box_loss: 0.0099 - val_classification_accuracy: 0.9053 - val_bounding_box_mse: 0.0099\n",
      "Epoch 2/10\n",
      "937/937 [==============================] - 123s 131ms/step - loss: 0.2683 - classification_loss: 0.2618 - bounding_box_loss: 0.0065 - classification_accuracy: 0.9201 - bounding_box_mse: 0.0065 - val_loss: 0.2099 - val_classification_loss: 0.2059 - val_bounding_box_loss: 0.0041 - val_classification_accuracy: 0.9399 - val_bounding_box_mse: 0.0041\n",
      "Epoch 3/10\n",
      "937/937 [==============================] - 125s 133ms/step - loss: 0.1968 - classification_loss: 0.1934 - bounding_box_loss: 0.0034 - classification_accuracy: 0.9422 - bounding_box_mse: 0.0034 - val_loss: 0.1291 - val_classification_loss: 0.1267 - val_bounding_box_loss: 0.0024 - val_classification_accuracy: 0.9621 - val_bounding_box_mse: 0.0024\n",
      "Epoch 4/10\n",
      "937/937 [==============================] - 124s 133ms/step - loss: 0.1573 - classification_loss: 0.1550 - bounding_box_loss: 0.0023 - classification_accuracy: 0.9523 - bounding_box_mse: 0.0023 - val_loss: 0.1151 - val_classification_loss: 0.1133 - val_bounding_box_loss: 0.0018 - val_classification_accuracy: 0.9644 - val_bounding_box_mse: 0.0018\n",
      "Epoch 5/10\n",
      "937/937 [==============================] - 123s 131ms/step - loss: 0.1326 - classification_loss: 0.1308 - bounding_box_loss: 0.0018 - classification_accuracy: 0.9591 - bounding_box_mse: 0.0018 - val_loss: 0.0913 - val_classification_loss: 0.0898 - val_bounding_box_loss: 0.0014 - val_classification_accuracy: 0.9735 - val_bounding_box_mse: 0.0014\n",
      "Epoch 6/10\n",
      "937/937 [==============================] - 123s 131ms/step - loss: 0.1151 - classification_loss: 0.1135 - bounding_box_loss: 0.0015 - classification_accuracy: 0.9649 - bounding_box_mse: 0.0015 - val_loss: 0.0979 - val_classification_loss: 0.0966 - val_bounding_box_loss: 0.0013 - val_classification_accuracy: 0.9701 - val_bounding_box_mse: 0.0013\n",
      "Epoch 7/10\n",
      "937/937 [==============================] - 123s 131ms/step - loss: 0.1038 - classification_loss: 0.1025 - bounding_box_loss: 0.0013 - classification_accuracy: 0.9676 - bounding_box_mse: 0.0013 - val_loss: 0.0982 - val_classification_loss: 0.0970 - val_bounding_box_loss: 0.0012 - val_classification_accuracy: 0.9685 - val_bounding_box_mse: 0.0012\n",
      "Epoch 8/10\n",
      "937/937 [==============================] - 125s 134ms/step - loss: 0.0906 - classification_loss: 0.0895 - bounding_box_loss: 0.0011 - classification_accuracy: 0.9721 - bounding_box_mse: 0.0011 - val_loss: 0.0803 - val_classification_loss: 0.0790 - val_bounding_box_loss: 0.0013 - val_classification_accuracy: 0.9738 - val_bounding_box_mse: 0.0013\n",
      "Epoch 9/10\n",
      "937/937 [==============================] - 123s 132ms/step - loss: 0.0837 - classification_loss: 0.0827 - bounding_box_loss: 0.0011 - classification_accuracy: 0.9740 - bounding_box_mse: 0.0011 - val_loss: 0.0713 - val_classification_loss: 0.0703 - val_bounding_box_loss: 9.7884e-04 - val_classification_accuracy: 0.9769 - val_bounding_box_mse: 9.7884e-04\n",
      "Epoch 10/10\n",
      "937/937 [==============================] - 123s 132ms/step - loss: 0.0781 - classification_loss: 0.0771 - bounding_box_loss: 0.0010 - classification_accuracy: 0.9765 - bounding_box_mse: 0.0010 - val_loss: 0.0607 - val_classification_loss: 0.0598 - val_bounding_box_loss: 9.5280e-04 - val_classification_accuracy: 0.9802 - val_bounding_box_mse: 9.5280e-04\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0666 - classification_loss: 0.0657 - bounding_box_loss: 9.4836e-04 - classification_accuracy: 0.9790 - bounding_box_mse: 9.4836e-04\n",
      "Validation accuracy:  0.9789999723434448\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10 # 45\n",
    "steps_per_epoch = 60000//BATCH_SIZE  # 60,000 items in this dataset\n",
    "validation_steps = 1\n",
    "\n",
    "history = model.fit(training_dataset,steps_per_epoch=steps_per_epoch, validation_data=validation_dataset, validation_steps=validation_steps, epochs=EPOCHS)\n",
    "loss, classification_loss, bounding_box_loss, classification_accuracy, bounding_box_mse = model.evaluate(validation_dataset, steps=1)\n",
    "print(\"Validation accuracy: \", classification_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9a211a-8a55-4569-be0d-4d1b80a777a8",
   "metadata": {},
   "source": [
    "#### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "800a2f47-815d-4550-8148-9529fa72848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_width = 75\n",
    "im_height = 75\n",
    "use_normalized_coordinates = True\n",
    "\n",
    "def draw_bounding_box_on_image(image,ymin,xmin,ymax,xmax,color='red',thickness=1,display_str=None,use_normalized_coordinates=True):\n",
    "  draw = PIL.ImageDraw.Draw(image)\n",
    "  im_width, im_height = image.size\n",
    "  if use_normalized_coordinates:\n",
    "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width, ymin * im_height, ymax * im_height)\n",
    "  else:\n",
    "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
    "  draw.line([(left, top), (left, bottom), (right, bottom),(right, top), (left, top)], width=thickness, fill=color)\n",
    "\n",
    "def draw_bounding_boxes_on_image(image,boxes,thickness=1,display_str_list=()):\n",
    "  boxes_shape = boxes.shape\n",
    "  if not boxes_shape:\n",
    "    return\n",
    "  if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n",
    "    raise ValueError('Input must be of size [N, 4]')\n",
    "  for i in range(boxes_shape[0]):\n",
    "    draw_bounding_box_on_image(image, boxes[i, 1], boxes[i, 0], boxes[i, 3],boxes[i, 2],)\n",
    "        \n",
    "def draw_bounding_boxes_on_image_array(image,boxes,thickness=1,display_str_list=()):\n",
    "  image_pil = PIL.Image.fromarray(image)\n",
    "  rgbimg = PIL.Image.new(\"RGBA\", image_pil.size)\n",
    "  rgbimg.paste(image_pil)\n",
    "  draw_bounding_boxes_on_image(rgbimg, boxes)\n",
    "  return np.array(rgbimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "482977b4-b4d3-4bc8-b80c-6d13089eb1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(pred_box, true_box):\n",
    "    xmin_pred, ymin_pred, xmax_pred, ymax_pred =  np.split(pred_box, 4, axis = 1)\n",
    "    xmin_true, ymin_true, xmax_true, ymax_true = np.split(true_box, 4, axis = 1)\n",
    "    smoothing_factor = 1e-10\n",
    "    xmin_overlap = np.maximum(xmin_pred, xmin_true)\n",
    "    xmax_overlap = np.minimum(xmax_pred, xmax_true)\n",
    "    ymin_overlap = np.maximum(ymin_pred, ymin_true)\n",
    "    ymax_overlap = np.minimum(ymax_pred, ymax_true)\n",
    "    pred_box_area = (xmax_pred - xmin_pred) * (ymax_pred - ymin_pred)\n",
    "    true_box_area = (xmax_true - xmin_true) * (ymax_true - ymin_true)\n",
    "    overlap_area = np.maximum((xmax_overlap - xmin_overlap), 0)  * np.maximum((ymax_overlap - ymin_overlap), 0)\n",
    "    union_area = (pred_box_area + true_box_area) - overlap_area\n",
    "    iou = (overlap_area + smoothing_factor) / (union_area + smoothing_factor)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8c89260b-90c8-4fea-9abf-71186ebb3b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_numpy_util(training_dataset, validation_dataset, N):\n",
    "  \n",
    "  # get one batch from each: 10000 validation digits, N training digits\n",
    "  batch_train_ds = training_dataset.unbatch().batch(N)\n",
    "  \n",
    "  # eager execution: loop through datasets normally\n",
    "  if tf.executing_eagerly():\n",
    "    for validation_digits, (validation_labels, validation_bboxes) in validation_dataset:\n",
    "      validation_digits = validation_digits.numpy()\n",
    "      validation_labels = validation_labels.numpy()\n",
    "      validation_bboxes = validation_bboxes.numpy()\n",
    "      break\n",
    "    for training_digits, (training_labels, training_bboxes) in batch_train_ds:\n",
    "      training_digits = training_digits.numpy()\n",
    "      training_labels = training_labels.numpy()\n",
    "      training_bboxes = training_bboxes.numpy()\n",
    "      break\n",
    "  \n",
    "  # these were one-hot encoded in the dataset\n",
    "  validation_labels = np.argmax(validation_labels, axis=1)\n",
    "  training_labels = np.argmax(training_labels, axis=1)\n",
    "  \n",
    "  return (training_digits, training_labels, training_bboxes,\n",
    "          validation_digits, validation_labels, validation_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "de497466-187b-4235-bea9-0c9af601b6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_digits, training_labels, training_bboxes,validation_digits, validation_labels, validation_bboxes) = dataset_to_numpy_util(training_dataset, validation_dataset, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "2875dbc4-c287-4f95-bcc7-cdddc7462bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions where iou > threshold(0.6): 9349\n",
      "Number of predictions where iou < threshold(0.6): 651\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(validation_digits, batch_size=64)\n",
    "predicted_labels = np.argmax(predictions[0], axis=1)\n",
    "predicted_bboxes = predictions[1]\n",
    "iou = intersection_over_union(predicted_bboxes, validation_bboxes)\n",
    "iou_threshold = 0.6\n",
    "\n",
    "print(\"Number of predictions where iou > threshold(%s): %s\" % (iou_threshold, (iou >= iou_threshold).sum()))\n",
    "print(\"Number of predictions where iou < threshold(%s): %s\" % (iou_threshold, (iou < iou_threshold).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "60ef388a-343b-4901-a22b-b9d2a6f5d9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: 2\n",
      "Prediction: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ce9bd59970>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_digits[0]\n",
    "img=validation_digits[0].reshape(75,75)\n",
    "print(f'True label: {predicted_labels[0]}')\n",
    "print(f'Prediction: {validation_labels[0]}')\n",
    "\n",
    "predictions[1][0]\n",
    "pred=predictions[1][0].reshape(1,4)\n",
    "box=draw_bounding_boxes_on_image_array(img,pred)\n",
    "new_box=[]\n",
    "for i in box:\n",
    "    k=[]\n",
    "    for j in i:\n",
    "        k.append(j[0])\n",
    "    new_box.append(k)\n",
    "new_box=np.array(new_box,dtype=\"float32\")\n",
    "image=new_box/255+img\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3f1742-b9ca-43b8-af44-7f7898d44509",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd74734-e82a-44db-bb94-b6dfc76e6998",
   "metadata": {},
   "source": [
    "#### displaying training images and boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "2f2277a0-e96d-4063-8802-a4dd9c5f938d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 75, 75, 1), ((64, 10), (64, 4))), types: (tf.float32, (tf.float32, tf.float32))>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "dcc28830-4988-4654-b169-dfbb4c7d9053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "for image, label in tfds.as_numpy(training_dataset.take(1)):\n",
    "  print(type(image), type(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "1cb79935-e773-4f2f-b07c-12f9d62e242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "251bf143-4da1-4587-9b7b-c187b478f927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ce9bf667c0>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img=image[0].reshape(75,75)\n",
    "lab=label[1][0]\n",
    "lab=lab.reshape(1,4)\n",
    "box=draw_bounding_boxes_on_image_array(img,lab)\n",
    "new_box=[]\n",
    "for i in box:\n",
    "    k=[]\n",
    "    for j in i:\n",
    "        k.append(j[0])\n",
    "    new_box.append(k)\n",
    "new_box=np.array(new_box,dtype=\"float32\")\n",
    "image=new_box/255+img\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f53d5d-0235-4086-8cdf-6cd8f842ecfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
